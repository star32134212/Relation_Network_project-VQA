{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##冰激淋+擔擔麵+蜂蜜檸檬適用\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #使用第0張顯卡 ##冰激淋+擔擔麵+蜂蜜檸檬適用 0or1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Flatten, Convolution1D, MaxPooling1D, Activation, BatchNormalization,\\\n",
    "Lambda, Concatenate, Add, Conv2D, Conv1D,TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基本設定\n",
    "\"\"\"\n",
    "#n=148335 #用2016 1~3月的資料 288x(31+29+31+30+31+30+...)=52416 210528-1(all) 差值在減一  \n",
    "l=144 #區間為12小時\n",
    "currency = [\"SEK\",\"CHF\",\"CAD\",\"GBP\",\"JPY\",\"EUR\",\"AUD\"]\n",
    "#SEK:瑞典克朗  CHF:瑞士法郎 CAD:加拿大幣 GBP:英鎊 \n",
    "#currency = [\"BTC\",\"DASH\",\"ETH\",\"LTC\",\"JBY\",\"GBP\",\"EUR\",\"AUD\",\"US\"]\n",
    "currencynum = len(currency)\n",
    "month = [0,31,60,91,121,152,182,213,244,274,305,335,366,397,425,456,486,517,547,578,609,639,670,700,731] #2016是閏年 366天\n",
    "daynum = [0,5802,11745,18351,24380,30579,36861,42795,49201,55342,61230,67400,73732,79984,85726,92343,98127,104692,111028,117076,123700,129807,136083,142401,148335]\n",
    "question = [\"trand\",\"volatility\"]\n",
    "# 貨幣組合，1 : P, 0 : C\n",
    "M=1\n",
    "head = 12\n",
    "tail = 16\n",
    "epochs = 300\n",
    "batch_size = 1024\n",
    "\n",
    "all_cur_pair = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "if(M==0):\n",
    "    all_cur_pair_P = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "elif(M==1):\n",
    "    all_cur_pair_P = list(permutations(currency,2))# (Pn取2) 問題有先後順序時使用  \n",
    "all_question= list(permutations(question,1))\n",
    "np.set_printoptions(suppress=True)#不要用科學符號輸出\n",
    "lastepoch_train_acc = []\n",
    "lastepoch_test_acc = []\n",
    "lastepoch_train_loss = []\n",
    "lastepoch_test_loss = []\n",
    "total_test_vol = []\n",
    "total_test_trend = []\n",
    "train_length=3 \n",
    "test_length=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\n",
    "str, onehotcode, company code轉換\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "currencylist = {}\n",
    "questionlist = {}\n",
    "for i in range(len(currency)):\n",
    "    currencylist[i] = currency[i]\n",
    "\n",
    "for i in range(len(question)):\n",
    "    questionlist[i] = question[i]\n",
    "\n",
    "def str_to_currency(cur):\n",
    "    return {v: k for k, v in currencylist.items()}[cur]\n",
    "\n",
    "def str_to_question(q):\n",
    "    return {v: k for k, v in questionlist.items()}[q]\n",
    "\n",
    "\n",
    "def one_hot_currency(currencylist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(currencylist)))\n",
    "    for i in range(len(currencylist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "def one_hot_question(questionlist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(questionlist)))\n",
    "    for i in range(len(questionlist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "onehotcurrency = one_hot_currency(currencylist)\n",
    "onehotquestion = one_hot_question(questionlist)\n",
    "\n",
    "\"\"\"\n",
    "Q&A\n",
    "\"\"\"\n",
    "vol='volatility'\n",
    "def set_question(com1, com2, typeq):\n",
    "\n",
    "    return np.concatenate((onehotquestion[str_to_question(typeq)],onehotcurrency[str_to_currency(com1)], onehotcurrency[str_to_currency(com2)]))\n",
    "\n",
    "qtype = ['big']\n",
    "HVqtype = ['big']\n",
    "\n",
    "\"\"\"比漲幅程度類問題\"\"\"\n",
    "def set_question_and_answer_pair(data, data2, n, all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_pair = {}\n",
    "    a_pair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    #data_sum=np.sum(data,axis=1)\n",
    "    data2_sum=np.sum(data2,axis=1)\n",
    "    outcome[0]=(data2_sum[0])\n",
    "    outcome[1]=(data2_sum[1])\n",
    "    outcome[2]=(data2_sum[2])\n",
    "    outcome[3]=(data2_sum[3])\n",
    "    outcome[4]=(data2_sum[4])\n",
    "    outcome[5]=(data2_sum[5])\n",
    "    outcome[6]=(data2_sum[6])\n",
    "    tmp_q = []\n",
    "    tmp_a = []\n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1],all_question[0][0]))\n",
    "    q_pair[i] = tmp_q        \n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "            tmp_a.append(1)\n",
    "        else:\n",
    "            tmp_a.append(0)\n",
    "    a_pair[i] = tmp_a\n",
    "    return (data, q_pair, a_pair)\n",
    "\n",
    "\n",
    "\"\"\"History Volatility類問題\"\"\"\n",
    "def set_HVquestion_and_HVanswer_pair(data, data2,all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_HVpair = {}\n",
    "    a_HVpair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    data2_std=np.std(data2,axis=1) \n",
    "    outcome[0]=data2_std[0]\n",
    "    outcome[1]=data2_std[1]\n",
    "    outcome[2]=data2_std[2]\n",
    "    outcome[3]=data2_std[3]\n",
    "    outcome[4]=data2_std[4]\n",
    "    outcome[5]=data2_std[5]     \n",
    "    outcome[6]=data2_std[6]     \n",
    "    tmp_q = []\n",
    "    tmp_a = []   \n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1],all_question[1][0]))\n",
    "    q_HVpair[i] = tmp_q\n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "            tmp_a.append(1)\n",
    "        else:\n",
    "            tmp_a.append(0)\n",
    "                \n",
    "    a_HVpair[i] = tmp_a    \n",
    "    return (data, q_HVpair, a_HVpair)\n",
    "\n",
    "\"\"\"\n",
    "網路函數\n",
    "\"\"\"\n",
    "def ConvolutionNetworks(filter_num,kernel_size):\n",
    "    def conv(model):\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = (MaxPooling1D(pool_size=7))(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    tag = np.zeros((d,1))\n",
    "    for i in range(d):\n",
    "        tag[i] = float(int(i%d))/(d-1)*2-1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "此處開始寫rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    # 處理訓練資料\n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(\"[Training model......]\")\n",
    "\n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    \n",
    "    history = model.fit([Train_v, Train_q], Train_a, validation_data=([Test_v,Test_q],Test_a),batch_size=batch_size ,epochs = epochs,shuffle=False)\n",
    "    pred = model.predict([Test_v, Test_q])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i] <= 0.5:\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "            \n",
    "        if pred[i] == Test_a[i]:\n",
    "            count+=1\n",
    "    print(count,count/pred.shape[0])\n",
    "    \"\"\"\n",
    "    分開兩種問題的test集\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    v_trend=[]\n",
    "    q_trend=[]\n",
    "    a_trend=[]\n",
    "    v_vol=[]\n",
    "    q_vol=[]\n",
    "    a_vol=[]\n",
    "    trend_count=0\n",
    "    vol_count=0\n",
    "    for ii in range(0,len(Test_q)):\n",
    "        if(flag==0):\n",
    "            #print(0)\n",
    "            v_trend.append(Test_v[ii])\n",
    "            q_trend.append(Test_q[ii])\n",
    "            a_trend.append(Test_a[ii])\n",
    "            trend_count=trend_count+1\n",
    "            if(trend_count==84):\n",
    "                trend_count=0\n",
    "                flag=1\n",
    "        elif(flag==1):\n",
    "            #print(1)\n",
    "            v_vol.append(Test_v[ii])\n",
    "            q_vol.append(Test_q[ii])\n",
    "            a_vol.append(Test_a[ii])\n",
    "            vol_count=vol_count+1\n",
    "            if(vol_count==84):\n",
    "                vol_count=0\n",
    "                flag=0\n",
    "    \n",
    "    \"\"\"\n",
    "    分開兩種問題的predict正確率\n",
    "    \"\"\"\n",
    "    pred_trend = model.predict([v_trend, q_trend])\n",
    "    count = 0\n",
    "    for i in range(pred_trend.shape[0]):\n",
    "        if pred_trend[i] <= 0.5:\n",
    "            pred_trend[i] = 0\n",
    "        else:\n",
    "            pred_trend[i] = 1\n",
    "            \n",
    "        if pred_trend[i] == a_trend[i]:\n",
    "            count+=1\n",
    "    print(\"trend_test_acc:\")\n",
    "    print(count,count/pred_trend.shape[0])    \n",
    "    total_test_trend.append(count/pred_trend.shape[0])\n",
    "\n",
    "    pred_vol = model.predict([v_vol, q_vol])\n",
    "    count = 0\n",
    "    for i in range(pred_vol.shape[0]):\n",
    "        if pred_vol[i] <= 0.5:\n",
    "            pred_vol[i] = 0\n",
    "        else:\n",
    "            pred_vol[i] = 1\n",
    "            \n",
    "        if pred_vol[i] == a_vol[i]:\n",
    "            count+=1\n",
    "    print(\"vol_test_acc:\")\n",
    "    print(count,count/pred_vol.shape[0]) \n",
    "    total_test_vol.append(count/pred_vol.shape[0])\n",
    "    \"\"\"\n",
    "    benchmark1\n",
    "    \"\"\"\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]):\n",
    "        benchacc=benchacc+Test_a[i]\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"猜答案多的那邊 benchacc1:\")\n",
    "    print(benchacc)\n",
    "\n",
    "    \"\"\"\n",
    "    benchmark2\n",
    "    \"\"\"\n",
    "    #第一個直接猜1\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]-1):\n",
    "        if(Test_a[i]!=Test_a[i+1]):\n",
    "            benchacc=benchacc+1\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"參考前一個答案 benchacc2:\")\n",
    "    print(benchacc)    \n",
    "    \n",
    "    \"\"\"\n",
    "    畫圖\n",
    "    \"\"\"\n",
    "    benchfunction=np.ones(a.shape[0])\n",
    "    benchfunction=benchfunction*benchacc\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    lastepoch_train_loss.append(history.history['loss'][-1])\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    lastepoch_test_loss.append(history.history['val_loss'][-1])\n",
    "    print(\"loss:\")\n",
    "    print(history.history['loss'][-1])\n",
    "    print(\"val_loss:\")\n",
    "    print(history.history['val_loss'][-1])\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"loss\") \n",
    "    #plt.title(\"The Title\") \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    lastepoch_train_acc.append(history.history['acc'][-1])\n",
    "    plt.plot(history.history['val_acc'], label='test')\n",
    "    lastepoch_test_acc.append(history.history['val_acc'][-1])  \n",
    "    plt.plot(np.arange(epochs),np.repeat(benchacc,epochs), label='benchmark')\n",
    "    print(\"acc:\")\n",
    "    print(history.history['acc'][-1])\n",
    "    print(\"val_acc:\")\n",
    "    print(history.history['val_acc'][-1])\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"acc\") \n",
    "    #plt.title(\"The Title\") \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 18, 1)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "12\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=15.2 GiB, count=2, average=7787 MiB\n",
      "[Training model......]\n",
      "Train on 1539132 samples, validate on 461664 samples\n",
      "Epoch 1/300\n",
      "1539132/1539132 [==============================] - 550s 357us/step - loss: 0.9668 - acc: 0.5436 - val_loss: 0.6552 - val_acc: 0.6023\n",
      "Epoch 2/300\n",
      "1539132/1539132 [==============================] - 173s 112us/step - loss: 0.6544 - acc: 0.5876 - val_loss: 0.6486 - val_acc: 0.5858\n",
      "Epoch 3/300\n",
      "1539132/1539132 [==============================] - 173s 112us/step - loss: 0.6462 - acc: 0.5972 - val_loss: 0.6465 - val_acc: 0.5939\n",
      "Epoch 4/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6422 - acc: 0.6010 - val_loss: 0.6499 - val_acc: 0.5845\n",
      "Epoch 5/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6399 - acc: 0.6038 - val_loss: 0.6492 - val_acc: 0.5947\n",
      "Epoch 6/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6377 - acc: 0.6049 - val_loss: 0.6483 - val_acc: 0.5898\n",
      "Epoch 7/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6374 - acc: 0.6047 - val_loss: 0.6503 - val_acc: 0.5893\n",
      "Epoch 8/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6362 - acc: 0.6070 - val_loss: 0.6487 - val_acc: 0.5906\n",
      "Epoch 9/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6354 - acc: 0.6069 - val_loss: 0.6490 - val_acc: 0.5862\n",
      "Epoch 10/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6354 - acc: 0.6082 - val_loss: 0.6483 - val_acc: 0.5977\n",
      "Epoch 11/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6352 - acc: 0.6092 - val_loss: 0.6482 - val_acc: 0.5979\n",
      "Epoch 12/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6356 - acc: 0.6086 - val_loss: 0.6478 - val_acc: 0.5961\n",
      "Epoch 13/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6347 - acc: 0.6125 - val_loss: 0.6478 - val_acc: 0.5997\n",
      "Epoch 14/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6337 - acc: 0.6132 - val_loss: 0.6483 - val_acc: 0.5995\n",
      "Epoch 15/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6333 - acc: 0.6139 - val_loss: 0.6479 - val_acc: 0.6002\n",
      "Epoch 16/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6332 - acc: 0.6151 - val_loss: 0.6485 - val_acc: 0.6016\n",
      "Epoch 17/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6331 - acc: 0.6150 - val_loss: 0.6488 - val_acc: 0.5974\n",
      "Epoch 18/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6325 - acc: 0.6154 - val_loss: 0.6477 - val_acc: 0.6007\n",
      "Epoch 19/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6324 - acc: 0.6152 - val_loss: 0.6483 - val_acc: 0.6015\n",
      "Epoch 20/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6323 - acc: 0.6147 - val_loss: 0.6486 - val_acc: 0.6015\n",
      "Epoch 21/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6322 - acc: 0.6136 - val_loss: 0.6488 - val_acc: 0.5996\n",
      "Epoch 22/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6317 - acc: 0.6144 - val_loss: 0.6478 - val_acc: 0.5987\n",
      "Epoch 23/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6307 - acc: 0.6150 - val_loss: 0.6464 - val_acc: 0.6016\n",
      "Epoch 24/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6278 - acc: 0.6178 - val_loss: 0.6474 - val_acc: 0.5988\n",
      "Epoch 25/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6238 - acc: 0.6218 - val_loss: 0.6478 - val_acc: 0.5998\n",
      "Epoch 26/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6184 - acc: 0.6277 - val_loss: 0.6496 - val_acc: 0.5916\n",
      "Epoch 27/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6127 - acc: 0.6332 - val_loss: 0.6530 - val_acc: 0.5897\n",
      "Epoch 28/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6075 - acc: 0.6369 - val_loss: 0.6535 - val_acc: 0.5907\n",
      "Epoch 29/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6042 - acc: 0.6385 - val_loss: 0.6535 - val_acc: 0.5929\n",
      "Epoch 30/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6018 - acc: 0.6398 - val_loss: 0.6576 - val_acc: 0.5886\n",
      "Epoch 31/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.6005 - acc: 0.6409 - val_loss: 0.6589 - val_acc: 0.5891\n",
      "Epoch 32/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5987 - acc: 0.6413 - val_loss: 0.6595 - val_acc: 0.5886\n",
      "Epoch 33/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5977 - acc: 0.6427 - val_loss: 0.6607 - val_acc: 0.5889\n",
      "Epoch 34/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5967 - acc: 0.6423 - val_loss: 0.6635 - val_acc: 0.5847\n",
      "Epoch 35/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5955 - acc: 0.6432 - val_loss: 0.6697 - val_acc: 0.5821\n",
      "Epoch 36/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5948 - acc: 0.6435 - val_loss: 0.6666 - val_acc: 0.5810\n",
      "Epoch 37/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5943 - acc: 0.6443 - val_loss: 0.6700 - val_acc: 0.5792\n",
      "Epoch 38/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5935 - acc: 0.6444 - val_loss: 0.6707 - val_acc: 0.5804\n",
      "Epoch 39/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5931 - acc: 0.6444 - val_loss: 0.6695 - val_acc: 0.5792\n",
      "Epoch 40/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5925 - acc: 0.6443 - val_loss: 0.6723 - val_acc: 0.5793\n",
      "Epoch 41/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5916 - acc: 0.6455 - val_loss: 0.6723 - val_acc: 0.5794\n",
      "Epoch 42/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5911 - acc: 0.6454 - val_loss: 0.6713 - val_acc: 0.5803\n",
      "Epoch 43/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5903 - acc: 0.6463 - val_loss: 0.6752 - val_acc: 0.5804\n",
      "Epoch 44/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5897 - acc: 0.6462 - val_loss: 0.6727 - val_acc: 0.5820\n",
      "Epoch 45/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5893 - acc: 0.6463 - val_loss: 0.6732 - val_acc: 0.5824\n",
      "Epoch 46/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5887 - acc: 0.6471 - val_loss: 0.6775 - val_acc: 0.5805\n",
      "Epoch 47/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5882 - acc: 0.6469 - val_loss: 0.6736 - val_acc: 0.5825\n",
      "Epoch 48/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5880 - acc: 0.6469 - val_loss: 0.6697 - val_acc: 0.5805\n",
      "Epoch 49/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5875 - acc: 0.6473 - val_loss: 0.6685 - val_acc: 0.5808\n",
      "Epoch 50/300\n",
      "1539132/1539132 [==============================] - 172s 111us/step - loss: 0.5869 - acc: 0.6482 - val_loss: 0.6668 - val_acc: 0.5802\n",
      "Epoch 51/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5867 - acc: 0.6481 - val_loss: 0.6685 - val_acc: 0.5802\n",
      "Epoch 52/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5861 - acc: 0.6474 - val_loss: 0.6671 - val_acc: 0.5799\n",
      "Epoch 53/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5854 - acc: 0.6482 - val_loss: 0.6702 - val_acc: 0.5778\n",
      "Epoch 54/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5853 - acc: 0.6488 - val_loss: 0.6695 - val_acc: 0.5766\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5848 - acc: 0.6485 - val_loss: 0.6696 - val_acc: 0.5777\n",
      "Epoch 56/300\n",
      "1539132/1539132 [==============================] - 172s 111us/step - loss: 0.5849 - acc: 0.6487 - val_loss: 0.6699 - val_acc: 0.5772\n",
      "Epoch 57/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5841 - acc: 0.6494 - val_loss: 0.6640 - val_acc: 0.5806\n",
      "Epoch 58/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5838 - acc: 0.6493 - val_loss: 0.6661 - val_acc: 0.5816\n",
      "Epoch 59/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5834 - acc: 0.6492 - val_loss: 0.6638 - val_acc: 0.5845\n",
      "Epoch 60/300\n",
      "1539132/1539132 [==============================] - 172s 111us/step - loss: 0.5827 - acc: 0.6493 - val_loss: 0.6647 - val_acc: 0.5811\n",
      "Epoch 61/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5822 - acc: 0.6500 - val_loss: 0.6647 - val_acc: 0.5817\n",
      "Epoch 62/300\n",
      "1539132/1539132 [==============================] - 171s 111us/step - loss: 0.5815 - acc: 0.6498 - val_loss: 0.6638 - val_acc: 0.5806\n",
      "Epoch 63/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5806 - acc: 0.6506 - val_loss: 0.6634 - val_acc: 0.5844\n",
      "Epoch 64/300\n",
      "1539132/1539132 [==============================] - 172s 111us/step - loss: 0.5799 - acc: 0.6508 - val_loss: 0.6654 - val_acc: 0.5838\n",
      "Epoch 65/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5788 - acc: 0.6513 - val_loss: 0.6660 - val_acc: 0.5832\n",
      "Epoch 66/300\n",
      "1539132/1539132 [==============================] - 171s 111us/step - loss: 0.5780 - acc: 0.6523 - val_loss: 0.6661 - val_acc: 0.5863\n",
      "Epoch 67/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5772 - acc: 0.6522 - val_loss: 0.6670 - val_acc: 0.5849\n",
      "Epoch 68/300\n",
      "1539132/1539132 [==============================] - 171s 111us/step - loss: 0.5762 - acc: 0.6529 - val_loss: 0.6670 - val_acc: 0.5852\n",
      "Epoch 69/300\n",
      "1539132/1539132 [==============================] - 172s 112us/step - loss: 0.5750 - acc: 0.6537 - val_loss: 0.6684 - val_acc: 0.5856\n",
      "Epoch 70/300\n",
      "  30720/1539132 [..............................] - ETA: 2:32 - loss: 0.6490 - acc: 0.5924"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4b4dc21d06b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m#model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mfit_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdaynum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdaynum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdaynum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-53a7cafef46b>\u001b[0m in \u001b[0;36mfit_show\u001b[0;34m(traindata, m1, m2, m3)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mTest_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cur_pair_P\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrain_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTest_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTest_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for z in range(head,tail,1):\n",
    "    \"\"\"\n",
    "     V\n",
    "    \"\"\"\n",
    "    n=daynum[tail]-daynum[head]\n",
    "    df = pd.read_csv('JPY_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    print(df.shape)  \n",
    "    jpy5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        jpy5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('EUR_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eur5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eur5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('AUD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    aud5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        aud5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('CHF_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    chf5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        chf5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('SEK_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    sek5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        sek5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('CAD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    cad5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        cad5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('GBP_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    gbp5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        gbp5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    print('finish dataread')\n",
    "    Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "    for p in range(n-l+1):\n",
    "        Train_data[p,0,:]=sek5months[p:p+l]\n",
    "        Train_data[p,1,:]=chf5months[p:p+l]\n",
    "        Train_data[p,2,:]=cad5months[p:p+l]\n",
    "        Train_data[p,3,:]=gbp5months[p:p+l]\n",
    "        Train_data[p,4,:]=jpy5months[p:p+l]\n",
    "        Train_data[p,5,:]=eur5months[p:p+l]\n",
    "        Train_data[p,6,:]=aud5months[p:p+l]    \n",
    "    \n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((l,currencynum))\n",
    "    visual_conv = ConvolutionNetworks(20,5)(visual_scene)\n",
    "    tag = build_tag(visual_conv)\n",
    "    visual_conv = Concatenate()([visual_conv, tag])\n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    features= []\n",
    "    for k1 in range(w):\n",
    "        def get_feature(t):\n",
    "            return t[:, k1, :]\n",
    "        get_feature_layer = Lambda(get_feature)\n",
    "        features.append(get_feature_layer(visual_conv))\n",
    "\n",
    "    input2 = Input((16,))\n",
    "    onehot_encode = input2   \n",
    "\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode]))    \n",
    "\n",
    "\n",
    "    g_MLP = get_MLP(5, get_dense(5,MLP_unit))\n",
    "    f_MLP = get_MLP(5, get_dense(5,MLP_unit))\n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "\n",
    "    combined_relation = Add()(mid_relations)\n",
    "\n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "\n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "\n",
    "    model = Model(inputs=[visual_scene, input2, tag], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "    #model.summary()\n",
    "    print(z)\n",
    "    fit_show(Train_data,daynum[z],daynum[z+3],daynum[z+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
