{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##冰激淋+擔擔麵+蜂蜜檸檬適用\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #使用第0張顯卡 ##冰激淋+擔擔麵+蜂蜜檸檬適用 0or1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Flatten, Convolution1D, MaxPooling1D, Activation, BatchNormalization,\\\n",
    "Lambda, Concatenate, Add, Conv2D, Conv1D,TimeDistributed,MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基本設定\n",
    "\"\"\"\n",
    "#n=148335 #用2016 1~3月的資料 288x(31+29+31+30+31+30+...)=52416 210528-1(all) 差值在減一  \n",
    "l=36 #區間為12小時\n",
    "currency = [\"CHF\",\"CAD\",\"GBP\",\"JPY\",\"EUR\",\"HKD\"]\n",
    "#SEK:瑞典克朗  CHF:瑞士法郎 CAD:加拿大幣 GBP:英鎊 \n",
    "#currency = [\"BTC\",\"DASH\",\"ETH\",\"LTC\",\"JBY\",\"GBP\",\"EUR\",\"AUD\",\"US\"]\n",
    "currencynum = len(currency)\n",
    "month = [0,31,60,91,121,152,182,213,244,274,305,335,366,397,425,456,486,517,547,578,609,639,670,700,731] #2016是閏年 366天\n",
    "daynum = [0,5802,11745,18351,24380,30579,36861,42795,49201,55342,61230,67400,73732,79984,85726,92343,98127,104692,111028,117076,123700,129807,136083,142401,148335]\n",
    "question = [\"trand\",\"volatility\"]\n",
    "# 貨幣組合，1 : P, 0 : C\n",
    "M=0\n",
    "head = 9\n",
    "tail = 21\n",
    "epochs = 45\n",
    "batch_size = 4096\n",
    "pl=4\n",
    "kn=5\n",
    "all_cur_pair = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "if(M==0):\n",
    "    all_cur_pair_P = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "elif(M==1):\n",
    "    all_cur_pair_P = list(permutations(currency,2))# (Pn取2) 問題有先後順序時使用  \n",
    "all_question= list(permutations(question,1))\n",
    "np.set_printoptions(suppress=True)#不要用科學符號輸出\n",
    "lastepoch_train_acc = []\n",
    "lastepoch_test_acc = []\n",
    "lastepoch_train_loss = []\n",
    "lastepoch_test_loss = []\n",
    "total_test_vol = []\n",
    "total_test_trend = []\n",
    "train_length=3 \n",
    "test_length=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\n",
    "str, onehotcode, company code轉換\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "currencylist = {}\n",
    "questionlist = {}\n",
    "for i in range(len(currency)):\n",
    "    currencylist[i] = currency[i]\n",
    "\n",
    "for i in range(len(question)):\n",
    "    questionlist[i] = question[i]\n",
    "\n",
    "def str_to_currency(cur):\n",
    "    return {v: k for k, v in currencylist.items()}[cur]\n",
    "\n",
    "def str_to_question(q):\n",
    "    return {v: k for k, v in questionlist.items()}[q]\n",
    "\n",
    "\n",
    "def one_hot_currency(currencylist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(currencylist)))\n",
    "    for i in range(len(currencylist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "def one_hot_question(questionlist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(questionlist)))\n",
    "    for i in range(len(questionlist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "onehotcurrency = one_hot_currency(currencylist)\n",
    "onehotquestion = one_hot_question(questionlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Q&A\n",
    "\"\"\"\n",
    "vol='volatility'\n",
    "def set_question(com1, com2,typeq):\n",
    "    return np.concatenate((onehotquestion[str_to_question(typeq)],onehotcurrency[str_to_currency(com1)], onehotcurrency[str_to_currency(com2)]))\n",
    "\n",
    "qtype = ['big']\n",
    "HVqtype = ['big']\n",
    "\n",
    "\"\"\"比漲幅程度類問題\"\"\"\n",
    "def set_question_and_answer_pair(data, data2, n, all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_pair = {}\n",
    "    a_pair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    #data_sum=np.sum(data,axis=1)\n",
    "    data2_sum=np.sum(data2,axis=1)\n",
    "    outcome[0]=(data2_sum[0])\n",
    "    outcome[1]=(data2_sum[1])\n",
    "    outcome[2]=(data2_sum[2])\n",
    "    outcome[3]=(data2_sum[3])\n",
    "    outcome[4]=(data2_sum[4])\n",
    "    outcome[5]=(data2_sum[5])\n",
    "\n",
    "    tmp_q = []\n",
    "    tmp_a = []\n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1],all_question[0][0]))\n",
    "    q_pair[i] = tmp_q        \n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "            tmp_a.append(1)\n",
    "        else:\n",
    "            tmp_a.append(0)\n",
    "    a_pair[i] = tmp_a\n",
    "    return (data, q_pair, a_pair)\n",
    "\n",
    "\n",
    "\"\"\"History Volatility類問題\"\"\"\n",
    "def set_HVquestion_and_HVanswer_pair(data, data2,all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_HVpair = {}\n",
    "    a_HVpair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    data2_std=np.std(data2,axis=1) \n",
    "    outcome[0]=data2_std[0]\n",
    "    outcome[1]=data2_std[1]\n",
    "    outcome[2]=data2_std[2]\n",
    "    outcome[3]=data2_std[3]\n",
    "    outcome[4]=data2_std[4]\n",
    "    outcome[5]=data2_std[5]     \n",
    "\n",
    "    tmp_q = []\n",
    "    tmp_a = []   \n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1],all_question[1][0]))\n",
    "    q_HVpair[i] = tmp_q\n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "            tmp_a.append(1)\n",
    "        else:\n",
    "            tmp_a.append(0)\n",
    "                \n",
    "    a_HVpair[i] = tmp_a    \n",
    "    return (data, q_HVpair, a_HVpair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "此處開始寫rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(v.shape)\n",
    "    v = v.reshape(len(v),currencynum,l,1)\n",
    "    print(v.shape)\n",
    "    print(\"[Training model......]\")\n",
    "    \n",
    "    #Train_v=v[:]\n",
    "    \n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    history = model.fit([Train_v, Train_q], Train_a,validation_data=([Test_v,Test_q],Test_a),batch_size=batch_size ,epochs = epochs,shuffle=False)\n",
    "    pred = model.predict([Test_v, Test_q])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i] <= 0.5:\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "            \n",
    "        if pred[i] == Test_a[i]:\n",
    "            count+=1\n",
    "    print(count,count/pred.shape[0])\n",
    "    \"\"\"\n",
    "    分開兩種問題的test集\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    v_trend=[]\n",
    "    q_trend=[]\n",
    "    a_trend=[]\n",
    "    v_vol=[]\n",
    "    q_vol=[]\n",
    "    a_vol=[]\n",
    "    trend_count=0\n",
    "    vol_count=0\n",
    "    for ii in range(0,len(Test_q)):\n",
    "        if(flag==0):\n",
    "            #print(0)\n",
    "            v_trend.append(Test_v[ii])\n",
    "            q_trend.append(Test_q[ii])\n",
    "            a_trend.append(Test_a[ii])\n",
    "            trend_count=trend_count+1\n",
    "            if(trend_count==90):\n",
    "                trend_count=0\n",
    "                flag=1\n",
    "        elif(flag==1):\n",
    "            #print(1)\n",
    "            v_vol.append(Test_v[ii])\n",
    "            q_vol.append(Test_q[ii])\n",
    "            a_vol.append(Test_a[ii])\n",
    "            vol_count=vol_count+1\n",
    "            if(vol_count==90):\n",
    "                vol_count=0\n",
    "                flag=0\n",
    "    \n",
    "    \"\"\"\n",
    "    分開兩種問題的predict正確率\n",
    "    \"\"\"\n",
    "    pred_trend = model.predict([v_trend, q_trend])\n",
    "    count = 0\n",
    "    for i in range(pred_trend.shape[0]):\n",
    "        if pred_trend[i] <= 0.5:\n",
    "            pred_trend[i] = 0\n",
    "        else:\n",
    "            pred_trend[i] = 1\n",
    "            \n",
    "        if pred_trend[i] == a_trend[i]:\n",
    "            count+=1\n",
    "    print(\"trend_test_acc:\")\n",
    "    print(count,count/pred_trend.shape[0])    \n",
    "    total_test_trend.append(count/pred_trend.shape[0])\n",
    "\n",
    "    pred_vol = model.predict([v_vol, q_vol])\n",
    "    count = 0\n",
    "    for i in range(pred_vol.shape[0]):\n",
    "        if pred_vol[i] <= 0.5:\n",
    "            pred_vol[i] = 0\n",
    "        else:\n",
    "            pred_vol[i] = 1\n",
    "            \n",
    "        if pred_vol[i] == a_vol[i]:\n",
    "            count+=1\n",
    "    print(\"vol_test_acc:\")\n",
    "    print(count,count/pred_vol.shape[0]) \n",
    "    total_test_vol.append(count/pred_vol.shape[0])\n",
    "    \n",
    "    print(\"Reverse section--------------------------------------------------------------------------\")\n",
    "    Test_q_reverse=Test_q\n",
    "    temp=Test_q_reverse[:,2:8]\n",
    "    Test_q_reverse[:,2:8]=Test_q_reverse[:,8:14]\n",
    "    Test_q_reverse[:,8:14]=temp\n",
    "    Test_a_reverse=-1*Test_a+1\n",
    "    pred = model.predict([Test_v, Test_q_reverse])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i] <= 0.5:\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "            \n",
    "        if pred[i] == Test_a_reverse[i]:\n",
    "            count+=1\n",
    "    print(count,count/pred.shape[0])\n",
    "\n",
    "    \"\"\"\n",
    "    分開兩種問題的test集_reverse\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    v_trend=[]\n",
    "    q_trend=[]\n",
    "    a_trend=[]\n",
    "    v_vol=[]\n",
    "    q_vol=[]\n",
    "    a_vol=[]\n",
    "    trend_count=0\n",
    "    vol_count=0\n",
    "    cycle = ((currencynum*(currencynum-1))/2)*len(question)*len(qtype)\n",
    "    for ii in range(0,len(Test_q)):\n",
    "        if(flag==0):\n",
    "            #print(0)\n",
    "            v_trend.append(Test_v[ii])\n",
    "            q_trend.append(Test_q[ii])\n",
    "            a_trend.append(Test_a[ii])\n",
    "            trend_count=trend_count+1\n",
    "            if(trend_count==cycle):\n",
    "                trend_count=0\n",
    "                flag=1\n",
    "        elif(flag==1):\n",
    "            #print(1)\n",
    "            v_vol.append(Test_v[ii])\n",
    "            q_vol.append(Test_q[ii])\n",
    "            a_vol.append(Test_a[ii])\n",
    "            vol_count=vol_count+1\n",
    "            if(vol_count==cycle):\n",
    "                vol_count=0\n",
    "                flag=0\n",
    "    \n",
    "    \"\"\"\n",
    "    分開兩種問題的predict正確率_reverse\n",
    "    \"\"\"\n",
    "    pred_trend = model.predict([v_trend, q_trend])\n",
    "    count = 0\n",
    "    for i in range(pred_trend.shape[0]):\n",
    "        if pred_trend[i] <= 0.5:\n",
    "            pred_trend[i] = 0\n",
    "        else:\n",
    "            pred_trend[i] = 1\n",
    "            \n",
    "        if pred_trend[i] == a_trend[i]:\n",
    "            count+=1\n",
    "    print(\"trend_test_acc:\")\n",
    "    print(count,count/pred_trend.shape[0])    \n",
    "    total_test_trend.append(count/pred_trend.shape[0])\n",
    "\n",
    "    pred_vol = model.predict([v_vol, q_vol])\n",
    "    count = 0\n",
    "    for i in range(pred_vol.shape[0]):\n",
    "        if pred_vol[i] <= 0.5:\n",
    "            pred_vol[i] = 0\n",
    "        else:\n",
    "            pred_vol[i] = 1\n",
    "            \n",
    "        if pred_vol[i] == a_vol[i]:\n",
    "            count+=1\n",
    "    print(\"vol_test_acc:\")\n",
    "    print(count,count/pred_vol.shape[0]) \n",
    "    total_test_vol.append(count/pred_vol.shape[0])\n",
    "    \n",
    "    \"\"\"\n",
    "    benchmark1\n",
    "    \"\"\"\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]):\n",
    "        benchacc=benchacc+Test_a[i]\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"猜答案多的那邊 benchacc1:\")\n",
    "    print(benchacc)\n",
    "\n",
    "    \"\"\"\n",
    "    benchmark2\n",
    "    \"\"\"\n",
    "    #第一個直接猜1\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]-1):\n",
    "        if(Test_a[i]!=Test_a[i+1]):\n",
    "            benchacc=benchacc+1\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"參考前一個答案 benchacc2:\")\n",
    "    print(benchacc)    \n",
    "    \"\"\"\n",
    "    畫圖\n",
    "    \"\"\"\n",
    "    benchfunction=np.ones(a.shape[0])\n",
    "    benchfunction=benchfunction*benchacc\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    lastepoch_train_loss.append(history.history['loss'][-1])\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    lastepoch_test_loss.append(history.history['val_loss'][-1])\n",
    "    print(\"loss:\")\n",
    "    print(history.history['loss'][-1])\n",
    "    print(\"val_loss:\")\n",
    "    print(history.history['val_loss'][-1])\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"loss\") \n",
    "    #plt.title(\"The Title\") \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)\n",
    "    plt.show()    \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "12\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1202 MiB, count=2, average=601 MiB\n",
      "(729690, 36, 6)\n",
      "(729690, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 556170 samples, validate on 171360 samples\n",
      "Epoch 1/20\n",
      "556170/556170 [==============================] - 11s 19us/step - loss: 1.7906 - acc: 0.5025 - val_loss: 0.7128 - val_acc: 0.5242\n",
      "Epoch 2/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.9540 - acc: 0.5141 - val_loss: 0.6848 - val_acc: 0.5448\n",
      "Epoch 3/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.7619 - acc: 0.5297 - val_loss: 0.6762 - val_acc: 0.5517\n",
      "Epoch 4/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.7080 - acc: 0.5448 - val_loss: 0.6709 - val_acc: 0.5890\n",
      "Epoch 5/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6887 - acc: 0.5562 - val_loss: 0.6673 - val_acc: 0.5958\n",
      "Epoch 6/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6779 - acc: 0.5654 - val_loss: 0.6644 - val_acc: 0.5960\n",
      "Epoch 7/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6704 - acc: 0.5732 - val_loss: 0.6623 - val_acc: 0.5993\n",
      "Epoch 8/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6660 - acc: 0.5789 - val_loss: 0.6603 - val_acc: 0.5993\n",
      "Epoch 9/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6621 - acc: 0.5844 - val_loss: 0.6585 - val_acc: 0.5994\n",
      "Epoch 10/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6599 - acc: 0.5877 - val_loss: 0.6572 - val_acc: 0.5998\n",
      "Epoch 11/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6578 - acc: 0.5911 - val_loss: 0.6560 - val_acc: 0.6005\n",
      "Epoch 12/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6558 - acc: 0.5922 - val_loss: 0.6550 - val_acc: 0.6007\n",
      "Epoch 13/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6543 - acc: 0.5949 - val_loss: 0.6537 - val_acc: 0.6006\n",
      "Epoch 14/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6522 - acc: 0.5981 - val_loss: 0.6527 - val_acc: 0.6005\n",
      "Epoch 15/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6505 - acc: 0.5999 - val_loss: 0.6518 - val_acc: 0.6035\n",
      "Epoch 16/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6495 - acc: 0.6015 - val_loss: 0.6512 - val_acc: 0.6035\n",
      "Epoch 17/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6479 - acc: 0.6028 - val_loss: 0.6505 - val_acc: 0.6035\n",
      "Epoch 18/20\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6474 - acc: 0.6037 - val_loss: 0.6500 - val_acc: 0.6032\n",
      "Epoch 19/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6461 - acc: 0.6053 - val_loss: 0.6496 - val_acc: 0.6034\n",
      "Epoch 20/20\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6452 - acc: 0.6066 - val_loss: 0.6491 - val_acc: 0.6033\n",
      "[[0.49710953]\n",
      " [0.49808523]\n",
      " [0.47745633]\n",
      " ...\n",
      " [0.78150034]\n",
      " [0.71170723]\n",
      " [0.69766265]]\n",
      "103379 0.6032854808590102\n",
      "trend_test_acc:\n",
      "51675 0.6031162464985994\n",
      "vol_test_acc:\n",
      "51704 0.6034547152194211\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.488245  ]\n",
      " [0.50440997]\n",
      " [0.53784704]\n",
      " ...\n",
      " [0.7215919 ]\n",
      " [0.82023865]\n",
      " [0.82023865]]\n",
      "79479 0.46381302521008405\n",
      "trend_test_acc:\n",
      "45932 0.5360877684407096\n",
      "vol_test_acc:\n",
      "45949 0.5362861811391223\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5358835200746965\n",
      "參考前一個答案 benchacc2:\n",
      "0.6282738095238095\n",
      "loss:\n",
      "0.6452411678841039\n",
      "val_loss:\n",
      "0.6491432392296671\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "13\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1218 MiB, count=155, average=8046 KiB\n",
      "(739080, 36, 6)\n",
      "(739080, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 542130 samples, validate on 194790 samples\n",
      "Epoch 1/20\n",
      "542130/542130 [==============================] - 11s 20us/step - loss: 1.1562 - acc: 0.5073 - val_loss: 0.6942 - val_acc: 0.5770\n",
      "Epoch 2/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.8097 - acc: 0.5144 - val_loss: 0.6745 - val_acc: 0.5860\n",
      "Epoch 3/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.7300 - acc: 0.5280 - val_loss: 0.6697 - val_acc: 0.5812\n",
      "Epoch 4/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.7006 - acc: 0.5426 - val_loss: 0.6651 - val_acc: 0.5997\n",
      "Epoch 5/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6849 - acc: 0.5591 - val_loss: 0.6603 - val_acc: 0.5989\n",
      "Epoch 6/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6756 - acc: 0.5720 - val_loss: 0.6559 - val_acc: 0.6025\n",
      "Epoch 7/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6695 - acc: 0.5798 - val_loss: 0.6532 - val_acc: 0.6040\n",
      "Epoch 8/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6652 - acc: 0.5853 - val_loss: 0.6507 - val_acc: 0.6046\n",
      "Epoch 9/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6620 - acc: 0.5901 - val_loss: 0.6490 - val_acc: 0.6049\n",
      "Epoch 10/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6592 - acc: 0.5927 - val_loss: 0.6479 - val_acc: 0.6047\n",
      "Epoch 11/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6574 - acc: 0.5946 - val_loss: 0.6469 - val_acc: 0.6050\n",
      "Epoch 12/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6553 - acc: 0.5962 - val_loss: 0.6458 - val_acc: 0.6050\n",
      "Epoch 13/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6536 - acc: 0.5990 - val_loss: 0.6450 - val_acc: 0.6050\n",
      "Epoch 14/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6513 - acc: 0.6005 - val_loss: 0.6443 - val_acc: 0.6071\n",
      "Epoch 15/20\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6503 - acc: 0.6015 - val_loss: 0.6438 - val_acc: 0.6073\n",
      "Epoch 16/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6494 - acc: 0.6007 - val_loss: 0.6430 - val_acc: 0.6077\n",
      "Epoch 17/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6480 - acc: 0.6021 - val_loss: 0.6424 - val_acc: 0.6071\n",
      "Epoch 18/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6472 - acc: 0.6032 - val_loss: 0.6421 - val_acc: 0.6064\n",
      "Epoch 19/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6461 - acc: 0.6039 - val_loss: 0.6417 - val_acc: 0.6065\n",
      "Epoch 20/20\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6452 - acc: 0.6042 - val_loss: 0.6417 - val_acc: 0.6059\n",
      "[[0.50217634]\n",
      " [0.5095824 ]\n",
      " [0.49569824]\n",
      " ...\n",
      " [0.78072673]\n",
      " [0.73238295]\n",
      " [0.7124717 ]]\n",
      "118018 0.6058729914266646\n",
      "trend_test_acc:\n",
      "59026 0.6059542141463915\n",
      "vol_test_acc:\n",
      "58992 0.6057917436845348\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.4861826 ]\n",
      " [0.52943116]\n",
      " [0.5197328 ]\n",
      " ...\n",
      " [0.587053  ]\n",
      " [0.77278775]\n",
      " [0.77278775]]\n",
      "88346 0.4535448431644335\n",
      "trend_test_acc:\n",
      "53260 0.5467611128220922\n",
      "vol_test_acc:\n",
      "53184 0.5461491065927295\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5399917860259766\n",
      "參考前一個答案 benchacc2:\n",
      "0.6292776836593255\n",
      "loss:\n",
      "0.6451671754410343\n",
      "val_loss:\n",
      "0.6416556111191863\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "14\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1247 MiB, count=157, average=8136 KiB\n",
      "(756900, 36, 6)\n",
      "(756900, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 566820 samples, validate on 187920 samples\n",
      "Epoch 1/20\n",
      "566820/566820 [==============================] - 12s 20us/step - loss: 1.3703 - acc: 0.5120 - val_loss: 0.7022 - val_acc: 0.5381\n",
      "Epoch 2/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.8287 - acc: 0.5214 - val_loss: 0.6896 - val_acc: 0.5326\n",
      "Epoch 3/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.7318 - acc: 0.5338 - val_loss: 0.6821 - val_acc: 0.5477\n",
      "Epoch 4/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.7015 - acc: 0.5426 - val_loss: 0.6773 - val_acc: 0.5627\n",
      "Epoch 5/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6867 - acc: 0.5521 - val_loss: 0.6725 - val_acc: 0.5857\n",
      "Epoch 6/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6776 - acc: 0.5607 - val_loss: 0.6679 - val_acc: 0.5889\n",
      "Epoch 7/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6705 - acc: 0.5685 - val_loss: 0.6636 - val_acc: 0.5857\n",
      "Epoch 8/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6648 - acc: 0.5759 - val_loss: 0.6600 - val_acc: 0.5853\n",
      "Epoch 9/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6598 - acc: 0.5830 - val_loss: 0.6573 - val_acc: 0.5963\n",
      "Epoch 10/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6563 - acc: 0.5891 - val_loss: 0.6549 - val_acc: 0.5963\n",
      "Epoch 11/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6538 - acc: 0.5929 - val_loss: 0.6535 - val_acc: 0.5962\n",
      "Epoch 12/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6514 - acc: 0.5959 - val_loss: 0.6521 - val_acc: 0.5963\n",
      "Epoch 13/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6497 - acc: 0.5991 - val_loss: 0.6512 - val_acc: 0.5964\n",
      "Epoch 14/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6480 - acc: 0.6012 - val_loss: 0.6502 - val_acc: 0.5965\n",
      "Epoch 15/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6470 - acc: 0.6023 - val_loss: 0.6495 - val_acc: 0.5965\n",
      "Epoch 16/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6460 - acc: 0.6035 - val_loss: 0.6491 - val_acc: 0.5993\n",
      "Epoch 17/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6455 - acc: 0.6060 - val_loss: 0.6486 - val_acc: 0.5994\n",
      "Epoch 18/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6440 - acc: 0.6056 - val_loss: 0.6481 - val_acc: 0.5994\n",
      "Epoch 19/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6435 - acc: 0.6074 - val_loss: 0.6478 - val_acc: 0.5995\n",
      "Epoch 20/20\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6428 - acc: 0.6081 - val_loss: 0.6475 - val_acc: 0.5994\n",
      "[[0.49935043]\n",
      " [0.49524018]\n",
      " [0.48185596]\n",
      " ...\n",
      " [0.78220624]\n",
      " [0.70745647]\n",
      " [0.70364624]]\n",
      "112648 0.5994465730097914\n",
      "trend_test_acc:\n",
      "56352 0.5997445721583653\n",
      "vol_test_acc:\n",
      "56296 0.5991485738612176\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.47704515]\n",
      " [0.50645745]\n",
      " [0.46293837]\n",
      " ...\n",
      " [0.6296104 ]\n",
      " [0.616183  ]\n",
      " [0.616183  ]]\n",
      "83981 0.4468976160068114\n",
      "trend_test_acc:\n",
      "52041 0.5538633461047254\n",
      "vol_test_acc:\n",
      "51898 0.5523414218816518\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5425925925925926\n",
      "參考前一個答案 benchacc2:\n",
      "0.6291773094934014\n",
      "loss:\n",
      "0.6427746697385933\n",
      "val_loss:\n",
      "0.6474841715174058\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "15\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1219 MiB, count=158, average=7902 KiB\n",
      "(739830, 36, 6)\n",
      "(739830, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 558390 samples, validate on 179280 samples\n",
      "Epoch 1/20\n",
      "558390/558390 [==============================] - 12s 21us/step - loss: 1.0730 - acc: 0.5091 - val_loss: 0.7086 - val_acc: 0.4960\n",
      "Epoch 2/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.7629 - acc: 0.5263 - val_loss: 0.6819 - val_acc: 0.5797\n",
      "Epoch 3/20\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.7058 - acc: 0.5460 - val_loss: 0.6732 - val_acc: 0.5816\n",
      "Epoch 4/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6852 - acc: 0.5599 - val_loss: 0.6646 - val_acc: 0.5833\n",
      "Epoch 5/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6736 - acc: 0.5696 - val_loss: 0.6589 - val_acc: 0.5962\n",
      "Epoch 6/20\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.6648 - acc: 0.5770 - val_loss: 0.6548 - val_acc: 0.5998\n",
      "Epoch 7/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6591 - acc: 0.5831 - val_loss: 0.6522 - val_acc: 0.6004\n",
      "Epoch 8/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6555 - acc: 0.5878 - val_loss: 0.6503 - val_acc: 0.6005\n",
      "Epoch 9/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6527 - acc: 0.5906 - val_loss: 0.6489 - val_acc: 0.6006\n",
      "Epoch 10/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6501 - acc: 0.5947 - val_loss: 0.6480 - val_acc: 0.6005\n",
      "Epoch 11/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6484 - acc: 0.5960 - val_loss: 0.6472 - val_acc: 0.6008\n",
      "Epoch 12/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6471 - acc: 0.5985 - val_loss: 0.6464 - val_acc: 0.6008\n",
      "Epoch 13/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6457 - acc: 0.5996 - val_loss: 0.6458 - val_acc: 0.6008\n",
      "Epoch 14/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6445 - acc: 0.6005 - val_loss: 0.6453 - val_acc: 0.6008\n",
      "Epoch 15/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6434 - acc: 0.6028 - val_loss: 0.6448 - val_acc: 0.6010\n",
      "Epoch 16/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6419 - acc: 0.6038 - val_loss: 0.6443 - val_acc: 0.6010\n",
      "Epoch 17/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6408 - acc: 0.6047 - val_loss: 0.6443 - val_acc: 0.6009\n",
      "Epoch 18/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6399 - acc: 0.6043 - val_loss: 0.6447 - val_acc: 0.6004\n",
      "Epoch 19/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6377 - acc: 0.6045 - val_loss: 0.6469 - val_acc: 0.5996\n",
      "Epoch 20/20\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6362 - acc: 0.6038 - val_loss: 0.6484 - val_acc: 0.5989\n",
      "[[0.49554965]\n",
      " [0.4934631 ]\n",
      " [0.47542006]\n",
      " ...\n",
      " [0.791718  ]\n",
      " [0.731934  ]\n",
      " [0.74240696]]\n",
      "107374 0.5989178937974119\n",
      "trend_test_acc:\n",
      "53740 0.5995091477019188\n",
      "vol_test_acc:\n",
      "53634 0.5983266398929049\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.45889896]\n",
      " [0.44640386]\n",
      " [0.45294538]\n",
      " ...\n",
      " [0.5335467 ]\n",
      " [0.72834927]\n",
      " [0.72834927]]\n",
      "71693 0.39989402052655065\n",
      "trend_test_acc:\n",
      "53741 0.5995203034359661\n",
      "vol_test_acc:\n",
      "53846 0.6006916555109326\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5371541722445337\n",
      "參考前一個答案 benchacc2:\n",
      "0.628380187416332\n",
      "loss:\n",
      "0.6361712535542603\n",
      "val_loss:\n",
      "0.6484429217087058\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "16\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1261 MiB, count=158, average=8171 KiB\n",
      "(765030, 36, 6)\n",
      "(765030, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 566310 samples, validate on 196560 samples\n",
      "Epoch 1/20\n",
      "566310/566310 [==============================] - 12s 22us/step - loss: 1.1492 - acc: 0.5122 - val_loss: 0.6893 - val_acc: 0.5622\n",
      "Epoch 2/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.7890 - acc: 0.5246 - val_loss: 0.6778 - val_acc: 0.5690\n",
      "Epoch 3/20\n",
      "566310/566310 [==============================] - 5s 8us/step - loss: 0.7195 - acc: 0.5400 - val_loss: 0.6715 - val_acc: 0.5703\n",
      "Epoch 4/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6940 - acc: 0.5521 - val_loss: 0.6667 - val_acc: 0.5712\n",
      "Epoch 5/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6802 - acc: 0.5639 - val_loss: 0.6628 - val_acc: 0.5817\n",
      "Epoch 6/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6710 - acc: 0.5740 - val_loss: 0.6596 - val_acc: 0.5925\n",
      "Epoch 7/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6644 - acc: 0.5816 - val_loss: 0.6572 - val_acc: 0.5964\n",
      "Epoch 8/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6598 - acc: 0.5872 - val_loss: 0.6556 - val_acc: 0.5966\n",
      "Epoch 9/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6569 - acc: 0.5915 - val_loss: 0.6545 - val_acc: 0.5967\n",
      "Epoch 10/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6542 - acc: 0.5955 - val_loss: 0.6533 - val_acc: 0.5966\n",
      "Epoch 11/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6522 - acc: 0.5973 - val_loss: 0.6524 - val_acc: 0.5970\n",
      "Epoch 12/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6499 - acc: 0.5992 - val_loss: 0.6516 - val_acc: 0.5974\n",
      "Epoch 13/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6483 - acc: 0.6011 - val_loss: 0.6510 - val_acc: 0.5973\n",
      "Epoch 14/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6459 - acc: 0.6031 - val_loss: 0.6505 - val_acc: 0.5968\n",
      "Epoch 15/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6443 - acc: 0.6041 - val_loss: 0.6504 - val_acc: 0.5954\n",
      "Epoch 16/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6427 - acc: 0.6047 - val_loss: 0.6506 - val_acc: 0.5953\n",
      "Epoch 17/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6399 - acc: 0.6058 - val_loss: 0.6518 - val_acc: 0.5947\n",
      "Epoch 18/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6376 - acc: 0.6069 - val_loss: 0.6548 - val_acc: 0.5938\n",
      "Epoch 19/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6360 - acc: 0.6086 - val_loss: 0.6562 - val_acc: 0.5940\n",
      "Epoch 20/20\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6335 - acc: 0.6088 - val_loss: 0.6584 - val_acc: 0.5880\n",
      "[[0.4886115 ]\n",
      " [0.50345415]\n",
      " [0.4745663 ]\n",
      " ...\n",
      " [0.8413784 ]\n",
      " [0.84330195]\n",
      " [0.8243831 ]]\n",
      "115573 0.5879782254782254\n",
      "trend_test_acc:\n",
      "57826 0.5883801383801384\n",
      "vol_test_acc:\n",
      "57747 0.5875763125763126\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.5036923 ]\n",
      " [0.46806857]\n",
      " [0.46132234]\n",
      " ...\n",
      " [0.60588217]\n",
      " [0.56729025]\n",
      " [0.56729025]]\n",
      "83516 0.4248880748880749\n",
      "trend_test_acc:\n",
      "56526 0.5751526251526251\n",
      "vol_test_acc:\n",
      "56518 0.5750712250712251\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5424501424501424\n",
      "參考前一個答案 benchacc2:\n",
      "0.6272792022792022\n",
      "loss:\n",
      "0.6335236196130116\n",
      "val_loss:\n",
      "0.6584347294703411\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for z in range(head,tail,1):\n",
    "    \"\"\"\n",
    "     V\n",
    "    \"\"\"\n",
    "    n=daynum[tail]-daynum[head]\n",
    "    df = pd.read_csv('data/JPY_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    print(df.shape)  \n",
    "    jpy5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        jpy5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/EUR_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eur5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eur5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/CHF_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    chf5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        chf5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/CAD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    cad5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        cad5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/GBP_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    gbp5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        gbp5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/HKD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    hkd5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        hkd5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    print('finish dataread')\n",
    "    Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "    for p in range(n-l+1):\n",
    "        Train_data[p,0,:]=chf5months[p:p+l]\n",
    "        Train_data[p,1,:]=cad5months[p:p+l]\n",
    "        Train_data[p,2,:]=gbp5months[p:p+l]\n",
    "        Train_data[p,3,:]=jpy5months[p:p+l]\n",
    "        Train_data[p,4,:]=eur5months[p:p+l]\n",
    "        Train_data[p,5,:]=hkd5months[p:p+l]\n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((currencynum,l,1))\n",
    "    visual_conv = ConvolutionNetworks([20,10],[(1,kn),(1,kn)])(visual_scene)\n",
    "    print(K.int_shape(visual_conv))\n",
    "    tag = build_tag(visual_conv)\n",
    "    visual_conv = Concatenate()([tag, visual_conv])\n",
    "    print(K.int_shape(visual_conv))\n",
    "    \n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    f = shapes[2]\n",
    "    features= []\n",
    "    #features = np.zeros(0)\n",
    "    for k1 in range(w):\n",
    "        for k2 in range(f):\n",
    "            def get_feature(t):\n",
    "                return t[:, k1, k2, :]\n",
    "            #get_feature_layer = Lambda(get_feature)\n",
    "            features.append(Lambda(get_feature)(visual_conv))\n",
    "    \n",
    "      \n",
    "    input2 = Input((14,))\n",
    "    onehot_encode_question = input2\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode_question]))    \n",
    "    \n",
    "     \n",
    "    g_MLP = get_MLP(4, get_dense(4,MLP_unit))\n",
    "    \n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "    \n",
    "    combined_relation = Add()(mid_relations)\n",
    "    \n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "    \n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "    \n",
    "    \n",
    "    #model = Model(inputs=[visual_scene])\n",
    "    model = Model(inputs=[visual_scene, input2, tag], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "    #model.summary()\n",
    "    print(z)\n",
    "    fit_show(Train_data,daynum[z],daynum[z+3],daynum[z+4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 6, 36, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 6, 32, 20)    120         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 6, 8, 20)     0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 6, 8, 20)     80          max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 6, 4, 10)     1010        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 6, 1, 10)     0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 6, 1, 7)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 6, 1, 10)     40          max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 6, 1, 17)     0           input_14[0][0]                   \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 48)           0           lambda_25[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64)           3136        concatenate_10[0][0]             \n",
      "                                                                 concatenate_10[1][0]             \n",
      "                                                                 concatenate_10[2][0]             \n",
      "                                                                 concatenate_10[3][0]             \n",
      "                                                                 concatenate_10[4][0]             \n",
      "                                                                 concatenate_10[5][0]             \n",
      "                                                                 concatenate_10[6][0]             \n",
      "                                                                 concatenate_10[7][0]             \n",
      "                                                                 concatenate_10[8][0]             \n",
      "                                                                 concatenate_10[9][0]             \n",
      "                                                                 concatenate_10[10][0]            \n",
      "                                                                 concatenate_10[11][0]            \n",
      "                                                                 concatenate_10[12][0]            \n",
      "                                                                 concatenate_10[13][0]            \n",
      "                                                                 concatenate_10[14][0]            \n",
      "                                                                 concatenate_10[15][0]            \n",
      "                                                                 concatenate_10[16][0]            \n",
      "                                                                 concatenate_10[17][0]            \n",
      "                                                                 concatenate_10[18][0]            \n",
      "                                                                 concatenate_10[19][0]            \n",
      "                                                                 concatenate_10[20][0]            \n",
      "                                                                 concatenate_10[21][0]            \n",
      "                                                                 concatenate_10[22][0]            \n",
      "                                                                 concatenate_10[23][0]            \n",
      "                                                                 concatenate_10[24][0]            \n",
      "                                                                 concatenate_10[25][0]            \n",
      "                                                                 concatenate_10[26][0]            \n",
      "                                                                 concatenate_10[27][0]            \n",
      "                                                                 concatenate_10[28][0]            \n",
      "                                                                 concatenate_10[29][0]            \n",
      "                                                                 concatenate_10[30][0]            \n",
      "                                                                 concatenate_10[31][0]            \n",
      "                                                                 concatenate_10[32][0]            \n",
      "                                                                 concatenate_10[33][0]            \n",
      "                                                                 concatenate_10[34][0]            \n",
      "                                                                 concatenate_10[35][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 64)           4160        dense_29[0][0]                   \n",
      "                                                                 dense_29[1][0]                   \n",
      "                                                                 dense_29[2][0]                   \n",
      "                                                                 dense_29[3][0]                   \n",
      "                                                                 dense_29[4][0]                   \n",
      "                                                                 dense_29[5][0]                   \n",
      "                                                                 dense_29[6][0]                   \n",
      "                                                                 dense_29[7][0]                   \n",
      "                                                                 dense_29[8][0]                   \n",
      "                                                                 dense_29[9][0]                   \n",
      "                                                                 dense_29[10][0]                  \n",
      "                                                                 dense_29[11][0]                  \n",
      "                                                                 dense_29[12][0]                  \n",
      "                                                                 dense_29[13][0]                  \n",
      "                                                                 dense_29[14][0]                  \n",
      "                                                                 dense_29[15][0]                  \n",
      "                                                                 dense_29[16][0]                  \n",
      "                                                                 dense_29[17][0]                  \n",
      "                                                                 dense_29[18][0]                  \n",
      "                                                                 dense_29[19][0]                  \n",
      "                                                                 dense_29[20][0]                  \n",
      "                                                                 dense_29[21][0]                  \n",
      "                                                                 dense_29[22][0]                  \n",
      "                                                                 dense_29[23][0]                  \n",
      "                                                                 dense_29[24][0]                  \n",
      "                                                                 dense_29[25][0]                  \n",
      "                                                                 dense_29[26][0]                  \n",
      "                                                                 dense_29[27][0]                  \n",
      "                                                                 dense_29[28][0]                  \n",
      "                                                                 dense_29[29][0]                  \n",
      "                                                                 dense_29[30][0]                  \n",
      "                                                                 dense_29[31][0]                  \n",
      "                                                                 dense_29[32][0]                  \n",
      "                                                                 dense_29[33][0]                  \n",
      "                                                                 dense_29[34][0]                  \n",
      "                                                                 dense_29[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 64)           4160        dense_30[0][0]                   \n",
      "                                                                 dense_30[1][0]                   \n",
      "                                                                 dense_30[2][0]                   \n",
      "                                                                 dense_30[3][0]                   \n",
      "                                                                 dense_30[4][0]                   \n",
      "                                                                 dense_30[5][0]                   \n",
      "                                                                 dense_30[6][0]                   \n",
      "                                                                 dense_30[7][0]                   \n",
      "                                                                 dense_30[8][0]                   \n",
      "                                                                 dense_30[9][0]                   \n",
      "                                                                 dense_30[10][0]                  \n",
      "                                                                 dense_30[11][0]                  \n",
      "                                                                 dense_30[12][0]                  \n",
      "                                                                 dense_30[13][0]                  \n",
      "                                                                 dense_30[14][0]                  \n",
      "                                                                 dense_30[15][0]                  \n",
      "                                                                 dense_30[16][0]                  \n",
      "                                                                 dense_30[17][0]                  \n",
      "                                                                 dense_30[18][0]                  \n",
      "                                                                 dense_30[19][0]                  \n",
      "                                                                 dense_30[20][0]                  \n",
      "                                                                 dense_30[21][0]                  \n",
      "                                                                 dense_30[22][0]                  \n",
      "                                                                 dense_30[23][0]                  \n",
      "                                                                 dense_30[24][0]                  \n",
      "                                                                 dense_30[25][0]                  \n",
      "                                                                 dense_30[26][0]                  \n",
      "                                                                 dense_30[27][0]                  \n",
      "                                                                 dense_30[28][0]                  \n",
      "                                                                 dense_30[29][0]                  \n",
      "                                                                 dense_30[30][0]                  \n",
      "                                                                 dense_30[31][0]                  \n",
      "                                                                 dense_30[32][0]                  \n",
      "                                                                 dense_30[33][0]                  \n",
      "                                                                 dense_30[34][0]                  \n",
      "                                                                 dense_30[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 64)           4160        dense_31[0][0]                   \n",
      "                                                                 dense_31[1][0]                   \n",
      "                                                                 dense_31[2][0]                   \n",
      "                                                                 dense_31[3][0]                   \n",
      "                                                                 dense_31[4][0]                   \n",
      "                                                                 dense_31[5][0]                   \n",
      "                                                                 dense_31[6][0]                   \n",
      "                                                                 dense_31[7][0]                   \n",
      "                                                                 dense_31[8][0]                   \n",
      "                                                                 dense_31[9][0]                   \n",
      "                                                                 dense_31[10][0]                  \n",
      "                                                                 dense_31[11][0]                  \n",
      "                                                                 dense_31[12][0]                  \n",
      "                                                                 dense_31[13][0]                  \n",
      "                                                                 dense_31[14][0]                  \n",
      "                                                                 dense_31[15][0]                  \n",
      "                                                                 dense_31[16][0]                  \n",
      "                                                                 dense_31[17][0]                  \n",
      "                                                                 dense_31[18][0]                  \n",
      "                                                                 dense_31[19][0]                  \n",
      "                                                                 dense_31[20][0]                  \n",
      "                                                                 dense_31[21][0]                  \n",
      "                                                                 dense_31[22][0]                  \n",
      "                                                                 dense_31[23][0]                  \n",
      "                                                                 dense_31[24][0]                  \n",
      "                                                                 dense_31[25][0]                  \n",
      "                                                                 dense_31[26][0]                  \n",
      "                                                                 dense_31[27][0]                  \n",
      "                                                                 dense_31[28][0]                  \n",
      "                                                                 dense_31[29][0]                  \n",
      "                                                                 dense_31[30][0]                  \n",
      "                                                                 dense_31[31][0]                  \n",
      "                                                                 dense_31[32][0]                  \n",
      "                                                                 dense_31[33][0]                  \n",
      "                                                                 dense_31[34][0]                  \n",
      "                                                                 dense_31[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64)           0           dense_32[0][0]                   \n",
      "                                                                 dense_32[1][0]                   \n",
      "                                                                 dense_32[2][0]                   \n",
      "                                                                 dense_32[3][0]                   \n",
      "                                                                 dense_32[4][0]                   \n",
      "                                                                 dense_32[5][0]                   \n",
      "                                                                 dense_32[6][0]                   \n",
      "                                                                 dense_32[7][0]                   \n",
      "                                                                 dense_32[8][0]                   \n",
      "                                                                 dense_32[9][0]                   \n",
      "                                                                 dense_32[10][0]                  \n",
      "                                                                 dense_32[11][0]                  \n",
      "                                                                 dense_32[12][0]                  \n",
      "                                                                 dense_32[13][0]                  \n",
      "                                                                 dense_32[14][0]                  \n",
      "                                                                 dense_32[15][0]                  \n",
      "                                                                 dense_32[16][0]                  \n",
      "                                                                 dense_32[17][0]                  \n",
      "                                                                 dense_32[18][0]                  \n",
      "                                                                 dense_32[19][0]                  \n",
      "                                                                 dense_32[20][0]                  \n",
      "                                                                 dense_32[21][0]                  \n",
      "                                                                 dense_32[22][0]                  \n",
      "                                                                 dense_32[23][0]                  \n",
      "                                                                 dense_32[24][0]                  \n",
      "                                                                 dense_32[25][0]                  \n",
      "                                                                 dense_32[26][0]                  \n",
      "                                                                 dense_32[27][0]                  \n",
      "                                                                 dense_32[28][0]                  \n",
      "                                                                 dense_32[29][0]                  \n",
      "                                                                 dense_32[30][0]                  \n",
      "                                                                 dense_32[31][0]                  \n",
      "                                                                 dense_32[32][0]                  \n",
      "                                                                 dense_32[33][0]                  \n",
      "                                                                 dense_32[34][0]                  \n",
      "                                                                 dense_32[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 64)           4160        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64)           0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64)           0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 64)           4160        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64)           0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 64)           0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            65          activation_10[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 25,251\n",
      "Trainable params: 25,191\n",
      "Non-trainable params: 60\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "此處開始寫rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(v.shape)\n",
    "    v = v.reshape(len(v),currencynum,l,1)\n",
    "    print(v.shape)\n",
    "    print(\"[Training model......]\")\n",
    "    \n",
    "    #Train_v=v[:]\n",
    "    \n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    history = model.fit([Train_v, Train_q], Train_a,validation_data=([Test_v,Test_q],Test_a),batch_size=batch_size ,epochs = epochs,shuffle=False)\n",
    "    pred = model.predict([Test_v, Test_q])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i] <= 0.5:\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "            \n",
    "        if pred[i] == Test_a[i]:\n",
    "            count+=1\n",
    "    print(count,count/pred.shape[0])\n",
    "    \"\"\"\n",
    "    分開兩種問題的test集\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    v_trend=[]\n",
    "    q_trend=[]\n",
    "    a_trend=[]\n",
    "    v_vol=[]\n",
    "    q_vol=[]\n",
    "    a_vol=[]\n",
    "    trend_count=0\n",
    "    vol_count=0\n",
    "    for ii in range(0,len(Test_q)):\n",
    "        if(flag==0):\n",
    "            #print(0)\n",
    "            v_trend.append(Test_v[ii])\n",
    "            q_trend.append(Test_q[ii])\n",
    "            a_trend.append(Test_a[ii])\n",
    "            trend_count=trend_count+1\n",
    "            if(trend_count==90):\n",
    "                trend_count=0\n",
    "                flag=1\n",
    "        elif(flag==1):\n",
    "            #print(1)\n",
    "            v_vol.append(Test_v[ii])\n",
    "            q_vol.append(Test_q[ii])\n",
    "            a_vol.append(Test_a[ii])\n",
    "            vol_count=vol_count+1\n",
    "            if(vol_count==90):\n",
    "                vol_count=0\n",
    "                flag=0\n",
    "    \n",
    "    \"\"\"\n",
    "    分開兩種問題的predict正確率\n",
    "    \"\"\"\n",
    "    pred_trend = model.predict([v_trend, q_trend])\n",
    "    count = 0\n",
    "    for i in range(pred_trend.shape[0]):\n",
    "        if pred_trend[i] <= 0.5:\n",
    "            pred_trend[i] = 0\n",
    "        else:\n",
    "            pred_trend[i] = 1\n",
    "            \n",
    "        if pred_trend[i] == a_trend[i]:\n",
    "            count+=1\n",
    "    print(\"trend_test_acc:\")\n",
    "    print(count,count/pred_trend.shape[0])    \n",
    "    total_test_trend.append(count/pred_trend.shape[0])\n",
    "\n",
    "    pred_vol = model.predict([v_vol, q_vol])\n",
    "    count = 0\n",
    "    for i in range(pred_vol.shape[0]):\n",
    "        if pred_vol[i] <= 0.5:\n",
    "            pred_vol[i] = 0\n",
    "        else:\n",
    "            pred_vol[i] = 1\n",
    "            \n",
    "        if pred_vol[i] == a_vol[i]:\n",
    "            count+=1\n",
    "    print(\"vol_test_acc:\")\n",
    "    print(count,count/pred_vol.shape[0]) \n",
    "    total_test_vol.append(count/pred_vol.shape[0])\n",
    "    \n",
    "    print(\"Reverse section--------------------------------------------------------------------------\")\n",
    "    Test_q_reverse=Test_q\n",
    "    temp=Test_q_reverse[:,2:8]\n",
    "    Test_q_reverse[:,2:8]=Test_q_reverse[:,8:14]\n",
    "    Test_q_reverse[:,8:14]=temp\n",
    "    Test_a_reverse=-1*Test_a+1\n",
    "    pred = model.predict([Test_v, Test_q_reverse])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "\n",
    "    \"\"\"\n",
    "    benchmark1\n",
    "    \"\"\"\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]):\n",
    "        benchacc=benchacc+Test_a[i]\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"猜答案多的那邊 benchacc1:\")\n",
    "    print(benchacc)\n",
    "\n",
    "    \"\"\"\n",
    "    benchmark2\n",
    "    \"\"\"\n",
    "    #第一個直接猜1\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]-1):\n",
    "        if(Test_a[i]!=Test_a[i+1]):\n",
    "            benchacc=benchacc+1\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"參考前一個答案 benchacc2:\")\n",
    "    print(benchacc)    \n",
    "\n",
    "    \n",
    "        \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)\n",
    "    plt.show()    \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "9\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1215 MiB, count=2, average=607 MiB\n",
      "(737100, 36, 6)\n",
      "(737100, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 549540 samples, validate on 185400 samples\n",
      "Epoch 1/45\n",
      "549540/549540 [==============================] - 10s 19us/step - loss: 1.1938 - acc: 0.5123 - val_loss: 0.7159 - val_acc: 0.5257\n",
      "Epoch 2/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.8004 - acc: 0.5110 - val_loss: 0.6938 - val_acc: 0.5411\n",
      "Epoch 3/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.7241 - acc: 0.5212 - val_loss: 0.6855 - val_acc: 0.5592\n",
      "Epoch 4/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6997 - acc: 0.5348 - val_loss: 0.6793 - val_acc: 0.5682\n",
      "Epoch 5/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6878 - acc: 0.5478 - val_loss: 0.6738 - val_acc: 0.5787\n",
      "Epoch 6/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6800 - acc: 0.5582 - val_loss: 0.6677 - val_acc: 0.5850\n",
      "Epoch 7/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6741 - acc: 0.5666 - val_loss: 0.6624 - val_acc: 0.5937\n",
      "Epoch 8/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6693 - acc: 0.5711 - val_loss: 0.6580 - val_acc: 0.5997\n",
      "Epoch 9/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6659 - acc: 0.5760 - val_loss: 0.6549 - val_acc: 0.6003\n",
      "Epoch 10/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6631 - acc: 0.5781 - val_loss: 0.6525 - val_acc: 0.6028\n",
      "Epoch 11/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6606 - acc: 0.5811 - val_loss: 0.6505 - val_acc: 0.6059\n",
      "Epoch 12/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6584 - acc: 0.5844 - val_loss: 0.6491 - val_acc: 0.6075\n",
      "Epoch 13/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6565 - acc: 0.5858 - val_loss: 0.6479 - val_acc: 0.6004\n",
      "Epoch 14/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6550 - acc: 0.5891 - val_loss: 0.6468 - val_acc: 0.6094\n",
      "Epoch 15/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6539 - acc: 0.5897 - val_loss: 0.6461 - val_acc: 0.6102\n",
      "Epoch 16/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6528 - acc: 0.5909 - val_loss: 0.6453 - val_acc: 0.6024\n",
      "Epoch 17/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6517 - acc: 0.5910 - val_loss: 0.6448 - val_acc: 0.6112\n",
      "Epoch 18/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6506 - acc: 0.5918 - val_loss: 0.6442 - val_acc: 0.6113\n",
      "Epoch 19/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6496 - acc: 0.5934 - val_loss: 0.6445 - val_acc: 0.6021\n",
      "Epoch 20/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6487 - acc: 0.5926 - val_loss: 0.6448 - val_acc: 0.6100\n",
      "Epoch 21/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6472 - acc: 0.5938 - val_loss: 0.6450 - val_acc: 0.6100\n",
      "Epoch 22/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6463 - acc: 0.5946 - val_loss: 0.6456 - val_acc: 0.6088\n",
      "Epoch 23/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6453 - acc: 0.5955 - val_loss: 0.6459 - val_acc: 0.6081\n",
      "Epoch 24/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6439 - acc: 0.5957 - val_loss: 0.6464 - val_acc: 0.6007\n",
      "Epoch 25/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6428 - acc: 0.5958 - val_loss: 0.6467 - val_acc: 0.6009\n",
      "Epoch 26/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6414 - acc: 0.5965 - val_loss: 0.6472 - val_acc: 0.6067\n",
      "Epoch 27/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6405 - acc: 0.5972 - val_loss: 0.6476 - val_acc: 0.5993\n",
      "Epoch 28/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6387 - acc: 0.5993 - val_loss: 0.6484 - val_acc: 0.5991\n",
      "Epoch 29/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6376 - acc: 0.5994 - val_loss: 0.6486 - val_acc: 0.5985\n",
      "Epoch 30/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6359 - acc: 0.6008 - val_loss: 0.6490 - val_acc: 0.5995\n",
      "Epoch 31/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6348 - acc: 0.6015 - val_loss: 0.6490 - val_acc: 0.6000\n",
      "Epoch 32/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6339 - acc: 0.6016 - val_loss: 0.6496 - val_acc: 0.6006\n",
      "Epoch 33/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6319 - acc: 0.6025 - val_loss: 0.6501 - val_acc: 0.6004\n",
      "Epoch 34/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6307 - acc: 0.6033 - val_loss: 0.6502 - val_acc: 0.6005\n",
      "Epoch 35/45\n",
      "549540/549540 [==============================] - 5s 8us/step - loss: 0.6296 - acc: 0.6041 - val_loss: 0.6504 - val_acc: 0.6009\n",
      "Epoch 36/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6281 - acc: 0.6053 - val_loss: 0.6502 - val_acc: 0.6009\n",
      "Epoch 37/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6267 - acc: 0.6061 - val_loss: 0.6503 - val_acc: 0.6012\n",
      "Epoch 38/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6254 - acc: 0.6068 - val_loss: 0.6506 - val_acc: 0.6015\n",
      "Epoch 39/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6246 - acc: 0.6079 - val_loss: 0.6511 - val_acc: 0.6014\n",
      "Epoch 40/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6235 - acc: 0.6099 - val_loss: 0.6516 - val_acc: 0.6021\n",
      "Epoch 41/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6226 - acc: 0.6095 - val_loss: 0.6524 - val_acc: 0.6025\n",
      "Epoch 42/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6211 - acc: 0.6116 - val_loss: 0.6525 - val_acc: 0.5982\n",
      "Epoch 43/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6202 - acc: 0.6119 - val_loss: 0.6533 - val_acc: 0.5986\n",
      "Epoch 44/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6196 - acc: 0.6131 - val_loss: 0.6539 - val_acc: 0.5984\n",
      "Epoch 45/45\n",
      "549540/549540 [==============================] - 5s 9us/step - loss: 0.6181 - acc: 0.6152 - val_loss: 0.6544 - val_acc: 0.5985\n",
      "[[0.49224633]\n",
      " [0.49868894]\n",
      " [0.49520218]\n",
      " ...\n",
      " [0.5921553 ]\n",
      " [0.9062078 ]\n",
      " [0.91836613]]\n",
      "110961 0.598495145631068\n",
      "trend_test_acc:\n",
      "55548 0.5992233009708738\n",
      "vol_test_acc:\n",
      "55413 0.5977669902912621\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.4776191 ]\n",
      " [0.47250563]\n",
      " [0.4465182 ]\n",
      " ...\n",
      " [0.5777502 ]\n",
      " [0.9662485 ]\n",
      " [0.9662485 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5417367853290184\n",
      "參考前一個答案 benchacc2:\n",
      "0.6166343042071197\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "10\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1207 MiB, count=155, average=7977 KiB\n",
      "(732720, 36, 6)\n",
      "(732720, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 560460 samples, validate on 170100 samples\n",
      "Epoch 1/45\n",
      "560460/560460 [==============================] - 11s 19us/step - loss: 1.7296 - acc: 0.4739 - val_loss: 0.7494 - val_acc: 0.4577\n",
      "Epoch 2/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.8797 - acc: 0.4956 - val_loss: 0.6816 - val_acc: 0.5829\n",
      "Epoch 3/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.7558 - acc: 0.5196 - val_loss: 0.6714 - val_acc: 0.5952\n",
      "Epoch 4/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.7179 - acc: 0.5350 - val_loss: 0.6660 - val_acc: 0.5977\n",
      "Epoch 5/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6996 - acc: 0.5466 - val_loss: 0.6636 - val_acc: 0.6012\n",
      "Epoch 6/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6900 - acc: 0.5537 - val_loss: 0.6620 - val_acc: 0.6088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6837 - acc: 0.5610 - val_loss: 0.6602 - val_acc: 0.6084\n",
      "Epoch 8/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6796 - acc: 0.5658 - val_loss: 0.6589 - val_acc: 0.6108\n",
      "Epoch 9/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6763 - acc: 0.5689 - val_loss: 0.6574 - val_acc: 0.6109\n",
      "Epoch 10/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6731 - acc: 0.5731 - val_loss: 0.6557 - val_acc: 0.6117\n",
      "Epoch 11/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6703 - acc: 0.5773 - val_loss: 0.6544 - val_acc: 0.6122\n",
      "Epoch 12/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6683 - acc: 0.5800 - val_loss: 0.6525 - val_acc: 0.6129\n",
      "Epoch 13/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6658 - acc: 0.5836 - val_loss: 0.6511 - val_acc: 0.6129\n",
      "Epoch 14/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6634 - acc: 0.5856 - val_loss: 0.6495 - val_acc: 0.6135\n",
      "Epoch 15/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6616 - acc: 0.5889 - val_loss: 0.6481 - val_acc: 0.6137\n",
      "Epoch 16/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6602 - acc: 0.5910 - val_loss: 0.6467 - val_acc: 0.6140\n",
      "Epoch 17/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6585 - acc: 0.5922 - val_loss: 0.6453 - val_acc: 0.6141\n",
      "Epoch 18/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6568 - acc: 0.5940 - val_loss: 0.6444 - val_acc: 0.6073\n",
      "Epoch 19/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6561 - acc: 0.5947 - val_loss: 0.6437 - val_acc: 0.6050\n",
      "Epoch 20/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6548 - acc: 0.5963 - val_loss: 0.6430 - val_acc: 0.6058\n",
      "Epoch 21/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6535 - acc: 0.5974 - val_loss: 0.6424 - val_acc: 0.6052\n",
      "Epoch 22/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6531 - acc: 0.5968 - val_loss: 0.6421 - val_acc: 0.6054\n",
      "Epoch 23/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6526 - acc: 0.5977 - val_loss: 0.6416 - val_acc: 0.6053\n",
      "Epoch 24/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6516 - acc: 0.5985 - val_loss: 0.6412 - val_acc: 0.6055\n",
      "Epoch 25/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6509 - acc: 0.5987 - val_loss: 0.6407 - val_acc: 0.6054\n",
      "Epoch 26/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6503 - acc: 0.5999 - val_loss: 0.6407 - val_acc: 0.6053\n",
      "Epoch 27/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6494 - acc: 0.5996 - val_loss: 0.6406 - val_acc: 0.6052\n",
      "Epoch 28/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6490 - acc: 0.6005 - val_loss: 0.6406 - val_acc: 0.6051\n",
      "Epoch 29/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6484 - acc: 0.6006 - val_loss: 0.6404 - val_acc: 0.6049\n",
      "Epoch 30/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6474 - acc: 0.6015 - val_loss: 0.6403 - val_acc: 0.6049\n",
      "Epoch 31/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6469 - acc: 0.6015 - val_loss: 0.6402 - val_acc: 0.6050\n",
      "Epoch 32/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6021 - val_loss: 0.6406 - val_acc: 0.6049\n",
      "Epoch 33/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6450 - acc: 0.6021 - val_loss: 0.6412 - val_acc: 0.6051\n",
      "Epoch 34/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6438 - acc: 0.6033 - val_loss: 0.6418 - val_acc: 0.6053\n",
      "Epoch 35/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6431 - acc: 0.6023 - val_loss: 0.6425 - val_acc: 0.6049\n",
      "Epoch 36/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6423 - acc: 0.6036 - val_loss: 0.6432 - val_acc: 0.6055\n",
      "Epoch 37/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6415 - acc: 0.6043 - val_loss: 0.6442 - val_acc: 0.6053\n",
      "Epoch 38/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6408 - acc: 0.6043 - val_loss: 0.6449 - val_acc: 0.6049\n",
      "Epoch 39/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6400 - acc: 0.6051 - val_loss: 0.6453 - val_acc: 0.6045\n",
      "Epoch 40/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6387 - acc: 0.6048 - val_loss: 0.6457 - val_acc: 0.6043\n",
      "Epoch 41/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6380 - acc: 0.6057 - val_loss: 0.6464 - val_acc: 0.6041\n",
      "Epoch 42/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6367 - acc: 0.6066 - val_loss: 0.6469 - val_acc: 0.6041\n",
      "Epoch 43/45\n",
      "560460/560460 [==============================] - 5s 9us/step - loss: 0.6357 - acc: 0.6075 - val_loss: 0.6475 - val_acc: 0.6039\n",
      "Epoch 44/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6349 - acc: 0.6071 - val_loss: 0.6482 - val_acc: 0.6037\n",
      "Epoch 45/45\n",
      "560460/560460 [==============================] - 5s 8us/step - loss: 0.6335 - acc: 0.6075 - val_loss: 0.6490 - val_acc: 0.6033\n",
      "[[0.5115538 ]\n",
      " [0.49909195]\n",
      " [0.48495567]\n",
      " ...\n",
      " [0.65460587]\n",
      " [0.7810495 ]\n",
      " [0.76426256]]\n",
      "102627 0.6033333333333334\n",
      "trend_test_acc:\n",
      "51406 0.6044209288653734\n",
      "vol_test_acc:\n",
      "51221 0.6022457378012933\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.4878017 ]\n",
      " [0.48331192]\n",
      " [0.45437858]\n",
      " ...\n",
      " [0.555494  ]\n",
      " [0.8303643 ]\n",
      " [0.8303643 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5395884773662551\n",
      "參考前一個答案 benchacc2:\n",
      "0.61787771898883\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "11\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1230 MiB, count=157, average=8020 KiB\n",
      "(746130, 36, 6)\n",
      "(746130, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 547620 samples, validate on 196350 samples\n",
      "Epoch 1/45\n",
      "547620/547620 [==============================] - 11s 21us/step - loss: 1.1229 - acc: 0.5082 - val_loss: 0.7150 - val_acc: 0.4781\n",
      "Epoch 2/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.7982 - acc: 0.5130 - val_loss: 0.6936 - val_acc: 0.5238\n",
      "Epoch 3/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.7312 - acc: 0.5235 - val_loss: 0.6832 - val_acc: 0.5928\n",
      "Epoch 4/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.7061 - acc: 0.5354 - val_loss: 0.6753 - val_acc: 0.5982\n",
      "Epoch 5/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6933 - acc: 0.5464 - val_loss: 0.6707 - val_acc: 0.5978\n",
      "Epoch 6/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6850 - acc: 0.5551 - val_loss: 0.6668 - val_acc: 0.5997\n",
      "Epoch 7/45\n",
      "547620/547620 [==============================] - 5s 10us/step - loss: 0.6791 - acc: 0.5628 - val_loss: 0.6625 - val_acc: 0.6014\n",
      "Epoch 8/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6745 - acc: 0.5689 - val_loss: 0.6589 - val_acc: 0.6098\n",
      "Epoch 9/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6704 - acc: 0.5730 - val_loss: 0.6553 - val_acc: 0.6122\n",
      "Epoch 10/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6670 - acc: 0.5787 - val_loss: 0.6520 - val_acc: 0.6130\n",
      "Epoch 11/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6637 - acc: 0.5830 - val_loss: 0.6490 - val_acc: 0.6144\n",
      "Epoch 12/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6606 - acc: 0.5869 - val_loss: 0.6466 - val_acc: 0.6148\n",
      "Epoch 13/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6582 - acc: 0.5902 - val_loss: 0.6448 - val_acc: 0.6153\n",
      "Epoch 14/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6563 - acc: 0.5921 - val_loss: 0.6436 - val_acc: 0.6157\n",
      "Epoch 15/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6545 - acc: 0.5942 - val_loss: 0.6422 - val_acc: 0.6074\n",
      "Epoch 16/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6529 - acc: 0.5969 - val_loss: 0.6414 - val_acc: 0.6079\n",
      "Epoch 17/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6527 - acc: 0.5968 - val_loss: 0.6406 - val_acc: 0.6081\n",
      "Epoch 18/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6517 - acc: 0.5979 - val_loss: 0.6396 - val_acc: 0.6080\n",
      "Epoch 19/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6501 - acc: 0.5993 - val_loss: 0.6389 - val_acc: 0.6083\n",
      "Epoch 20/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6496 - acc: 0.5998 - val_loss: 0.6386 - val_acc: 0.6083\n",
      "Epoch 21/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6489 - acc: 0.6016 - val_loss: 0.6382 - val_acc: 0.6084\n",
      "Epoch 22/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6483 - acc: 0.6008 - val_loss: 0.6380 - val_acc: 0.6085\n",
      "Epoch 23/45\n",
      "547620/547620 [==============================] - 5s 8us/step - loss: 0.6481 - acc: 0.6019 - val_loss: 0.6376 - val_acc: 0.6086\n",
      "Epoch 24/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6469 - acc: 0.6039 - val_loss: 0.6374 - val_acc: 0.6086\n",
      "Epoch 25/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6468 - acc: 0.6033 - val_loss: 0.6370 - val_acc: 0.6085\n",
      "Epoch 26/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6463 - acc: 0.6039 - val_loss: 0.6368 - val_acc: 0.6087\n",
      "Epoch 27/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6460 - acc: 0.6039 - val_loss: 0.6368 - val_acc: 0.6087\n",
      "Epoch 28/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6459 - acc: 0.6049 - val_loss: 0.6367 - val_acc: 0.6088\n",
      "Epoch 29/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6453 - acc: 0.6049 - val_loss: 0.6366 - val_acc: 0.6087\n",
      "Epoch 30/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6455 - acc: 0.6049 - val_loss: 0.6364 - val_acc: 0.6087\n",
      "Epoch 31/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6448 - acc: 0.6057 - val_loss: 0.6360 - val_acc: 0.6087\n",
      "Epoch 32/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6446 - acc: 0.6058 - val_loss: 0.6360 - val_acc: 0.6089\n",
      "Epoch 33/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6446 - acc: 0.6053 - val_loss: 0.6359 - val_acc: 0.6089\n",
      "Epoch 34/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6439 - acc: 0.6074 - val_loss: 0.6360 - val_acc: 0.6090\n",
      "Epoch 35/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6438 - acc: 0.6067 - val_loss: 0.6355 - val_acc: 0.6091\n",
      "Epoch 36/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6436 - acc: 0.6074 - val_loss: 0.6356 - val_acc: 0.6092\n",
      "Epoch 37/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6434 - acc: 0.6074 - val_loss: 0.6356 - val_acc: 0.6092\n",
      "Epoch 38/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6434 - acc: 0.6074 - val_loss: 0.6356 - val_acc: 0.6063\n",
      "Epoch 39/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6431 - acc: 0.6078 - val_loss: 0.6358 - val_acc: 0.6064\n",
      "Epoch 40/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6428 - acc: 0.6071 - val_loss: 0.6358 - val_acc: 0.6064\n",
      "Epoch 41/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6426 - acc: 0.6080 - val_loss: 0.6358 - val_acc: 0.6093\n",
      "Epoch 42/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6421 - acc: 0.6088 - val_loss: 0.6356 - val_acc: 0.6093\n",
      "Epoch 43/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6424 - acc: 0.6078 - val_loss: 0.6360 - val_acc: 0.6066\n",
      "Epoch 44/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6421 - acc: 0.6082 - val_loss: 0.6356 - val_acc: 0.6093\n",
      "Epoch 45/45\n",
      "547620/547620 [==============================] - 5s 9us/step - loss: 0.6419 - acc: 0.6077 - val_loss: 0.6355 - val_acc: 0.6094\n",
      "[[0.4965858 ]\n",
      " [0.49466404]\n",
      " [0.4853504 ]\n",
      " ...\n",
      " [0.62262785]\n",
      " [0.67126274]\n",
      " [0.66471094]]\n",
      "119654 0.6093913929208047\n",
      "trend_test_acc:\n",
      "59847 0.6095019859456157\n",
      "vol_test_acc:\n",
      "59807 0.6092807660961695\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.5200017 ]\n",
      " [0.47112262]\n",
      " [0.4844755 ]\n",
      " ...\n",
      " [0.5375231 ]\n",
      " [0.67681456]\n",
      " [0.67681456]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5425362872421696\n",
      "參考前一個答案 benchacc2:\n",
      "0.6169136745607333\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "12\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1202 MiB, count=158, average=7793 KiB\n",
      "(729690, 36, 6)\n",
      "(729690, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 556170 samples, validate on 171360 samples\n",
      "Epoch 1/45\n",
      "556170/556170 [==============================] - 12s 22us/step - loss: 1.5312 - acc: 0.5127 - val_loss: 0.6950 - val_acc: 0.5447\n",
      "Epoch 2/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.8861 - acc: 0.5199 - val_loss: 0.6767 - val_acc: 0.5647\n",
      "Epoch 3/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.7569 - acc: 0.5266 - val_loss: 0.6744 - val_acc: 0.5663\n",
      "Epoch 4/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.7160 - acc: 0.5342 - val_loss: 0.6715 - val_acc: 0.5853\n",
      "Epoch 5/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6977 - acc: 0.5425 - val_loss: 0.6688 - val_acc: 0.5859\n",
      "Epoch 6/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6887 - acc: 0.5479 - val_loss: 0.6664 - val_acc: 0.5922\n",
      "Epoch 7/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6814 - acc: 0.5565 - val_loss: 0.6635 - val_acc: 0.5945\n",
      "Epoch 8/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6763 - acc: 0.5632 - val_loss: 0.6606 - val_acc: 0.5956\n",
      "Epoch 9/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6721 - acc: 0.5691 - val_loss: 0.6572 - val_acc: 0.6059\n",
      "Epoch 10/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6687 - acc: 0.5720 - val_loss: 0.6542 - val_acc: 0.6115\n",
      "Epoch 11/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6660 - acc: 0.5758 - val_loss: 0.6524 - val_acc: 0.6140\n",
      "Epoch 12/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6636 - acc: 0.5790 - val_loss: 0.6505 - val_acc: 0.6116\n",
      "Epoch 13/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6614 - acc: 0.5815 - val_loss: 0.6488 - val_acc: 0.6126\n",
      "Epoch 14/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6596 - acc: 0.5847 - val_loss: 0.6477 - val_acc: 0.6127\n",
      "Epoch 15/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6585 - acc: 0.5854 - val_loss: 0.6465 - val_acc: 0.6132\n",
      "Epoch 16/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6571 - acc: 0.5881 - val_loss: 0.6457 - val_acc: 0.6134\n",
      "Epoch 17/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6557 - acc: 0.5897 - val_loss: 0.6448 - val_acc: 0.6040\n",
      "Epoch 18/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6544 - acc: 0.5914 - val_loss: 0.6443 - val_acc: 0.6238\n",
      "Epoch 19/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6533 - acc: 0.5920 - val_loss: 0.6433 - val_acc: 0.6042\n",
      "Epoch 20/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6526 - acc: 0.5934 - val_loss: 0.6428 - val_acc: 0.6141\n",
      "Epoch 21/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6516 - acc: 0.5949 - val_loss: 0.6420 - val_acc: 0.6044\n",
      "Epoch 22/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6510 - acc: 0.5956 - val_loss: 0.6413 - val_acc: 0.6045\n",
      "Epoch 23/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6502 - acc: 0.5960 - val_loss: 0.6408 - val_acc: 0.6145\n",
      "Epoch 24/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6496 - acc: 0.5965 - val_loss: 0.6403 - val_acc: 0.6051\n",
      "Epoch 25/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6491 - acc: 0.5980 - val_loss: 0.6400 - val_acc: 0.6149\n",
      "Epoch 26/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6489 - acc: 0.5973 - val_loss: 0.6399 - val_acc: 0.6053\n",
      "Epoch 27/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6482 - acc: 0.5983 - val_loss: 0.6397 - val_acc: 0.6149\n",
      "Epoch 28/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6478 - acc: 0.5998 - val_loss: 0.6393 - val_acc: 0.6150\n",
      "Epoch 29/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6474 - acc: 0.5997 - val_loss: 0.6391 - val_acc: 0.6151\n",
      "Epoch 30/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6466 - acc: 0.6002 - val_loss: 0.6388 - val_acc: 0.6055\n",
      "Epoch 31/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6466 - acc: 0.6005 - val_loss: 0.6387 - val_acc: 0.6053\n",
      "Epoch 32/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6463 - acc: 0.6006 - val_loss: 0.6386 - val_acc: 0.6152\n",
      "Epoch 33/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6460 - acc: 0.6012 - val_loss: 0.6383 - val_acc: 0.6151\n",
      "Epoch 34/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6456 - acc: 0.6009 - val_loss: 0.6383 - val_acc: 0.6155\n",
      "Epoch 35/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6452 - acc: 0.6007 - val_loss: 0.6381 - val_acc: 0.6148\n",
      "Epoch 36/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6453 - acc: 0.6009 - val_loss: 0.6381 - val_acc: 0.6158\n",
      "Epoch 37/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6450 - acc: 0.6015 - val_loss: 0.6379 - val_acc: 0.6153\n",
      "Epoch 38/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6444 - acc: 0.6021 - val_loss: 0.6378 - val_acc: 0.6056\n",
      "Epoch 39/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6445 - acc: 0.6026 - val_loss: 0.6378 - val_acc: 0.6055\n",
      "Epoch 40/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6445 - acc: 0.6022 - val_loss: 0.6376 - val_acc: 0.6059\n",
      "Epoch 41/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6441 - acc: 0.6028 - val_loss: 0.6376 - val_acc: 0.6154\n",
      "Epoch 42/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6436 - acc: 0.6027 - val_loss: 0.6378 - val_acc: 0.6153\n",
      "Epoch 43/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6435 - acc: 0.6022 - val_loss: 0.6380 - val_acc: 0.6054\n",
      "Epoch 44/45\n",
      "556170/556170 [==============================] - 5s 9us/step - loss: 0.6434 - acc: 0.6028 - val_loss: 0.6382 - val_acc: 0.6145\n",
      "Epoch 45/45\n",
      "556170/556170 [==============================] - 5s 8us/step - loss: 0.6433 - acc: 0.6024 - val_loss: 0.6387 - val_acc: 0.6144\n",
      "[[0.49991518]\n",
      " [0.495387  ]\n",
      " [0.48839632]\n",
      " ...\n",
      " [0.60720813]\n",
      " [0.68139637]\n",
      " [0.6842341 ]]\n",
      "105288 0.6144257703081233\n",
      "trend_test_acc:\n",
      "52664 0.6146591970121382\n",
      "vol_test_acc:\n",
      "52624 0.6141923436041083\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.49063244]\n",
      " [0.46738487]\n",
      " [0.47684655]\n",
      " ...\n",
      " [0.52419525]\n",
      " [0.5577824 ]\n",
      " [0.5577824 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5388655462184874\n",
      "參考前一個答案 benchacc2:\n",
      "0.6160539215686274\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "13\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1218 MiB, count=158, average=7894 KiB\n",
      "(739080, 36, 6)\n",
      "(739080, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 542130 samples, validate on 194790 samples\n",
      "Epoch 1/45\n",
      "542130/542130 [==============================] - 12s 22us/step - loss: 1.6182 - acc: 0.5067 - val_loss: 0.7011 - val_acc: 0.5606\n",
      "Epoch 2/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.8971 - acc: 0.5124 - val_loss: 0.6840 - val_acc: 0.5745\n",
      "Epoch 3/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.7584 - acc: 0.5178 - val_loss: 0.6813 - val_acc: 0.5807\n",
      "Epoch 4/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.7170 - acc: 0.5291 - val_loss: 0.6766 - val_acc: 0.5888\n",
      "Epoch 5/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.7006 - acc: 0.5412 - val_loss: 0.6718 - val_acc: 0.5924\n",
      "Epoch 6/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6907 - acc: 0.5509 - val_loss: 0.6669 - val_acc: 0.5987\n",
      "Epoch 7/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6837 - acc: 0.5600 - val_loss: 0.6634 - val_acc: 0.6005\n",
      "Epoch 8/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6789 - acc: 0.5665 - val_loss: 0.6601 - val_acc: 0.6075\n",
      "Epoch 9/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6749 - acc: 0.5713 - val_loss: 0.6569 - val_acc: 0.6083\n",
      "Epoch 10/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6714 - acc: 0.5767 - val_loss: 0.6542 - val_acc: 0.6038\n",
      "Epoch 11/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6687 - acc: 0.5803 - val_loss: 0.6515 - val_acc: 0.6108\n",
      "Epoch 12/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6657 - acc: 0.5836 - val_loss: 0.6496 - val_acc: 0.6044\n",
      "Epoch 13/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6638 - acc: 0.5860 - val_loss: 0.6477 - val_acc: 0.6049\n",
      "Epoch 14/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6613 - acc: 0.5889 - val_loss: 0.6461 - val_acc: 0.6032\n",
      "Epoch 15/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6595 - acc: 0.5911 - val_loss: 0.6446 - val_acc: 0.6042\n",
      "Epoch 16/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6578 - acc: 0.5939 - val_loss: 0.6434 - val_acc: 0.6046\n",
      "Epoch 17/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6562 - acc: 0.5949 - val_loss: 0.6427 - val_acc: 0.6051\n",
      "Epoch 18/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6547 - acc: 0.5962 - val_loss: 0.6417 - val_acc: 0.6050\n",
      "Epoch 19/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6538 - acc: 0.5967 - val_loss: 0.6410 - val_acc: 0.6054\n",
      "Epoch 20/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6528 - acc: 0.5987 - val_loss: 0.6404 - val_acc: 0.6056\n",
      "Epoch 21/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6518 - acc: 0.5999 - val_loss: 0.6400 - val_acc: 0.6057\n",
      "Epoch 22/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6507 - acc: 0.6002 - val_loss: 0.6394 - val_acc: 0.6056\n",
      "Epoch 23/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6503 - acc: 0.6000 - val_loss: 0.6390 - val_acc: 0.6056\n",
      "Epoch 24/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6497 - acc: 0.6021 - val_loss: 0.6386 - val_acc: 0.6064\n",
      "Epoch 25/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6488 - acc: 0.6018 - val_loss: 0.6385 - val_acc: 0.6065\n",
      "Epoch 26/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6485 - acc: 0.6028 - val_loss: 0.6383 - val_acc: 0.6064\n",
      "Epoch 27/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6476 - acc: 0.6043 - val_loss: 0.6381 - val_acc: 0.6065\n",
      "Epoch 28/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6471 - acc: 0.6037 - val_loss: 0.6381 - val_acc: 0.6063\n",
      "Epoch 29/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6465 - acc: 0.6039 - val_loss: 0.6379 - val_acc: 0.6060\n",
      "Epoch 30/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6043 - val_loss: 0.6379 - val_acc: 0.6064\n",
      "Epoch 31/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6455 - acc: 0.6045 - val_loss: 0.6376 - val_acc: 0.6063\n",
      "Epoch 32/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6447 - acc: 0.6043 - val_loss: 0.6377 - val_acc: 0.6063\n",
      "Epoch 33/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6444 - acc: 0.6044 - val_loss: 0.6375 - val_acc: 0.6060\n",
      "Epoch 34/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6437 - acc: 0.6053 - val_loss: 0.6375 - val_acc: 0.6055\n",
      "Epoch 35/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6430 - acc: 0.6062 - val_loss: 0.6376 - val_acc: 0.6056\n",
      "Epoch 36/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6427 - acc: 0.6057 - val_loss: 0.6379 - val_acc: 0.6050\n",
      "Epoch 37/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6419 - acc: 0.6062 - val_loss: 0.6379 - val_acc: 0.6053\n",
      "Epoch 38/45\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6414 - acc: 0.6067 - val_loss: 0.6379 - val_acc: 0.6050\n",
      "Epoch 39/45\n",
      "542130/542130 [==============================] - 5s 8us/step - loss: 0.6401 - acc: 0.6068 - val_loss: 0.6381 - val_acc: 0.6051\n",
      "Epoch 40/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6397 - acc: 0.6069 - val_loss: 0.6384 - val_acc: 0.6046\n",
      "Epoch 41/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6387 - acc: 0.6069 - val_loss: 0.6390 - val_acc: 0.6052\n",
      "Epoch 42/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6380 - acc: 0.6065 - val_loss: 0.6393 - val_acc: 0.6049\n",
      "Epoch 43/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6374 - acc: 0.6065 - val_loss: 0.6401 - val_acc: 0.6047\n",
      "Epoch 44/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6365 - acc: 0.6059 - val_loss: 0.6402 - val_acc: 0.6042\n",
      "Epoch 45/45\n",
      "542130/542130 [==============================] - 5s 9us/step - loss: 0.6348 - acc: 0.6071 - val_loss: 0.6411 - val_acc: 0.6038\n",
      "[[0.5053732 ]\n",
      " [0.48540017]\n",
      " [0.48925558]\n",
      " ...\n",
      " [0.60915226]\n",
      " [0.7035961 ]\n",
      " [0.67463094]]\n",
      "117606 0.603757893115663\n",
      "trend_test_acc:\n",
      "58811 0.6037470485576429\n",
      "vol_test_acc:\n",
      "58795 0.603768741014582\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.5143339 ]\n",
      " [0.42710313]\n",
      " [0.43090183]\n",
      " ...\n",
      " [0.47357178]\n",
      " [0.9019744 ]\n",
      " [0.9019744 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5429898865444838\n",
      "參考前一個答案 benchacc2:\n",
      "0.6153858000924073\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "14\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1247 MiB, count=158, average=8084 KiB\n",
      "(756900, 36, 6)\n",
      "(756900, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 566820 samples, validate on 187920 samples\n",
      "Epoch 1/45\n",
      "566820/566820 [==============================] - 13s 23us/step - loss: 1.1627 - acc: 0.5059 - val_loss: 0.6908 - val_acc: 0.5580\n",
      "Epoch 2/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.7974 - acc: 0.5193 - val_loss: 0.6742 - val_acc: 0.5864\n",
      "Epoch 3/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.7229 - acc: 0.5346 - val_loss: 0.6671 - val_acc: 0.5993\n",
      "Epoch 4/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6979 - acc: 0.5469 - val_loss: 0.6639 - val_acc: 0.6053\n",
      "Epoch 5/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6866 - acc: 0.5578 - val_loss: 0.6611 - val_acc: 0.6086\n",
      "Epoch 6/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6790 - acc: 0.5673 - val_loss: 0.6572 - val_acc: 0.6062\n",
      "Epoch 7/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6733 - acc: 0.5734 - val_loss: 0.6533 - val_acc: 0.6068\n",
      "Epoch 8/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6683 - acc: 0.5802 - val_loss: 0.6498 - val_acc: 0.6080\n",
      "Epoch 9/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6649 - acc: 0.5834 - val_loss: 0.6474 - val_acc: 0.6116\n",
      "Epoch 10/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6610 - acc: 0.5877 - val_loss: 0.6456 - val_acc: 0.6048\n",
      "Epoch 11/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6590 - acc: 0.5906 - val_loss: 0.6439 - val_acc: 0.6097\n",
      "Epoch 12/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6571 - acc: 0.5920 - val_loss: 0.6428 - val_acc: 0.6109\n",
      "Epoch 13/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6554 - acc: 0.5932 - val_loss: 0.6420 - val_acc: 0.6109\n",
      "Epoch 14/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6539 - acc: 0.5950 - val_loss: 0.6410 - val_acc: 0.6113\n",
      "Epoch 15/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6527 - acc: 0.5966 - val_loss: 0.6401 - val_acc: 0.6116\n",
      "Epoch 16/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6518 - acc: 0.5975 - val_loss: 0.6393 - val_acc: 0.6120\n",
      "Epoch 17/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6510 - acc: 0.5976 - val_loss: 0.6387 - val_acc: 0.6122\n",
      "Epoch 18/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6501 - acc: 0.5994 - val_loss: 0.6383 - val_acc: 0.6124\n",
      "Epoch 19/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6493 - acc: 0.5993 - val_loss: 0.6378 - val_acc: 0.6125\n",
      "Epoch 20/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6487 - acc: 0.5997 - val_loss: 0.6375 - val_acc: 0.6124\n",
      "Epoch 21/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6480 - acc: 0.6007 - val_loss: 0.6371 - val_acc: 0.6125\n",
      "Epoch 22/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6475 - acc: 0.6004 - val_loss: 0.6366 - val_acc: 0.6159\n",
      "Epoch 23/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6467 - acc: 0.6018 - val_loss: 0.6360 - val_acc: 0.6159\n",
      "Epoch 24/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6463 - acc: 0.6019 - val_loss: 0.6357 - val_acc: 0.6159\n",
      "Epoch 25/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6015 - val_loss: 0.6354 - val_acc: 0.6160\n",
      "Epoch 26/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6451 - acc: 0.6029 - val_loss: 0.6351 - val_acc: 0.6161\n",
      "Epoch 27/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6447 - acc: 0.6029 - val_loss: 0.6348 - val_acc: 0.6162\n",
      "Epoch 28/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6446 - acc: 0.6035 - val_loss: 0.6345 - val_acc: 0.6164\n",
      "Epoch 29/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6441 - acc: 0.6038 - val_loss: 0.6344 - val_acc: 0.6164\n",
      "Epoch 30/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6439 - acc: 0.6038 - val_loss: 0.6343 - val_acc: 0.6163\n",
      "Epoch 31/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6432 - acc: 0.6047 - val_loss: 0.6339 - val_acc: 0.6161\n",
      "Epoch 32/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6428 - acc: 0.6047 - val_loss: 0.6341 - val_acc: 0.6161\n",
      "Epoch 33/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6423 - acc: 0.6062 - val_loss: 0.6348 - val_acc: 0.6137\n",
      "Epoch 34/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6421 - acc: 0.6055 - val_loss: 0.6350 - val_acc: 0.6121\n",
      "Epoch 35/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6420 - acc: 0.6054 - val_loss: 0.6355 - val_acc: 0.6105\n",
      "Epoch 36/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6417 - acc: 0.6052 - val_loss: 0.6354 - val_acc: 0.6089\n",
      "Epoch 37/45\n",
      "566820/566820 [==============================] - 5s 8us/step - loss: 0.6414 - acc: 0.6058 - val_loss: 0.6355 - val_acc: 0.6095\n",
      "Epoch 38/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6408 - acc: 0.6053 - val_loss: 0.6357 - val_acc: 0.6096\n",
      "Epoch 39/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6406 - acc: 0.6063 - val_loss: 0.6361 - val_acc: 0.6097\n",
      "Epoch 40/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6402 - acc: 0.6055 - val_loss: 0.6369 - val_acc: 0.6083\n",
      "Epoch 41/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6400 - acc: 0.6075 - val_loss: 0.6361 - val_acc: 0.6100\n",
      "Epoch 42/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6396 - acc: 0.6073 - val_loss: 0.6363 - val_acc: 0.6110\n",
      "Epoch 43/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6393 - acc: 0.6074 - val_loss: 0.6367 - val_acc: 0.6095\n",
      "Epoch 44/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6388 - acc: 0.6073 - val_loss: 0.6372 - val_acc: 0.6102\n",
      "Epoch 45/45\n",
      "566820/566820 [==============================] - 5s 9us/step - loss: 0.6384 - acc: 0.6072 - val_loss: 0.6382 - val_acc: 0.6098\n",
      "[[0.5003475 ]\n",
      " [0.5029592 ]\n",
      " [0.4883614 ]\n",
      " ...\n",
      " [0.6281545 ]\n",
      " [0.68727857]\n",
      " [0.70712435]]\n",
      "114599 0.60982865048957\n",
      "trend_test_acc:\n",
      "57221 0.6089931885908897\n",
      "vol_test_acc:\n",
      "57378 0.6106641123882504\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.50722337]\n",
      " [0.45898217]\n",
      " [0.45784557]\n",
      " ...\n",
      " [0.5163909 ]\n",
      " [0.8148251 ]\n",
      " [0.8148251 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5397296722009366\n",
      "參考前一個答案 benchacc2:\n",
      "0.6186941251596424\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "15\n",
      "<ipython-input-7-55fd6c965666>:47: size=1247 MiB, count=2, average=624 MiB\n",
      "(739830, 36, 6)\n",
      "(739830, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 558390 samples, validate on 179280 samples\n",
      "Epoch 1/45\n",
      "558390/558390 [==============================] - 13s 23us/step - loss: 1.2513 - acc: 0.5214 - val_loss: 0.6769 - val_acc: 0.5763\n",
      "Epoch 2/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.8257 - acc: 0.5266 - val_loss: 0.6652 - val_acc: 0.5996\n",
      "Epoch 3/45\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.7348 - acc: 0.5357 - val_loss: 0.6656 - val_acc: 0.5954\n",
      "Epoch 4/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.7029 - acc: 0.5470 - val_loss: 0.6642 - val_acc: 0.5894\n",
      "Epoch 5/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6893 - acc: 0.5559 - val_loss: 0.6614 - val_acc: 0.5899\n",
      "Epoch 6/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6808 - acc: 0.5624 - val_loss: 0.6584 - val_acc: 0.5905\n",
      "Epoch 7/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6750 - acc: 0.5688 - val_loss: 0.6558 - val_acc: 0.5898\n",
      "Epoch 8/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6717 - acc: 0.5730 - val_loss: 0.6536 - val_acc: 0.5964\n",
      "Epoch 9/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6678 - acc: 0.5776 - val_loss: 0.6515 - val_acc: 0.5985\n",
      "Epoch 10/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6655 - acc: 0.5810 - val_loss: 0.6496 - val_acc: 0.5997\n",
      "Epoch 11/45\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.6628 - acc: 0.5841 - val_loss: 0.6481 - val_acc: 0.6051\n",
      "Epoch 12/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6604 - acc: 0.5867 - val_loss: 0.6465 - val_acc: 0.6067\n",
      "Epoch 13/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6590 - acc: 0.5882 - val_loss: 0.6452 - val_acc: 0.6075\n",
      "Epoch 14/45\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.6570 - acc: 0.5910 - val_loss: 0.6443 - val_acc: 0.6080\n",
      "Epoch 15/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6556 - acc: 0.5921 - val_loss: 0.6432 - val_acc: 0.6082\n",
      "Epoch 16/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6544 - acc: 0.5939 - val_loss: 0.6424 - val_acc: 0.6083\n",
      "Epoch 17/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6532 - acc: 0.5957 - val_loss: 0.6413 - val_acc: 0.6084\n",
      "Epoch 18/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6517 - acc: 0.5972 - val_loss: 0.6404 - val_acc: 0.6086\n",
      "Epoch 19/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6511 - acc: 0.5978 - val_loss: 0.6398 - val_acc: 0.6086\n",
      "Epoch 20/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6501 - acc: 0.5996 - val_loss: 0.6391 - val_acc: 0.6086\n",
      "Epoch 21/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6492 - acc: 0.5994 - val_loss: 0.6385 - val_acc: 0.6086\n",
      "Epoch 22/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6485 - acc: 0.6009 - val_loss: 0.6378 - val_acc: 0.6087\n",
      "Epoch 23/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6478 - acc: 0.6016 - val_loss: 0.6373 - val_acc: 0.6087\n",
      "Epoch 24/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6474 - acc: 0.6013 - val_loss: 0.6370 - val_acc: 0.6087\n",
      "Epoch 25/45\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.6470 - acc: 0.6019 - val_loss: 0.6367 - val_acc: 0.6087\n",
      "Epoch 26/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6461 - acc: 0.6030 - val_loss: 0.6360 - val_acc: 0.6088\n",
      "Epoch 27/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6456 - acc: 0.6033 - val_loss: 0.6357 - val_acc: 0.6089\n",
      "Epoch 28/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6455 - acc: 0.6037 - val_loss: 0.6355 - val_acc: 0.6089\n",
      "Epoch 29/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6449 - acc: 0.6038 - val_loss: 0.6354 - val_acc: 0.6089\n",
      "Epoch 30/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6444 - acc: 0.6045 - val_loss: 0.6349 - val_acc: 0.6090\n",
      "Epoch 31/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6442 - acc: 0.6042 - val_loss: 0.6350 - val_acc: 0.6090\n",
      "Epoch 32/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6443 - acc: 0.6050 - val_loss: 0.6348 - val_acc: 0.6089\n",
      "Epoch 33/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6436 - acc: 0.6058 - val_loss: 0.6348 - val_acc: 0.6090\n",
      "Epoch 34/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6433 - acc: 0.6052 - val_loss: 0.6346 - val_acc: 0.6090\n",
      "Epoch 35/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6429 - acc: 0.6060 - val_loss: 0.6345 - val_acc: 0.6090\n",
      "Epoch 36/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6432 - acc: 0.6050 - val_loss: 0.6348 - val_acc: 0.6089\n",
      "Epoch 37/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6428 - acc: 0.6067 - val_loss: 0.6345 - val_acc: 0.6089\n",
      "Epoch 38/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6427 - acc: 0.6064 - val_loss: 0.6343 - val_acc: 0.6089\n",
      "Epoch 39/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6424 - acc: 0.6061 - val_loss: 0.6343 - val_acc: 0.6089\n",
      "Epoch 40/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6423 - acc: 0.6072 - val_loss: 0.6343 - val_acc: 0.6089\n",
      "Epoch 41/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6422 - acc: 0.6070 - val_loss: 0.6343 - val_acc: 0.6089\n",
      "Epoch 42/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6418 - acc: 0.6068 - val_loss: 0.6342 - val_acc: 0.6089\n",
      "Epoch 43/45\n",
      "558390/558390 [==============================] - 5s 8us/step - loss: 0.6417 - acc: 0.6069 - val_loss: 0.6340 - val_acc: 0.6088\n",
      "Epoch 44/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6417 - acc: 0.6072 - val_loss: 0.6342 - val_acc: 0.6089\n",
      "Epoch 45/45\n",
      "558390/558390 [==============================] - 5s 9us/step - loss: 0.6412 - acc: 0.6067 - val_loss: 0.6340 - val_acc: 0.6089\n",
      "[[0.4975531 ]\n",
      " [0.49672475]\n",
      " [0.48423052]\n",
      " ...\n",
      " [0.61017215]\n",
      " [0.6653819 ]\n",
      " [0.6677963 ]]\n",
      "109169 0.6089301651048639\n",
      "trend_test_acc:\n",
      "54455 0.6074854975457386\n",
      "vol_test_acc:\n",
      "54714 0.6103748326639893\n",
      "Reverse section--------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48554412]\n",
      " [0.4810999 ]\n",
      " [0.47849414]\n",
      " ...\n",
      " [0.52169937]\n",
      " [0.59496444]\n",
      " [0.59496444]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5392124051762606\n",
      "參考前一個答案 benchacc2:\n",
      "0.615863453815261\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "16\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1261 MiB, count=158, average=8171 KiB\n",
      "(765030, 36, 6)\n",
      "(765030, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 566310 samples, validate on 196560 samples\n",
      "Epoch 1/45\n",
      "566310/566310 [==============================] - 14s 24us/step - loss: 1.4668 - acc: 0.5244 - val_loss: 0.7196 - val_acc: 0.5337\n",
      "Epoch 2/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.8534 - acc: 0.5260 - val_loss: 0.6783 - val_acc: 0.5922\n",
      "Epoch 3/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.7457 - acc: 0.5338 - val_loss: 0.6706 - val_acc: 0.6162\n",
      "Epoch 4/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.7111 - acc: 0.5421 - val_loss: 0.6663 - val_acc: 0.6192\n",
      "Epoch 5/45\n",
      "566310/566310 [==============================] - 5s 8us/step - loss: 0.6932 - acc: 0.5534 - val_loss: 0.6618 - val_acc: 0.6144\n",
      "Epoch 6/45\n",
      "566310/566310 [==============================] - 5s 8us/step - loss: 0.6831 - acc: 0.5635 - val_loss: 0.6577 - val_acc: 0.6126\n",
      "Epoch 7/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6764 - acc: 0.5716 - val_loss: 0.6541 - val_acc: 0.6135\n",
      "Epoch 8/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6713 - acc: 0.5768 - val_loss: 0.6510 - val_acc: 0.6140\n",
      "Epoch 9/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6673 - acc: 0.5825 - val_loss: 0.6484 - val_acc: 0.6190\n",
      "Epoch 10/45\n",
      "566310/566310 [==============================] - 5s 8us/step - loss: 0.6641 - acc: 0.5856 - val_loss: 0.6464 - val_acc: 0.6171\n",
      "Epoch 11/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6616 - acc: 0.5884 - val_loss: 0.6446 - val_acc: 0.6235\n",
      "Epoch 12/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6596 - acc: 0.5900 - val_loss: 0.6429 - val_acc: 0.6234\n",
      "Epoch 13/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6579 - acc: 0.5928 - val_loss: 0.6415 - val_acc: 0.6232\n",
      "Epoch 14/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6563 - acc: 0.5948 - val_loss: 0.6403 - val_acc: 0.6233\n",
      "Epoch 15/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6547 - acc: 0.5962 - val_loss: 0.6387 - val_acc: 0.6234\n",
      "Epoch 16/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6536 - acc: 0.5972 - val_loss: 0.6377 - val_acc: 0.6236\n",
      "Epoch 17/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6521 - acc: 0.5994 - val_loss: 0.6366 - val_acc: 0.6236\n",
      "Epoch 18/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6514 - acc: 0.5985 - val_loss: 0.6358 - val_acc: 0.6233\n",
      "Epoch 19/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6503 - acc: 0.6007 - val_loss: 0.6354 - val_acc: 0.6231\n",
      "Epoch 20/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6494 - acc: 0.6006 - val_loss: 0.6349 - val_acc: 0.6232\n",
      "Epoch 21/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6486 - acc: 0.6017 - val_loss: 0.6343 - val_acc: 0.6233\n",
      "Epoch 22/45\n",
      "566310/566310 [==============================] - 5s 8us/step - loss: 0.6472 - acc: 0.6033 - val_loss: 0.6340 - val_acc: 0.6151\n",
      "Epoch 23/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6474 - acc: 0.6031 - val_loss: 0.6339 - val_acc: 0.6152\n",
      "Epoch 24/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6466 - acc: 0.6034 - val_loss: 0.6336 - val_acc: 0.6153\n",
      "Epoch 25/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6462 - acc: 0.6032 - val_loss: 0.6335 - val_acc: 0.6154\n",
      "Epoch 26/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6457 - acc: 0.6042 - val_loss: 0.6333 - val_acc: 0.6152\n",
      "Epoch 27/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6038 - val_loss: 0.6333 - val_acc: 0.6156\n",
      "Epoch 28/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6456 - acc: 0.6051 - val_loss: 0.6334 - val_acc: 0.6236\n",
      "Epoch 29/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6449 - acc: 0.6043 - val_loss: 0.6329 - val_acc: 0.6151\n",
      "Epoch 30/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6444 - acc: 0.6052 - val_loss: 0.6329 - val_acc: 0.6152\n",
      "Epoch 31/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6439 - acc: 0.6056 - val_loss: 0.6327 - val_acc: 0.6153\n",
      "Epoch 32/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6439 - acc: 0.6064 - val_loss: 0.6327 - val_acc: 0.6153\n",
      "Epoch 33/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6435 - acc: 0.6057 - val_loss: 0.6326 - val_acc: 0.6155\n",
      "Epoch 34/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6436 - acc: 0.6061 - val_loss: 0.6324 - val_acc: 0.6154\n",
      "Epoch 35/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6433 - acc: 0.6070 - val_loss: 0.6325 - val_acc: 0.6154\n",
      "Epoch 36/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6432 - acc: 0.6069 - val_loss: 0.6324 - val_acc: 0.6154\n",
      "Epoch 37/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6431 - acc: 0.6069 - val_loss: 0.6322 - val_acc: 0.6154\n",
      "Epoch 38/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6427 - acc: 0.6069 - val_loss: 0.6319 - val_acc: 0.6153\n",
      "Epoch 39/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6425 - acc: 0.6076 - val_loss: 0.6320 - val_acc: 0.6154\n",
      "Epoch 40/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6426 - acc: 0.6074 - val_loss: 0.6320 - val_acc: 0.6152\n",
      "Epoch 41/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6419 - acc: 0.6076 - val_loss: 0.6317 - val_acc: 0.6152\n",
      "Epoch 42/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6421 - acc: 0.6078 - val_loss: 0.6318 - val_acc: 0.6152\n",
      "Epoch 43/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6419 - acc: 0.6068 - val_loss: 0.6318 - val_acc: 0.6153\n",
      "Epoch 44/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6416 - acc: 0.6076 - val_loss: 0.6319 - val_acc: 0.6153\n",
      "Epoch 45/45\n",
      "566310/566310 [==============================] - 5s 9us/step - loss: 0.6414 - acc: 0.6073 - val_loss: 0.6319 - val_acc: 0.6153\n",
      "[[0.49977064]\n",
      " [0.4948418 ]\n",
      " [0.48532882]\n",
      " ...\n",
      " [0.58910805]\n",
      " [0.6755304 ]\n",
      " [0.6756336 ]]\n",
      "120952 0.6153439153439153\n",
      "trend_test_acc:\n",
      "60381 0.6143772893772894\n",
      "vol_test_acc:\n",
      "60571 0.6163105413105413\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.51012146]\n",
      " [0.4812723 ]\n",
      " [0.44753373]\n",
      " ...\n",
      " [0.55190206]\n",
      " [0.559251  ]\n",
      " [0.559251  ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.539514652014652\n",
      "參考前一個答案 benchacc2:\n",
      "0.6185795685795685\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "17\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1238 MiB, count=158, average=8024 KiB\n",
      "(751290, 36, 6)\n",
      "(751290, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 568080 samples, validate on 181050 samples\n",
      "Epoch 1/45\n",
      "568080/568080 [==============================] - 14s 24us/step - loss: 1.5371 - acc: 0.5124 - val_loss: 0.6941 - val_acc: 0.5473\n",
      "Epoch 2/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.8719 - acc: 0.5108 - val_loss: 0.6818 - val_acc: 0.5724\n",
      "Epoch 3/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.7510 - acc: 0.5176 - val_loss: 0.6784 - val_acc: 0.5884\n",
      "Epoch 4/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.7162 - acc: 0.5274 - val_loss: 0.6745 - val_acc: 0.5938\n",
      "Epoch 5/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6997 - acc: 0.5387 - val_loss: 0.6695 - val_acc: 0.5947\n",
      "Epoch 6/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6884 - acc: 0.5510 - val_loss: 0.6641 - val_acc: 0.5958\n",
      "Epoch 7/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6808 - acc: 0.5595 - val_loss: 0.6587 - val_acc: 0.6017\n",
      "Epoch 8/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6736 - acc: 0.5677 - val_loss: 0.6541 - val_acc: 0.6041\n",
      "Epoch 9/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6684 - acc: 0.5742 - val_loss: 0.6507 - val_acc: 0.6181\n",
      "Epoch 10/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6650 - acc: 0.5782 - val_loss: 0.6479 - val_acc: 0.6135\n",
      "Epoch 11/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6614 - acc: 0.5825 - val_loss: 0.6459 - val_acc: 0.6172\n",
      "Epoch 12/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6588 - acc: 0.5853 - val_loss: 0.6441 - val_acc: 0.6182\n",
      "Epoch 13/45\n",
      "568080/568080 [==============================] - 5s 8us/step - loss: 0.6569 - acc: 0.5882 - val_loss: 0.6426 - val_acc: 0.6182\n",
      "Epoch 14/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6555 - acc: 0.5895 - val_loss: 0.6414 - val_acc: 0.6190\n",
      "Epoch 15/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6543 - acc: 0.5924 - val_loss: 0.6407 - val_acc: 0.6189\n",
      "Epoch 16/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6529 - acc: 0.5928 - val_loss: 0.6401 - val_acc: 0.6098\n",
      "Epoch 17/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6516 - acc: 0.5957 - val_loss: 0.6394 - val_acc: 0.6097\n",
      "Epoch 18/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6507 - acc: 0.5964 - val_loss: 0.6389 - val_acc: 0.6093\n",
      "Epoch 19/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6502 - acc: 0.5971 - val_loss: 0.6386 - val_acc: 0.6093\n",
      "Epoch 20/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6493 - acc: 0.5984 - val_loss: 0.6382 - val_acc: 0.6095\n",
      "Epoch 21/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6485 - acc: 0.5989 - val_loss: 0.6376 - val_acc: 0.6098\n",
      "Epoch 22/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6480 - acc: 0.6006 - val_loss: 0.6373 - val_acc: 0.6098\n",
      "Epoch 23/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6474 - acc: 0.5996 - val_loss: 0.6368 - val_acc: 0.6097\n",
      "Epoch 24/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6470 - acc: 0.6010 - val_loss: 0.6365 - val_acc: 0.6096\n",
      "Epoch 25/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6462 - acc: 0.6006 - val_loss: 0.6362 - val_acc: 0.6097\n",
      "Epoch 26/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6462 - acc: 0.6014 - val_loss: 0.6360 - val_acc: 0.6191\n",
      "Epoch 27/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6015 - val_loss: 0.6356 - val_acc: 0.6192\n",
      "Epoch 28/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6014 - val_loss: 0.6353 - val_acc: 0.6190\n",
      "Epoch 29/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6449 - acc: 0.6013 - val_loss: 0.6350 - val_acc: 0.6190\n",
      "Epoch 30/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6447 - acc: 0.6027 - val_loss: 0.6349 - val_acc: 0.6188\n",
      "Epoch 31/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6442 - acc: 0.6031 - val_loss: 0.6349 - val_acc: 0.6094\n",
      "Epoch 32/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6443 - acc: 0.6037 - val_loss: 0.6347 - val_acc: 0.6128\n",
      "Epoch 33/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6441 - acc: 0.6035 - val_loss: 0.6345 - val_acc: 0.6189\n",
      "Epoch 34/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6438 - acc: 0.6036 - val_loss: 0.6345 - val_acc: 0.6220\n",
      "Epoch 35/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6433 - acc: 0.6043 - val_loss: 0.6346 - val_acc: 0.6187\n",
      "Epoch 36/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6431 - acc: 0.6039 - val_loss: 0.6343 - val_acc: 0.6186\n",
      "Epoch 37/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6433 - acc: 0.6037 - val_loss: 0.6348 - val_acc: 0.6181\n",
      "Epoch 38/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6427 - acc: 0.6046 - val_loss: 0.6347 - val_acc: 0.6187\n",
      "Epoch 39/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6425 - acc: 0.6054 - val_loss: 0.6349 - val_acc: 0.6095\n",
      "Epoch 40/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6421 - acc: 0.6053 - val_loss: 0.6349 - val_acc: 0.6106\n",
      "Epoch 41/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6417 - acc: 0.6046 - val_loss: 0.6352 - val_acc: 0.6100\n",
      "Epoch 42/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6417 - acc: 0.6054 - val_loss: 0.6351 - val_acc: 0.6101\n",
      "Epoch 43/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6413 - acc: 0.6050 - val_loss: 0.6353 - val_acc: 0.6095\n",
      "Epoch 44/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6409 - acc: 0.6054 - val_loss: 0.6358 - val_acc: 0.6099\n",
      "Epoch 45/45\n",
      "568080/568080 [==============================] - 5s 9us/step - loss: 0.6404 - acc: 0.6057 - val_loss: 0.6361 - val_acc: 0.6096\n",
      "[[0.49850863]\n",
      " [0.4984931 ]\n",
      " [0.48059088]\n",
      " ...\n",
      " [0.6149153 ]\n",
      " [0.7224562 ]\n",
      " [0.67213887]]\n",
      "110365 0.6095829881248274\n",
      "trend_test_acc:\n",
      "55294 0.6107134967969958\n",
      "vol_test_acc:\n",
      "55071 0.6084521047398077\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.47277802]\n",
      " [0.49360445]\n",
      " [0.46345517]\n",
      " ...\n",
      " [0.52616113]\n",
      " [0.9280536 ]\n",
      " [0.9280536 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5393040596520299\n",
      "參考前一個答案 benchacc2:\n",
      "0.6187738193869097\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "18\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1235 MiB, count=158, average=8005 KiB\n",
      "(749490, 36, 6)\n",
      "(749490, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 561210 samples, validate on 186120 samples\n",
      "Epoch 1/45\n",
      "561210/561210 [==============================] - 14s 24us/step - loss: 1.5866 - acc: 0.4953 - val_loss: 0.7446 - val_acc: 0.5309\n",
      "Epoch 2/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.9299 - acc: 0.5105 - val_loss: 0.6943 - val_acc: 0.5380\n",
      "Epoch 3/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.7763 - acc: 0.5221 - val_loss: 0.6766 - val_acc: 0.5551\n",
      "Epoch 4/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.7219 - acc: 0.5342 - val_loss: 0.6668 - val_acc: 0.5637\n",
      "Epoch 5/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6969 - acc: 0.5475 - val_loss: 0.6611 - val_acc: 0.5882\n",
      "Epoch 6/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6843 - acc: 0.5577 - val_loss: 0.6566 - val_acc: 0.5907\n",
      "Epoch 7/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6763 - acc: 0.5660 - val_loss: 0.6524 - val_acc: 0.5970\n",
      "Epoch 8/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6697 - acc: 0.5745 - val_loss: 0.6486 - val_acc: 0.6002\n",
      "Epoch 9/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6650 - acc: 0.5802 - val_loss: 0.6457 - val_acc: 0.6016\n",
      "Epoch 10/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6615 - acc: 0.5843 - val_loss: 0.6437 - val_acc: 0.6033\n",
      "Epoch 11/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6585 - acc: 0.5877 - val_loss: 0.6424 - val_acc: 0.6039\n",
      "Epoch 12/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6566 - acc: 0.5898 - val_loss: 0.6412 - val_acc: 0.6050\n",
      "Epoch 13/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6551 - acc: 0.5907 - val_loss: 0.6403 - val_acc: 0.6059\n",
      "Epoch 14/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6538 - acc: 0.5926 - val_loss: 0.6395 - val_acc: 0.6067\n",
      "Epoch 15/45\n",
      "561210/561210 [==============================] - 5s 8us/step - loss: 0.6522 - acc: 0.5945 - val_loss: 0.6388 - val_acc: 0.6074\n",
      "Epoch 16/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6519 - acc: 0.5943 - val_loss: 0.6383 - val_acc: 0.6074\n",
      "Epoch 17/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6506 - acc: 0.5962 - val_loss: 0.6377 - val_acc: 0.6074\n",
      "Epoch 18/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6501 - acc: 0.5964 - val_loss: 0.6373 - val_acc: 0.6079\n",
      "Epoch 19/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6493 - acc: 0.5980 - val_loss: 0.6369 - val_acc: 0.6082\n",
      "Epoch 20/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6486 - acc: 0.5989 - val_loss: 0.6363 - val_acc: 0.6080\n",
      "Epoch 21/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6484 - acc: 0.5985 - val_loss: 0.6363 - val_acc: 0.6080\n",
      "Epoch 22/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6477 - acc: 0.6003 - val_loss: 0.6361 - val_acc: 0.6080\n",
      "Epoch 23/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6472 - acc: 0.6003 - val_loss: 0.6357 - val_acc: 0.6113\n",
      "Epoch 24/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6468 - acc: 0.6009 - val_loss: 0.6355 - val_acc: 0.6121\n",
      "Epoch 25/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6461 - acc: 0.6010 - val_loss: 0.6353 - val_acc: 0.6123\n",
      "Epoch 26/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6458 - acc: 0.6023 - val_loss: 0.6352 - val_acc: 0.6129\n",
      "Epoch 27/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6454 - acc: 0.6019 - val_loss: 0.6350 - val_acc: 0.6129\n",
      "Epoch 28/45\n",
      "561210/561210 [==============================] - 5s 8us/step - loss: 0.6450 - acc: 0.6029 - val_loss: 0.6347 - val_acc: 0.6131\n",
      "Epoch 29/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6447 - acc: 0.6024 - val_loss: 0.6345 - val_acc: 0.6129\n",
      "Epoch 30/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6443 - acc: 0.6038 - val_loss: 0.6341 - val_acc: 0.6132\n",
      "Epoch 31/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6437 - acc: 0.6036 - val_loss: 0.6337 - val_acc: 0.6130\n",
      "Epoch 32/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6433 - acc: 0.6046 - val_loss: 0.6334 - val_acc: 0.6127\n",
      "Epoch 33/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6432 - acc: 0.6034 - val_loss: 0.6335 - val_acc: 0.6128\n",
      "Epoch 34/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6429 - acc: 0.6036 - val_loss: 0.6334 - val_acc: 0.6119\n",
      "Epoch 35/45\n",
      "561210/561210 [==============================] - 5s 8us/step - loss: 0.6425 - acc: 0.6036 - val_loss: 0.6331 - val_acc: 0.6119\n",
      "Epoch 36/45\n",
      "561210/561210 [==============================] - 5s 8us/step - loss: 0.6421 - acc: 0.6059 - val_loss: 0.6329 - val_acc: 0.6118\n",
      "Epoch 37/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6420 - acc: 0.6044 - val_loss: 0.6325 - val_acc: 0.6113\n",
      "Epoch 38/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6412 - acc: 0.6055 - val_loss: 0.6321 - val_acc: 0.6116\n",
      "Epoch 39/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6408 - acc: 0.6045 - val_loss: 0.6317 - val_acc: 0.6112\n",
      "Epoch 40/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6403 - acc: 0.6057 - val_loss: 0.6312 - val_acc: 0.6099\n",
      "Epoch 41/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6398 - acc: 0.6061 - val_loss: 0.6312 - val_acc: 0.6100\n",
      "Epoch 42/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6394 - acc: 0.6060 - val_loss: 0.6310 - val_acc: 0.6102\n",
      "Epoch 43/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6389 - acc: 0.6051 - val_loss: 0.6306 - val_acc: 0.6083\n",
      "Epoch 44/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6384 - acc: 0.6068 - val_loss: 0.6300 - val_acc: 0.6080\n",
      "Epoch 45/45\n",
      "561210/561210 [==============================] - 5s 9us/step - loss: 0.6379 - acc: 0.6069 - val_loss: 0.6297 - val_acc: 0.6079\n",
      "[[0.4934938 ]\n",
      " [0.50362384]\n",
      " [0.50319844]\n",
      " ...\n",
      " [0.6315274 ]\n",
      " [0.70124006]\n",
      " [0.6989262 ]]\n",
      "113144 0.6079088759939824\n",
      "trend_test_acc:\n",
      "56637 0.6086073500967119\n",
      "vol_test_acc:\n",
      "56507 0.607210401891253\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.45048246]\n",
      " [0.48247215]\n",
      " [0.4830706 ]\n",
      " ...\n",
      " [0.5357865 ]\n",
      " [0.6598849 ]\n",
      " [0.6598849 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5392005157962605\n",
      "參考前一個答案 benchacc2:\n",
      "0.6175961745110681\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "19\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1248 MiB, count=158, average=8091 KiB\n",
      "(757590, 36, 6)\n",
      "(757590, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 568050 samples, validate on 187380 samples\n",
      "Epoch 1/45\n",
      "568050/568050 [==============================] - 14s 25us/step - loss: 1.0259 - acc: 0.5152 - val_loss: 0.6838 - val_acc: 0.5650\n",
      "Epoch 2/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.7511 - acc: 0.5309 - val_loss: 0.6744 - val_acc: 0.5835\n",
      "Epoch 3/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.7019 - acc: 0.5432 - val_loss: 0.6715 - val_acc: 0.5951\n",
      "Epoch 4/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6847 - acc: 0.5550 - val_loss: 0.6667 - val_acc: 0.6012\n",
      "Epoch 5/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6763 - acc: 0.5646 - val_loss: 0.6620 - val_acc: 0.6031\n",
      "Epoch 6/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6710 - acc: 0.5705 - val_loss: 0.6579 - val_acc: 0.6045\n",
      "Epoch 7/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6677 - acc: 0.5760 - val_loss: 0.6548 - val_acc: 0.6054\n",
      "Epoch 8/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6644 - acc: 0.5787 - val_loss: 0.6520 - val_acc: 0.6071\n",
      "Epoch 9/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6617 - acc: 0.5816 - val_loss: 0.6498 - val_acc: 0.6068\n",
      "Epoch 10/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6598 - acc: 0.5845 - val_loss: 0.6475 - val_acc: 0.6084\n",
      "Epoch 11/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6580 - acc: 0.5866 - val_loss: 0.6455 - val_acc: 0.6032\n",
      "Epoch 12/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6562 - acc: 0.5901 - val_loss: 0.6439 - val_acc: 0.6068\n",
      "Epoch 13/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6546 - acc: 0.5910 - val_loss: 0.6426 - val_acc: 0.6082\n",
      "Epoch 14/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6531 - acc: 0.5927 - val_loss: 0.6410 - val_acc: 0.6092\n",
      "Epoch 15/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6523 - acc: 0.5944 - val_loss: 0.6406 - val_acc: 0.6103\n",
      "Epoch 16/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6513 - acc: 0.5943 - val_loss: 0.6395 - val_acc: 0.6109\n",
      "Epoch 17/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6508 - acc: 0.5961 - val_loss: 0.6387 - val_acc: 0.6113\n",
      "Epoch 18/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6500 - acc: 0.5974 - val_loss: 0.6377 - val_acc: 0.6109\n",
      "Epoch 19/45\n",
      "568050/568050 [==============================] - 5s 8us/step - loss: 0.6489 - acc: 0.5979 - val_loss: 0.6374 - val_acc: 0.6116\n",
      "Epoch 20/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6483 - acc: 0.5982 - val_loss: 0.6366 - val_acc: 0.6183\n",
      "Epoch 21/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6479 - acc: 0.6000 - val_loss: 0.6362 - val_acc: 0.6116\n",
      "Epoch 22/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6471 - acc: 0.6016 - val_loss: 0.6353 - val_acc: 0.6119\n",
      "Epoch 23/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6463 - acc: 0.6015 - val_loss: 0.6347 - val_acc: 0.6124\n",
      "Epoch 24/45\n",
      "568050/568050 [==============================] - 5s 8us/step - loss: 0.6460 - acc: 0.6020 - val_loss: 0.6348 - val_acc: 0.6129\n",
      "Epoch 25/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6457 - acc: 0.6016 - val_loss: 0.6343 - val_acc: 0.6126\n",
      "Epoch 26/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6453 - acc: 0.6028 - val_loss: 0.6335 - val_acc: 0.6128\n",
      "Epoch 27/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6451 - acc: 0.6021 - val_loss: 0.6334 - val_acc: 0.6130\n",
      "Epoch 28/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6442 - acc: 0.6021 - val_loss: 0.6332 - val_acc: 0.6130\n",
      "Epoch 29/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6441 - acc: 0.6034 - val_loss: 0.6324 - val_acc: 0.6134\n",
      "Epoch 30/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6438 - acc: 0.6039 - val_loss: 0.6330 - val_acc: 0.6142\n",
      "Epoch 31/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6428 - acc: 0.6046 - val_loss: 0.6318 - val_acc: 0.6169\n",
      "Epoch 32/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6432 - acc: 0.6043 - val_loss: 0.6316 - val_acc: 0.6157\n",
      "Epoch 33/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6424 - acc: 0.6040 - val_loss: 0.6313 - val_acc: 0.6168\n",
      "Epoch 34/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6422 - acc: 0.6042 - val_loss: 0.6313 - val_acc: 0.6181\n",
      "Epoch 35/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6419 - acc: 0.6047 - val_loss: 0.6313 - val_acc: 0.6193\n",
      "Epoch 36/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6408 - acc: 0.6052 - val_loss: 0.6312 - val_acc: 0.6126\n",
      "Epoch 37/45\n",
      "568050/568050 [==============================] - 5s 8us/step - loss: 0.6404 - acc: 0.6056 - val_loss: 0.6307 - val_acc: 0.6121\n",
      "Epoch 38/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6397 - acc: 0.6052 - val_loss: 0.6304 - val_acc: 0.6125\n",
      "Epoch 39/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6391 - acc: 0.6054 - val_loss: 0.6297 - val_acc: 0.6128\n",
      "Epoch 40/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6380 - acc: 0.6055 - val_loss: 0.6288 - val_acc: 0.6129\n",
      "Epoch 41/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6371 - acc: 0.6065 - val_loss: 0.6281 - val_acc: 0.6186\n",
      "Epoch 42/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6358 - acc: 0.6057 - val_loss: 0.6281 - val_acc: 0.6132\n",
      "Epoch 43/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6348 - acc: 0.6072 - val_loss: 0.6282 - val_acc: 0.6128\n",
      "Epoch 44/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6335 - acc: 0.6066 - val_loss: 0.6258 - val_acc: 0.6147\n",
      "Epoch 45/45\n",
      "568050/568050 [==============================] - 5s 9us/step - loss: 0.6326 - acc: 0.6068 - val_loss: 0.6245 - val_acc: 0.6155\n",
      "[[0.50028807]\n",
      " [0.50498796]\n",
      " [0.4894211 ]\n",
      " ...\n",
      " [0.62363863]\n",
      " [0.7898101 ]\n",
      " [0.76175916]]\n",
      "115328 0.6154765716725371\n",
      "trend_test_acc:\n",
      "57658 0.6154125306863059\n",
      "vol_test_acc:\n",
      "57670 0.6155406126587682\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.48013476]\n",
      " [0.49381143]\n",
      " [0.47210804]\n",
      " ...\n",
      " [0.5047748 ]\n",
      " [0.9199541 ]\n",
      " [0.9199541 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5399028711708826\n",
      "參考前一個答案 benchacc2:\n",
      "0.6186892944818017\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "20\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=1214 MiB, count=158, average=7870 KiB\n",
      "(736890, 36, 6)\n",
      "(736890, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 558870 samples, validate on 175860 samples\n",
      "Epoch 1/45\n",
      "558870/558870 [==============================] - 14s 25us/step - loss: 1.4125 - acc: 0.5078 - val_loss: 0.6833 - val_acc: 0.5629\n",
      "Epoch 2/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.8488 - acc: 0.5148 - val_loss: 0.6723 - val_acc: 0.5651\n",
      "Epoch 3/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.7306 - acc: 0.5258 - val_loss: 0.6640 - val_acc: 0.5918\n",
      "Epoch 4/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6947 - acc: 0.5422 - val_loss: 0.6586 - val_acc: 0.6067\n",
      "Epoch 5/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6824 - acc: 0.5536 - val_loss: 0.6550 - val_acc: 0.6077\n",
      "Epoch 6/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6758 - acc: 0.5615 - val_loss: 0.6523 - val_acc: 0.6093\n",
      "Epoch 7/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6713 - acc: 0.5673 - val_loss: 0.6503 - val_acc: 0.6088\n",
      "Epoch 8/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6679 - acc: 0.5722 - val_loss: 0.6483 - val_acc: 0.6058\n",
      "Epoch 9/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6652 - acc: 0.5761 - val_loss: 0.6462 - val_acc: 0.6081\n",
      "Epoch 10/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6627 - acc: 0.5788 - val_loss: 0.6448 - val_acc: 0.6145\n",
      "Epoch 11/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6608 - acc: 0.5808 - val_loss: 0.6434 - val_acc: 0.6115\n",
      "Epoch 12/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6585 - acc: 0.5843 - val_loss: 0.6425 - val_acc: 0.6140\n",
      "Epoch 13/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6570 - acc: 0.5863 - val_loss: 0.6417 - val_acc: 0.6147\n",
      "Epoch 14/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6560 - acc: 0.5877 - val_loss: 0.6408 - val_acc: 0.6153\n",
      "Epoch 15/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6545 - acc: 0.5900 - val_loss: 0.6397 - val_acc: 0.6161\n",
      "Epoch 16/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6533 - acc: 0.5911 - val_loss: 0.6389 - val_acc: 0.6163\n",
      "Epoch 17/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6522 - acc: 0.5921 - val_loss: 0.6381 - val_acc: 0.6076\n",
      "Epoch 18/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6515 - acc: 0.5930 - val_loss: 0.6374 - val_acc: 0.6076\n",
      "Epoch 19/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6508 - acc: 0.5940 - val_loss: 0.6374 - val_acc: 0.6078\n",
      "Epoch 20/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6501 - acc: 0.5958 - val_loss: 0.6366 - val_acc: 0.6077\n",
      "Epoch 21/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6491 - acc: 0.5969 - val_loss: 0.6360 - val_acc: 0.6078\n",
      "Epoch 22/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6488 - acc: 0.5977 - val_loss: 0.6354 - val_acc: 0.6077\n",
      "Epoch 23/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6482 - acc: 0.5977 - val_loss: 0.6350 - val_acc: 0.6078\n",
      "Epoch 24/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6478 - acc: 0.5999 - val_loss: 0.6347 - val_acc: 0.6083\n",
      "Epoch 25/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6473 - acc: 0.6001 - val_loss: 0.6346 - val_acc: 0.6080\n",
      "Epoch 26/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6470 - acc: 0.5996 - val_loss: 0.6340 - val_acc: 0.6075\n",
      "Epoch 27/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6466 - acc: 0.6009 - val_loss: 0.6340 - val_acc: 0.6112\n",
      "Epoch 28/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6464 - acc: 0.6010 - val_loss: 0.6339 - val_acc: 0.6112\n",
      "Epoch 29/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6460 - acc: 0.6009 - val_loss: 0.6337 - val_acc: 0.6098\n",
      "Epoch 30/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6459 - acc: 0.6013 - val_loss: 0.6335 - val_acc: 0.6101\n",
      "Epoch 31/45\n",
      "558870/558870 [==============================] - 5s 8us/step - loss: 0.6454 - acc: 0.6032 - val_loss: 0.6333 - val_acc: 0.6102\n",
      "Epoch 32/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6448 - acc: 0.6023 - val_loss: 0.6333 - val_acc: 0.6079\n",
      "Epoch 33/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6450 - acc: 0.6029 - val_loss: 0.6333 - val_acc: 0.6036\n",
      "Epoch 34/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6449 - acc: 0.6028 - val_loss: 0.6333 - val_acc: 0.6064\n",
      "Epoch 35/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6446 - acc: 0.6035 - val_loss: 0.6331 - val_acc: 0.6063\n",
      "Epoch 36/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6442 - acc: 0.6042 - val_loss: 0.6327 - val_acc: 0.6064\n",
      "Epoch 37/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6440 - acc: 0.6037 - val_loss: 0.6324 - val_acc: 0.6070\n",
      "Epoch 38/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6436 - acc: 0.6035 - val_loss: 0.6322 - val_acc: 0.6070\n",
      "Epoch 39/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6432 - acc: 0.6039 - val_loss: 0.6325 - val_acc: 0.6069\n",
      "Epoch 40/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6430 - acc: 0.6051 - val_loss: 0.6321 - val_acc: 0.6072\n",
      "Epoch 41/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6428 - acc: 0.6048 - val_loss: 0.6320 - val_acc: 0.6071\n",
      "Epoch 42/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6426 - acc: 0.6049 - val_loss: 0.6320 - val_acc: 0.6081\n",
      "Epoch 43/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6425 - acc: 0.6049 - val_loss: 0.6334 - val_acc: 0.6091\n",
      "Epoch 44/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6422 - acc: 0.6048 - val_loss: 0.6329 - val_acc: 0.6129\n",
      "Epoch 45/45\n",
      "558870/558870 [==============================] - 5s 9us/step - loss: 0.6414 - acc: 0.6058 - val_loss: 0.6328 - val_acc: 0.6127\n",
      "[[0.5035305 ]\n",
      " [0.5032701 ]\n",
      " [0.48946157]\n",
      " ...\n",
      " [0.6239649 ]\n",
      " [0.706073  ]\n",
      " [0.72273916]]\n",
      "107744 0.6126691686568861\n",
      "trend_test_acc:\n",
      "53985 0.6139542818150802\n",
      "vol_test_acc:\n",
      "53759 0.6113840554986921\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.4817727 ]\n",
      " [0.43900585]\n",
      " [0.4843876 ]\n",
      " ...\n",
      " [0.48453438]\n",
      " [0.6263689 ]\n",
      " [0.6263689 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5391732059592858\n",
      "參考前一個答案 benchacc2:\n",
      "0.6161605822813602\n"
     ]
    }
   ],
   "source": [
    "for z in range(head,tail,1):\n",
    "    \"\"\"\n",
    "     V\n",
    "    \"\"\"\n",
    "    n=daynum[tail]-daynum[head]\n",
    "    df = pd.read_csv('data/JPY_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    print(df.shape)  \n",
    "    jpy5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        jpy5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/EUR_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eur5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eur5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/CHF_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    chf5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        chf5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/CAD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    cad5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        cad5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/GBP_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    gbp5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        gbp5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/HKD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    hkd5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        hkd5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    print('finish dataread')\n",
    "    Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "    for p in range(n-l+1):\n",
    "        Train_data[p,0,:]=chf5months[p:p+l]\n",
    "        Train_data[p,1,:]=cad5months[p:p+l]\n",
    "        Train_data[p,2,:]=gbp5months[p:p+l]\n",
    "        Train_data[p,3,:]=jpy5months[p:p+l]\n",
    "        Train_data[p,4,:]=eur5months[p:p+l]\n",
    "        Train_data[p,5,:]=hkd5months[p:p+l]\n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((currencynum,l,1))\n",
    "    visual_conv = ConvolutionNetworks([20,10],[(1,kn),(1,kn)])(visual_scene)\n",
    "    print(K.int_shape(visual_conv))\n",
    "    tag = build_tag(visual_conv)\n",
    "    visual_conv = Concatenate()([tag, visual_conv])\n",
    "    print(K.int_shape(visual_conv))\n",
    "    \n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    f = shapes[2]\n",
    "    features= []\n",
    "    #features = np.zeros(0)\n",
    "    for k1 in range(w):\n",
    "        for k2 in range(f):\n",
    "            def get_feature(t):\n",
    "                return t[:, k1, k2, :]\n",
    "            #get_feature_layer = Lambda(get_feature)\n",
    "            features.append(Lambda(get_feature)(visual_conv))\n",
    "    \n",
    "      \n",
    "    input2 = Input((14,))\n",
    "    onehot_encode_question = input2\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode_question]))    \n",
    "    \n",
    "     \n",
    "    g_MLP = get_MLP(4, get_dense(4,MLP_unit))\n",
    "    \n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "    \n",
    "    combined_relation = Add()(mid_relations)\n",
    "    \n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "    \n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "    \n",
    "    \n",
    "    #model = Model(inputs=[visual_scene])\n",
    "    model = Model(inputs=[visual_scene, input2, tag], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "    #model.summary()\n",
    "    print(z)\n",
    "    fit_show(Train_data,daynum[z],daynum[z+3],daynum[z+4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
