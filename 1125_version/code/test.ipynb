{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##冰激淋+擔擔麵+蜂蜜檸檬適用\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #使用第0張顯卡 ##冰激淋+擔擔麵+蜂蜜檸檬適用 0or1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Flatten, Convolution1D, MaxPooling1D, Activation, BatchNormalization,\\\n",
    "Lambda, Concatenate, Add, Conv2D, Conv1D,TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基本設定\n",
    "\"\"\"\n",
    "#n=148335 #用2016 1~3月的資料 288x(31+29+31+30+31+30+...)=52416 210528-1(all) 差值在減一  \n",
    "l=144 #區間為12小時\n",
    "currency = [\"SEK\",\"CHF\",\"CAD\",\"GBP\",\"JPY\",\"EUR\",\"AUD\",\"SGD\",\"HKD\",\"NZD\"]\n",
    "#SEK:瑞典克朗  CHF:瑞士法郎 CAD:加拿大幣 GBP:英鎊 \n",
    "#currency = [\"BTC\",\"DASH\",\"ETH\",\"LTC\",\"JBY\",\"GBP\",\"EUR\",\"AUD\",\"US\"]\n",
    "currencynum = len(currency)\n",
    "month = [0,31,60,91,121,152,182,213,244,274,305,335,366,397,425,456,486,517,547,578,609,639,670,700,731] #2016是閏年 366天\n",
    "daynum = [0,5802,11745,18351,24380,30579,36861,42795,49201,55342,61230,67400,73732,79984,85726,92343,98127,104692,111028,117076,123700,129807,136083,142401,148335]\n",
    "question = [\"trand\",\"volatility\"]\n",
    "# 貨幣組合，1 : P, 0 : C\n",
    "M=0\n",
    "head = 12\n",
    "tail = 17\n",
    "epochs = 20\n",
    "batch_size = 4096\n",
    "\n",
    "all_cur_pair = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "if(M==0):\n",
    "    all_cur_pair_P = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "elif(M==1):\n",
    "    all_cur_pair_P = list(permutations(currency,2))# (Pn取2) 問題有先後順序時使用  \n",
    "all_question= list(permutations(question,1))\n",
    "np.set_printoptions(suppress=True)#不要用科學符號輸出\n",
    "lastepoch_train_acc = []\n",
    "lastepoch_test_acc = []\n",
    "lastepoch_train_loss = []\n",
    "lastepoch_test_loss = []\n",
    "total_test_vol = []\n",
    "total_test_trend = []\n",
    "train_length=3 \n",
    "test_length=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\n",
    "str, onehotcode, company code轉換\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "currencylist = {}\n",
    "questionlist = {}\n",
    "for i in range(len(currency)):\n",
    "    currencylist[i] = currency[i]\n",
    "\n",
    "for i in range(len(question)):\n",
    "    questionlist[i] = question[i]\n",
    "\n",
    "def str_to_currency(cur):\n",
    "    return {v: k for k, v in currencylist.items()}[cur]\n",
    "\n",
    "def str_to_question(q):\n",
    "    return {v: k for k, v in questionlist.items()}[q]\n",
    "\n",
    "\n",
    "def one_hot_currency(currencylist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(currencylist)))\n",
    "    for i in range(len(currencylist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "def one_hot_question(questionlist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(questionlist)))\n",
    "    for i in range(len(questionlist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "onehotcurrency = one_hot_currency(currencylist)\n",
    "onehotquestion = one_hot_question(questionlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Q&A\n",
    "\"\"\"\n",
    "vol='volatility'\n",
    "def set_question(com1, com2, typeq):\n",
    "\n",
    "    return np.concatenate((onehotquestion[str_to_question(typeq)],onehotcurrency[str_to_currency(com1)], onehotcurrency[str_to_currency(com2)]))\n",
    "\n",
    "qtype = ['big','small']\n",
    "HVqtype = ['big','small']\n",
    "\n",
    "\"\"\"比漲幅程度類問題\"\"\"\n",
    "def set_question_and_answer_pair(data, data2, n, all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_pair = {}\n",
    "    a_pair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    #data_sum=np.sum(data,axis=1)\n",
    "    data2_sum=np.sum(data2,axis=1)\n",
    "    outcome[0]=(data2_sum[0])\n",
    "    outcome[1]=(data2_sum[1])\n",
    "    outcome[2]=(data2_sum[2])\n",
    "    outcome[3]=(data2_sum[3])\n",
    "    outcome[4]=(data2_sum[4])\n",
    "    outcome[5]=(data2_sum[5])\n",
    "    outcome[6]=(data2_sum[6])\n",
    "    outcome[7]=(data2_sum[7])\n",
    "    outcome[8]=(data2_sum[8])\n",
    "    outcome[9]=(data2_sum[9])\n",
    "    tmp_q = []\n",
    "    tmp_a = []\n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1],all_question[0][0]))\n",
    "    q_pair[i] = tmp_q        \n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "            tmp_a.append(1)\n",
    "        else:\n",
    "            tmp_a.append(0)\n",
    "    a_pair[i] = tmp_a\n",
    "    return (data, q_pair, a_pair)\n",
    "\n",
    "\n",
    "\"\"\"History Volatility類問題\"\"\"\n",
    "def set_HVquestion_and_HVanswer_pair(data, data2,all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_HVpair = {}\n",
    "    a_HVpair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    data2_std=np.std(data2,axis=1) \n",
    "    outcome[0]=data2_std[0]\n",
    "    outcome[1]=data2_std[1]\n",
    "    outcome[2]=data2_std[2]\n",
    "    outcome[3]=data2_std[3]\n",
    "    outcome[4]=data2_std[4]\n",
    "    outcome[5]=data2_std[5]     \n",
    "    outcome[6]=data2_std[6]\n",
    "    outcome[7]=data2_std[7]\n",
    "    outcome[8]=data2_std[8]     \n",
    "    outcome[9]=data2_std[9]     \n",
    "    tmp_q = []\n",
    "    tmp_a = []   \n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1],all_question[1][0]))\n",
    "    q_HVpair[i] = tmp_q\n",
    "    for j in range(len(all_cur_pair_P)):\n",
    "        if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "            tmp_a.append(1)\n",
    "        else:\n",
    "            tmp_a.append(0)\n",
    "                \n",
    "    a_HVpair[i] = tmp_a    \n",
    "    return (data, q_HVpair, a_HVpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "網路函數\n",
    "\"\"\"\n",
    "def ConvolutionNetworks(filter_num,kernel_size):\n",
    "    def conv(model):\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = (MaxPooling1D(pool_size=7))(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    tag = np.zeros((d,1))\n",
    "    for i in range(d):\n",
    "        tag[i] = float(int(i%d))/(d-1)*2-1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    # 處理訓練資料\n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(\"[Training model......]\")\n",
    "\n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 18, 1)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "12\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=46.6 GiB, count=2, average=23.3 GiB\n",
      "[Training model......]\n"
     ]
    }
   ],
   "source": [
    "for z in range(12,13,1):\n",
    "    n=daynum[tail]-daynum[head]\n",
    "    df = pd.read_csv('data/JPY_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    print(df.shape)  \n",
    "    jpy5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        jpy5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/EUR_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eur5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eur5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/AUD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    aud5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        aud5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/CHF_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    chf5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        chf5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/SEK_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    sek5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        sek5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/CAD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    cad5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        cad5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/GBP_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    gbp5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        gbp5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/SGD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    sgd5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        sgd5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/HKD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    hkd5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        hkd5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/NZD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    nzd5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        nzd5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    print('finish dataread')\n",
    "    Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "    for p in range(n-l+1):\n",
    "        Train_data[p,0,:]=sek5months[p:p+l]\n",
    "        Train_data[p,1,:]=chf5months[p:p+l]\n",
    "        Train_data[p,2,:]=cad5months[p:p+l]\n",
    "        Train_data[p,3,:]=gbp5months[p:p+l]\n",
    "        Train_data[p,4,:]=jpy5months[p:p+l]\n",
    "        Train_data[p,5,:]=eur5months[p:p+l]\n",
    "        Train_data[p,6,:]=aud5months[p:p+l]    \n",
    "        Train_data[p,7,:]=sgd5months[p:p+l]\n",
    "        Train_data[p,8,:]=hkd5months[p:p+l]\n",
    "        Train_data[p,9,:]=nzd5months[p:p+l] \n",
    "    \n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((l,currencynum))\n",
    "    visual_conv = ConvolutionNetworks(20,5)(visual_scene)\n",
    "    tag = build_tag(visual_conv)\n",
    "    visual_conv = Concatenate()([visual_conv, tag])\n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    features= []\n",
    "    for k1 in range(w):\n",
    "        def get_feature(t):\n",
    "            return t[:, k1, :]\n",
    "        get_feature_layer = Lambda(get_feature)\n",
    "        features.append(get_feature_layer(visual_conv))\n",
    "\n",
    "    input2 = Input((24,))\n",
    "    onehot_encode = input2   \n",
    "\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode]))    \n",
    "\n",
    "\n",
    "    g_MLP = get_MLP(5, get_dense(5,MLP_unit))\n",
    "    f_MLP = get_MLP(5, get_dense(5,MLP_unit))\n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "\n",
    "    combined_relation = Add()(mid_relations)\n",
    "\n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "\n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "\n",
    "    model = Model(inputs=[visual_scene, input2, tag], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "\n",
    "    print(z)\n",
    "    \n",
    "    traindata = Train_data\n",
    "    m1 = daynum[z]\n",
    "    m2 = daynum[z+3]\n",
    "    m3 = daynum[z+4]\n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    # 處理訓練資料\n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(\"[Training model......]\")\n",
    "\n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_q[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_q[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
