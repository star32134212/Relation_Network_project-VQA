{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##冰激淋+擔擔麵+蜂蜜檸檬適用\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #使用第0張顯卡 ##冰激淋+擔擔麵+蜂蜜檸檬適用 0or1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Flatten, Convolution1D, MaxPooling1D, Activation, BatchNormalization,\\\n",
    "Lambda, Concatenate, Add, Conv2D, Conv1D,TimeDistributed,MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基本設定\n",
    "\"\"\"\n",
    "#n=148335 #用2016 1~3月的資料 288x(31+29+31+30+31+30+...)=52416 210528-1(all) 差值在減一  \n",
    "l=36 #區間為12小時\n",
    "currency = [\"BTC\",\"DASH\",\"ETH\",\"JPY\",\"EUR\",\"AUD\"]\n",
    "#SEK:瑞典克朗  CHF:瑞士法郎 CAD:加拿大幣 GBP:英鎊 \n",
    "#currency = [\"BTC\",\"DASH\",\"ETH\",\"LTC\",\"JBY\",\"GBP\",\"EUR\",\"AUD\",\"US\"]\n",
    "currencynum = len(currency)\n",
    "month = [0,31,60,91,121,152,182,213,244,274,305,335,366,397,425,456,486,517,547,578,609,639,670,700,731] #2016是閏年 366天\n",
    "daynum = [0,5802,11745,18351,24380,30579,36861,42795,49201,55342,61230,67400,73732,79984,85726,92343,98127,104692,111028,117076,123700,129807,136083,142401,148335]\n",
    "question = [\"trand\",\"volatility\"]\n",
    "# 貨幣組合，1 : P, 0 : C\n",
    "M=0\n",
    "head = 9\n",
    "tail = 21\n",
    "epochs = 70\n",
    "batch_size = 4096\n",
    "pl=4\n",
    "kn=5\n",
    "all_cur_pair = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "if(M==0):\n",
    "    all_cur_pair_P = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "elif(M==1):\n",
    "    all_cur_pair_P = list(permutations(currency,2))# (Pn取2) 問題有先後順序時使用  \n",
    "all_question= list(permutations(question,1))\n",
    "np.set_printoptions(suppress=True)#不要用科學符號輸出\n",
    "lastepoch_train_acc = []\n",
    "lastepoch_test_acc = []\n",
    "lastepoch_train_loss = []\n",
    "lastepoch_test_loss = []\n",
    "total_test_vol = []\n",
    "total_test_trend = []\n",
    "train_length=3 \n",
    "test_length=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\n",
    "str, onehotcode, company code轉換\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "currencylist = {}\n",
    "questionlist = {}\n",
    "for i in range(len(currency)):\n",
    "    currencylist[i] = currency[i]\n",
    "\n",
    "for i in range(len(question)):\n",
    "    questionlist[i] = question[i]\n",
    "\n",
    "def str_to_currency(cur):\n",
    "    return {v: k for k, v in currencylist.items()}[cur]\n",
    "\n",
    "def str_to_question(q):\n",
    "    return {v: k for k, v in questionlist.items()}[q]\n",
    "\n",
    "\n",
    "def one_hot_currency(currencylist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(currencylist)))\n",
    "    for i in range(len(currencylist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "def one_hot_question(questionlist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(questionlist)))\n",
    "    for i in range(len(questionlist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "onehotcurrency = one_hot_currency(currencylist)\n",
    "onehotquestion = one_hot_question(questionlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q&A\n",
    "\"\"\"\n",
    "vol='volatility'\n",
    "def set_question(com1, com2, condition,typeq):\n",
    "    # set condition\n",
    "    def rise_of_fall():\n",
    "        #目前問題設定只有三種設定：前者大(big)-0、後者大(small)-1、一樣(s)-2\n",
    "        tmp = np.eye((2))\n",
    "        d = {0:tmp[0],1:tmp[1]}\n",
    "        return d\n",
    "    # 拼接問題\n",
    "    return np.concatenate((onehotquestion[str_to_question(typeq)],onehotcurrency[str_to_currency(com1)], onehotcurrency[str_to_currency(com2)], rise_of_fall()[condition]))\n",
    "\n",
    "qtype = ['big','small']\n",
    "HVqtype = ['big','small']\n",
    "\n",
    "\"\"\"比漲幅程度類問題\"\"\"\n",
    "def set_question_and_answer_pair(data, data2, n, all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_pair = {}\n",
    "    a_pair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    #data_sum=np.sum(data,axis=1)\n",
    "    data2_sum=np.sum(data2,axis=1)\n",
    "    outcome[0]=(data2_sum[0])\n",
    "    outcome[1]=(data2_sum[1])\n",
    "    outcome[2]=(data2_sum[2])\n",
    "    outcome[3]=(data2_sum[3])\n",
    "    outcome[4]=(data2_sum[4])\n",
    "    outcome[5]=(data2_sum[5])\n",
    "    # question_type (目前有3種小問題：前者大、後者大或者一樣)\n",
    "    for i in range(2):\n",
    "        tmp_q = []\n",
    "        tmp_a = []\n",
    "            \n",
    "        #先塞question pairs\n",
    "        for j in range(len(all_cur_pair_P)):\n",
    "            tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1], i, all_question[0][0]))\n",
    "        q_pair[i] = tmp_q\n",
    "        \n",
    "        #再塞answer pairs\n",
    "        if(i == 0): #同漲\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "        elif(i == 1):\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] < outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "                    \n",
    "        a_pair[i] = tmp_a\n",
    "        \n",
    "    return (data, q_pair, a_pair)\n",
    "\n",
    "\n",
    "\"\"\"History Volatility類問題\"\"\"\n",
    "def set_HVquestion_and_HVanswer_pair(data, data2,all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_HVpair = {}\n",
    "    a_HVpair = {}\n",
    "    outcome=np.zeros((currencynum,1))\n",
    "    for c in range(currencynum):\n",
    "        data2[c]=data2[c]-data2[c][0]\n",
    "    data2_std=np.std(data2,axis=1) \n",
    "    outcome[0]=data2_std[0]\n",
    "    outcome[1]=data2_std[1]\n",
    "    outcome[2]=data2_std[2]\n",
    "    outcome[3]=data2_std[3]\n",
    "    outcome[4]=data2_std[4]\n",
    "    outcome[5]=data2_std[5]     \n",
    "    # question_type (目前有3種小問題：前者大、後者大或者一樣)\n",
    "    for i in range(2):\n",
    "        tmp_q = []\n",
    "        tmp_a = []\n",
    "        \n",
    "        #先塞question pairs\n",
    "        for j in range(len(all_cur_pair_P)):\n",
    "            tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1], i,all_question[1][0]))\n",
    "        q_HVpair[i] = tmp_q\n",
    "        \n",
    "        #再塞answer pairs\n",
    "        if(i == 0): #前者大\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] >= outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "        elif(i == 1):\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] < outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "                    \n",
    "        a_HVpair[i] = tmp_a\n",
    "        \n",
    "    return (data, q_HVpair, a_HVpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "此處開始寫rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(v.shape)\n",
    "    v = v.reshape(len(v),currencynum,l,1)\n",
    "    print(v.shape)\n",
    "    print(\"[Training model......]\")\n",
    "    \n",
    "    #Train_v=v[:]\n",
    "    \n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    history = model.fit([Train_v, Train_q], Train_a,validation_data=([Test_v,Test_q],Test_a),batch_size=batch_size ,epochs = epochs,shuffle=False)\n",
    "    pred = model.predict([Test_v, Test_q])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i] <= 0.5:\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "            \n",
    "        if pred[i] == Test_a[i]:\n",
    "            count+=1\n",
    "    print(count,count/pred.shape[0])\n",
    "    \"\"\"\n",
    "    分開兩種問題的test集\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    v_trend=[]\n",
    "    q_trend=[]\n",
    "    a_trend=[]\n",
    "    v_vol=[]\n",
    "    q_vol=[]\n",
    "    a_vol=[]\n",
    "    trend_count=0\n",
    "    vol_count=0\n",
    "    for ii in range(0,len(Test_q)):\n",
    "        if(flag==0):\n",
    "            #print(0)\n",
    "            v_trend.append(Test_v[ii])\n",
    "            q_trend.append(Test_q[ii])\n",
    "            a_trend.append(Test_a[ii])\n",
    "            trend_count=trend_count+1\n",
    "            if(trend_count==90):\n",
    "                trend_count=0\n",
    "                flag=1\n",
    "        elif(flag==1):\n",
    "            #print(1)\n",
    "            v_vol.append(Test_v[ii])\n",
    "            q_vol.append(Test_q[ii])\n",
    "            a_vol.append(Test_a[ii])\n",
    "            vol_count=vol_count+1\n",
    "            if(vol_count==90):\n",
    "                vol_count=0\n",
    "                flag=0\n",
    "    \n",
    "    \"\"\"\n",
    "    分開兩種問題的predict正確率\n",
    "    \"\"\"\n",
    "    pred_trend = model.predict([v_trend, q_trend])\n",
    "    count = 0\n",
    "    for i in range(pred_trend.shape[0]):\n",
    "        if pred_trend[i] <= 0.5:\n",
    "            pred_trend[i] = 0\n",
    "        else:\n",
    "            pred_trend[i] = 1\n",
    "            \n",
    "        if pred_trend[i] == a_trend[i]:\n",
    "            count+=1\n",
    "    print(\"trend_test_acc:\")\n",
    "    print(count,count/pred_trend.shape[0])    \n",
    "    total_test_trend.append(count/pred_trend.shape[0])\n",
    "\n",
    "    pred_vol = model.predict([v_vol, q_vol])\n",
    "    count = 0\n",
    "    for i in range(pred_vol.shape[0]):\n",
    "        if pred_vol[i] <= 0.5:\n",
    "            pred_vol[i] = 0\n",
    "        else:\n",
    "            pred_vol[i] = 1\n",
    "            \n",
    "        if pred_vol[i] == a_vol[i]:\n",
    "            count+=1\n",
    "    print(\"vol_test_acc:\")\n",
    "    print(count,count/pred_vol.shape[0]) \n",
    "    total_test_vol.append(count/pred_vol.shape[0])\n",
    "    \n",
    "    print(\"Reverse section--------------------------------------------------------------------------\")\n",
    "    Test_q_reverse=Test_q\n",
    "    temp=Test_q_reverse[:,2:8]\n",
    "    Test_q_reverse[:,2:8]=Test_q_reverse[:,8:14]\n",
    "    Test_q_reverse[:,8:14]=temp\n",
    "    Test_a_reverse=-1*Test_a+1\n",
    "    pred = model.predict([Test_v, Test_q_reverse])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i] <= 0.5:\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "            \n",
    "        if pred[i] == Test_a_reverse[i]:\n",
    "            count+=1\n",
    "    print(count,count/pred.shape[0])\n",
    "\n",
    "    \"\"\"\n",
    "    分開兩種問題的test集_reverse\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    v_trend=[]\n",
    "    q_trend=[]\n",
    "    a_trend=[]\n",
    "    v_vol=[]\n",
    "    q_vol=[]\n",
    "    a_vol=[]\n",
    "    trend_count=0\n",
    "    vol_count=0\n",
    "    cycle = ((currencynum*(currencynum-1))/2)*len(question)*len(qtype)\n",
    "    for ii in range(0,len(Test_q)):\n",
    "        if(flag==0):\n",
    "            #print(0)\n",
    "            v_trend.append(Test_v[ii])\n",
    "            q_trend.append(Test_q[ii])\n",
    "            a_trend.append(Test_a[ii])\n",
    "            trend_count=trend_count+1\n",
    "            if(trend_count==cycle):\n",
    "                trend_count=0\n",
    "                flag=1\n",
    "        elif(flag==1):\n",
    "            #print(1)\n",
    "            v_vol.append(Test_v[ii])\n",
    "            q_vol.append(Test_q[ii])\n",
    "            a_vol.append(Test_a[ii])\n",
    "            vol_count=vol_count+1\n",
    "            if(vol_count==cycle):\n",
    "                vol_count=0\n",
    "                flag=0\n",
    "    \n",
    "    \"\"\"\n",
    "    分開兩種問題的predict正確率_reverse\n",
    "    \"\"\"\n",
    "    pred_trend = model.predict([v_trend, q_trend])\n",
    "    count = 0\n",
    "    for i in range(pred_trend.shape[0]):\n",
    "        if pred_trend[i] <= 0.5:\n",
    "            pred_trend[i] = 0\n",
    "        else:\n",
    "            pred_trend[i] = 1\n",
    "            \n",
    "        if pred_trend[i] == a_trend[i]:\n",
    "            count+=1\n",
    "    print(\"trend_test_acc:\")\n",
    "    print(count,count/pred_trend.shape[0])    \n",
    "    total_test_trend.append(count/pred_trend.shape[0])\n",
    "\n",
    "    pred_vol = model.predict([v_vol, q_vol])\n",
    "    count = 0\n",
    "    for i in range(pred_vol.shape[0]):\n",
    "        if pred_vol[i] <= 0.5:\n",
    "            pred_vol[i] = 0\n",
    "        else:\n",
    "            pred_vol[i] = 1\n",
    "            \n",
    "        if pred_vol[i] == a_vol[i]:\n",
    "            count+=1\n",
    "    print(\"vol_test_acc:\")\n",
    "    print(count,count/pred_vol.shape[0]) \n",
    "    total_test_vol.append(count/pred_vol.shape[0])\n",
    "    \n",
    "    \"\"\"\n",
    "    benchmark1\n",
    "    \"\"\"\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]):\n",
    "        benchacc=benchacc+Test_a[i]\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"猜答案多的那邊 benchacc1:\")\n",
    "    print(benchacc)\n",
    "\n",
    "    \"\"\"\n",
    "    benchmark2\n",
    "    \"\"\"\n",
    "    #第一個直接猜1\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]-1):\n",
    "        if(Test_a[i]!=Test_a[i+1]):\n",
    "            benchacc=benchacc+1\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"參考前一個答案 benchacc2:\")\n",
    "    print(benchacc)    \n",
    "    \"\"\"\n",
    "    畫圖\n",
    "    \"\"\"\n",
    "    benchfunction=np.ones(a.shape[0])\n",
    "    benchfunction=benchfunction*benchacc\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    lastepoch_train_loss.append(history.history['loss'][-1])\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    lastepoch_test_loss.append(history.history['val_loss'][-1])\n",
    "    print(\"loss:\")\n",
    "    print(history.history['loss'][-1])\n",
    "    print(\"val_loss:\")\n",
    "    print(history.history['val_loss'][-1])\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"loss\") \n",
    "    #plt.title(\"The Title\") \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)\n",
    "    plt.show()    \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "12\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2405 MiB, count=2, average=1202 MiB\n",
      "(1459380, 36, 6)\n",
      "(1459380, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1112340 samples, validate on 342720 samples\n",
      "Epoch 1/40\n",
      "1112340/1112340 [==============================] - 16s 14us/step - loss: 1.0803 - acc: 0.5029 - val_loss: 0.6863 - val_acc: 0.5569\n",
      "Epoch 2/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.7396 - acc: 0.5207 - val_loss: 0.6681 - val_acc: 0.6053\n",
      "Epoch 3/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.6845 - acc: 0.5581 - val_loss: 0.6264 - val_acc: 0.6215\n",
      "Epoch 4/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.6363 - acc: 0.6054 - val_loss: 0.5714 - val_acc: 0.6248\n",
      "Epoch 5/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5851 - acc: 0.6356 - val_loss: 0.5330 - val_acc: 0.6264\n",
      "Epoch 6/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5436 - acc: 0.6538 - val_loss: 0.5155 - val_acc: 0.6441\n",
      "Epoch 7/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5179 - acc: 0.6649 - val_loss: 0.5079 - val_acc: 0.6706\n",
      "Epoch 8/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5031 - acc: 0.6724 - val_loss: 0.5016 - val_acc: 0.6881\n",
      "Epoch 9/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4948 - acc: 0.6764 - val_loss: 0.4971 - val_acc: 0.6981\n",
      "Epoch 10/40\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4895 - acc: 0.6796 - val_loss: 0.4932 - val_acc: 0.7018\n",
      "Epoch 11/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4857 - acc: 0.6831 - val_loss: 0.4894 - val_acc: 0.7042\n",
      "Epoch 12/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4829 - acc: 0.6839 - val_loss: 0.4862 - val_acc: 0.7053\n",
      "Epoch 13/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4802 - acc: 0.6859 - val_loss: 0.4826 - val_acc: 0.7048\n",
      "Epoch 14/40\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4778 - acc: 0.6883 - val_loss: 0.4799 - val_acc: 0.7062\n",
      "Epoch 15/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4764 - acc: 0.6902 - val_loss: 0.4781 - val_acc: 0.7054\n",
      "Epoch 16/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4751 - acc: 0.6924 - val_loss: 0.4756 - val_acc: 0.7051\n",
      "Epoch 17/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4739 - acc: 0.6935 - val_loss: 0.4746 - val_acc: 0.7042\n",
      "Epoch 18/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4734 - acc: 0.6948 - val_loss: 0.4732 - val_acc: 0.7046\n",
      "Epoch 19/40\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4725 - acc: 0.6964 - val_loss: 0.4722 - val_acc: 0.7046\n",
      "Epoch 20/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4718 - acc: 0.6981 - val_loss: 0.4712 - val_acc: 0.7052\n",
      "Epoch 21/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4714 - acc: 0.6990 - val_loss: 0.4705 - val_acc: 0.7068\n",
      "Epoch 22/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4708 - acc: 0.7000 - val_loss: 0.4699 - val_acc: 0.7076\n",
      "Epoch 23/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4702 - acc: 0.7006 - val_loss: 0.4695 - val_acc: 0.7082\n",
      "Epoch 24/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4696 - acc: 0.7010 - val_loss: 0.4683 - val_acc: 0.7088\n",
      "Epoch 25/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4693 - acc: 0.7019 - val_loss: 0.4681 - val_acc: 0.7094\n",
      "Epoch 26/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4689 - acc: 0.7015 - val_loss: 0.4676 - val_acc: 0.7094\n",
      "Epoch 27/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.7022 - val_loss: 0.4670 - val_acc: 0.7096\n",
      "Epoch 28/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4683 - acc: 0.7026 - val_loss: 0.4665 - val_acc: 0.7096\n",
      "Epoch 29/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4680 - acc: 0.7028 - val_loss: 0.4660 - val_acc: 0.7097\n",
      "Epoch 30/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4679 - acc: 0.7031 - val_loss: 0.4657 - val_acc: 0.7095\n",
      "Epoch 31/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4674 - acc: 0.7038 - val_loss: 0.4651 - val_acc: 0.7097\n",
      "Epoch 32/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4672 - acc: 0.7042 - val_loss: 0.4653 - val_acc: 0.7098\n",
      "Epoch 33/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7042 - val_loss: 0.4647 - val_acc: 0.7094\n",
      "Epoch 34/40\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4667 - acc: 0.7044 - val_loss: 0.4645 - val_acc: 0.7094\n",
      "Epoch 35/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4663 - acc: 0.7050 - val_loss: 0.4641 - val_acc: 0.7097\n",
      "Epoch 36/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4662 - acc: 0.7052 - val_loss: 0.4640 - val_acc: 0.7096\n",
      "Epoch 37/40\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4660 - acc: 0.7054 - val_loss: 0.4638 - val_acc: 0.7094\n",
      "Epoch 38/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7049 - val_loss: 0.4636 - val_acc: 0.7095\n",
      "Epoch 39/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4655 - acc: 0.7052 - val_loss: 0.4636 - val_acc: 0.7094\n",
      "Epoch 40/40\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4652 - acc: 0.7055 - val_loss: 0.4629 - val_acc: 0.7095\n",
      "[[0.5091695 ]\n",
      " [0.5101897 ]\n",
      " [0.5101897 ]\n",
      " ...\n",
      " [0.25052738]\n",
      " [0.5101897 ]\n",
      " [0.6420738 ]]\n",
      "243151 0.7094742063492063\n",
      "trend_test_acc:\n",
      "110675 0.6458625116713352\n",
      "vol_test_acc:\n",
      "132476 0.7730859010270775\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.49773625]\n",
      " [0.5101897 ]\n",
      " [0.56678605]\n",
      " ...\n",
      " [0.5702628 ]\n",
      " [0.04302343]\n",
      " [0.04302343]]\n",
      "135922 0.39659780578898224\n",
      "trend_test_acc:\n",
      "103481 0.6038807189542483\n",
      "vol_test_acc:\n",
      "103317 0.6029236694677871\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.771014239028945\n",
      "loss:\n",
      "0.4651909690238101\n",
      "val_loss:\n",
      "0.46287736884908737\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "13\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2436 MiB, count=155, average=15.7 MiB\n",
      "(1478160, 36, 6)\n",
      "(1478160, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1084260 samples, validate on 389580 samples\n",
      "Epoch 1/40\n",
      "1084260/1084260 [==============================] - 15s 14us/step - loss: 1.0507 - acc: 0.5157 - val_loss: 0.6741 - val_acc: 0.5981\n",
      "Epoch 2/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.7131 - acc: 0.5522 - val_loss: 0.6637 - val_acc: 0.6163\n",
      "Epoch 3/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6728 - acc: 0.5904 - val_loss: 0.6394 - val_acc: 0.6293\n",
      "Epoch 4/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6405 - acc: 0.6236 - val_loss: 0.6032 - val_acc: 0.6302\n",
      "Epoch 5/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6034 - acc: 0.6407 - val_loss: 0.5634 - val_acc: 0.6307\n",
      "Epoch 6/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.5649 - acc: 0.6502 - val_loss: 0.5313 - val_acc: 0.6371\n",
      "Epoch 7/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.5346 - acc: 0.6589 - val_loss: 0.5110 - val_acc: 0.6573\n",
      "Epoch 8/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.5156 - acc: 0.6660 - val_loss: 0.5013 - val_acc: 0.6739\n",
      "Epoch 9/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5040 - acc: 0.6714 - val_loss: 0.4959 - val_acc: 0.6812\n",
      "Epoch 10/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4961 - acc: 0.6754 - val_loss: 0.4912 - val_acc: 0.6886\n",
      "Epoch 11/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4918 - acc: 0.6785 - val_loss: 0.4876 - val_acc: 0.6925\n",
      "Epoch 12/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4883 - acc: 0.6812 - val_loss: 0.4848 - val_acc: 0.6959\n",
      "Epoch 13/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4854 - acc: 0.6834 - val_loss: 0.4819 - val_acc: 0.6992\n",
      "Epoch 14/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4832 - acc: 0.6851 - val_loss: 0.4794 - val_acc: 0.7027\n",
      "Epoch 15/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4809 - acc: 0.6872 - val_loss: 0.4759 - val_acc: 0.7059\n",
      "Epoch 16/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4790 - acc: 0.6892 - val_loss: 0.4736 - val_acc: 0.7067\n",
      "Epoch 17/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4776 - acc: 0.6904 - val_loss: 0.4717 - val_acc: 0.7066\n",
      "Epoch 18/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4757 - acc: 0.6921 - val_loss: 0.4694 - val_acc: 0.7072\n",
      "Epoch 19/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4745 - acc: 0.6933 - val_loss: 0.4678 - val_acc: 0.7075\n",
      "Epoch 20/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4735 - acc: 0.6943 - val_loss: 0.4669 - val_acc: 0.7083\n",
      "Epoch 21/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4724 - acc: 0.6948 - val_loss: 0.4651 - val_acc: 0.7096\n",
      "Epoch 22/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4718 - acc: 0.6952 - val_loss: 0.4640 - val_acc: 0.7102\n",
      "Epoch 23/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4710 - acc: 0.6953 - val_loss: 0.4629 - val_acc: 0.7110\n",
      "Epoch 24/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4706 - acc: 0.6959 - val_loss: 0.4622 - val_acc: 0.7113\n",
      "Epoch 25/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4699 - acc: 0.6968 - val_loss: 0.4615 - val_acc: 0.7111\n",
      "Epoch 26/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4697 - acc: 0.6968 - val_loss: 0.4608 - val_acc: 0.7111\n",
      "Epoch 27/40\n",
      "1084260/1084260 [==============================] - 9s 8us/step - loss: 0.4692 - acc: 0.6970 - val_loss: 0.4605 - val_acc: 0.7111\n",
      "Epoch 28/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4688 - acc: 0.6977 - val_loss: 0.4601 - val_acc: 0.7114\n",
      "Epoch 29/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4685 - acc: 0.6980 - val_loss: 0.4599 - val_acc: 0.7112\n",
      "Epoch 30/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4683 - acc: 0.6980 - val_loss: 0.4595 - val_acc: 0.7110\n",
      "Epoch 31/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4679 - acc: 0.6982 - val_loss: 0.4594 - val_acc: 0.7106\n",
      "Epoch 32/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4680 - acc: 0.6986 - val_loss: 0.4591 - val_acc: 0.7108\n",
      "Epoch 33/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.6980 - val_loss: 0.4590 - val_acc: 0.7112\n",
      "Epoch 34/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4672 - acc: 0.6991 - val_loss: 0.4586 - val_acc: 0.7112\n",
      "Epoch 35/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.6987 - val_loss: 0.4585 - val_acc: 0.7114\n",
      "Epoch 36/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.6994 - val_loss: 0.4583 - val_acc: 0.7112\n",
      "Epoch 37/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.6995 - val_loss: 0.4584 - val_acc: 0.7114\n",
      "Epoch 38/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7000 - val_loss: 0.4581 - val_acc: 0.7112\n",
      "Epoch 39/40\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4659 - acc: 0.7008 - val_loss: 0.4580 - val_acc: 0.7118\n",
      "Epoch 40/40\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7016 - val_loss: 0.4579 - val_acc: 0.7119\n",
      "[[0.5030322 ]\n",
      " [0.500262  ]\n",
      " [0.49573877]\n",
      " ...\n",
      " [0.23982811]\n",
      " [0.46736437]\n",
      " [0.69036436]]\n",
      "277360 0.7119461984701473\n",
      "trend_test_acc:\n",
      "126391 0.6487578277384252\n",
      "vol_test_acc:\n",
      "150969 0.7751540357362908\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.49573877]\n",
      " [0.49573877]\n",
      " [0.57474184]\n",
      " ...\n",
      " [0.3686621 ]\n",
      " [0.46699938]\n",
      " [0.46699938]]\n",
      "136150 0.34947892602289643\n",
      "trend_test_acc:\n",
      "126909 0.6514166923313828\n",
      "vol_test_acc:\n",
      "126521 0.649625179708359\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7711381487756045\n",
      "loss:\n",
      "0.46570016624419003\n",
      "val_loss:\n",
      "0.45786954676219405\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "14\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2495 MiB, count=157, average=15.9 MiB\n",
      "(1513800, 36, 6)\n",
      "(1513800, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1133640 samples, validate on 375840 samples\n",
      "Epoch 1/40\n",
      "1133640/1133640 [==============================] - 17s 15us/step - loss: 1.4686 - acc: 0.5041 - val_loss: 0.6904 - val_acc: 0.5541\n",
      "Epoch 2/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.7893 - acc: 0.5172 - val_loss: 0.6732 - val_acc: 0.5984\n",
      "Epoch 3/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.7109 - acc: 0.5394 - val_loss: 0.6573 - val_acc: 0.6206\n",
      "Epoch 4/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6775 - acc: 0.5759 - val_loss: 0.6310 - val_acc: 0.6355\n",
      "Epoch 5/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6465 - acc: 0.6107 - val_loss: 0.5954 - val_acc: 0.6458\n",
      "Epoch 6/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6093 - acc: 0.6379 - val_loss: 0.5544 - val_acc: 0.6650\n",
      "Epoch 7/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5690 - acc: 0.6587 - val_loss: 0.5242 - val_acc: 0.6815\n",
      "Epoch 8/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5389 - acc: 0.6710 - val_loss: 0.5049 - val_acc: 0.6905\n",
      "Epoch 9/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5183 - acc: 0.6785 - val_loss: 0.4939 - val_acc: 0.6923\n",
      "Epoch 10/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5043 - acc: 0.6835 - val_loss: 0.4862 - val_acc: 0.6934\n",
      "Epoch 11/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4948 - acc: 0.6867 - val_loss: 0.4804 - val_acc: 0.6935\n",
      "Epoch 12/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4890 - acc: 0.6888 - val_loss: 0.4774 - val_acc: 0.6941\n",
      "Epoch 13/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4843 - acc: 0.6913 - val_loss: 0.4734 - val_acc: 0.6950\n",
      "Epoch 14/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4810 - acc: 0.6924 - val_loss: 0.4706 - val_acc: 0.6956\n",
      "Epoch 15/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4789 - acc: 0.6933 - val_loss: 0.4684 - val_acc: 0.6961\n",
      "Epoch 16/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4771 - acc: 0.6949 - val_loss: 0.4668 - val_acc: 0.6969\n",
      "Epoch 17/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4754 - acc: 0.6962 - val_loss: 0.4646 - val_acc: 0.6986\n",
      "Epoch 18/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4738 - acc: 0.6971 - val_loss: 0.4643 - val_acc: 0.7002\n",
      "Epoch 19/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4725 - acc: 0.6982 - val_loss: 0.4633 - val_acc: 0.7028\n",
      "Epoch 20/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4718 - acc: 0.6991 - val_loss: 0.4622 - val_acc: 0.7047\n",
      "Epoch 21/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6996 - val_loss: 0.4612 - val_acc: 0.7054\n",
      "Epoch 22/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4697 - acc: 0.7002 - val_loss: 0.4607 - val_acc: 0.7061\n",
      "Epoch 23/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4693 - acc: 0.7008 - val_loss: 0.4607 - val_acc: 0.7062\n",
      "Epoch 24/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.7014 - val_loss: 0.4601 - val_acc: 0.7068\n",
      "Epoch 25/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.7016 - val_loss: 0.4596 - val_acc: 0.7074\n",
      "Epoch 26/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4676 - acc: 0.7017 - val_loss: 0.4594 - val_acc: 0.7073\n",
      "Epoch 27/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7028 - val_loss: 0.4591 - val_acc: 0.7075\n",
      "Epoch 28/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4666 - acc: 0.7025 - val_loss: 0.4586 - val_acc: 0.7076\n",
      "Epoch 29/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4661 - acc: 0.7032 - val_loss: 0.4586 - val_acc: 0.7082\n",
      "Epoch 30/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7033 - val_loss: 0.4586 - val_acc: 0.7079\n",
      "Epoch 31/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7033 - val_loss: 0.4581 - val_acc: 0.7083\n",
      "Epoch 32/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4651 - acc: 0.7041 - val_loss: 0.4581 - val_acc: 0.7080\n",
      "Epoch 33/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4649 - acc: 0.7040 - val_loss: 0.4580 - val_acc: 0.7084\n",
      "Epoch 34/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4646 - acc: 0.7043 - val_loss: 0.4579 - val_acc: 0.7079\n",
      "Epoch 35/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4641 - acc: 0.7040 - val_loss: 0.4578 - val_acc: 0.7073\n",
      "Epoch 36/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4639 - acc: 0.7046 - val_loss: 0.4577 - val_acc: 0.7078\n",
      "Epoch 37/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4635 - acc: 0.7044 - val_loss: 0.4574 - val_acc: 0.7083\n",
      "Epoch 38/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4633 - acc: 0.7046 - val_loss: 0.4573 - val_acc: 0.7071\n",
      "Epoch 39/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4629 - acc: 0.7044 - val_loss: 0.4573 - val_acc: 0.7071\n",
      "Epoch 40/40\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4630 - acc: 0.7045 - val_loss: 0.4573 - val_acc: 0.7066\n",
      "[[0.50920695]\n",
      " [0.50150144]\n",
      " [0.5025928 ]\n",
      " ...\n",
      " [0.16553667]\n",
      " [0.47548395]\n",
      " [0.83086014]]\n",
      "265574 0.7066145168156662\n",
      "trend_test_acc:\n",
      "121211 0.6450138356747552\n",
      "vol_test_acc:\n",
      "144363 0.7682151979565772\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.51027405]\n",
      " [0.5025928 ]\n",
      " [0.5716365 ]\n",
      " ...\n",
      " [0.44718862]\n",
      " [0.52727944]\n",
      " [0.52727944]]\n",
      "152192 0.4049382716049383\n",
      "trend_test_acc:\n",
      "112112 0.5965942954448702\n",
      "vol_test_acc:\n",
      "111536 0.5935291613452534\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7688244997871434\n",
      "loss:\n",
      "0.46300616088227636\n",
      "val_loss:\n",
      "0.4572964352082577\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "15\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2438 MiB, count=158, average=15.4 MiB\n",
      "(1479660, 36, 6)\n",
      "(1479660, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1116780 samples, validate on 358560 samples\n",
      "Epoch 1/40\n",
      "1116780/1116780 [==============================] - 17s 15us/step - loss: 1.3095 - acc: 0.5131 - val_loss: 0.6976 - val_acc: 0.5412\n",
      "Epoch 2/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.7487 - acc: 0.5365 - val_loss: 0.6744 - val_acc: 0.5991\n",
      "Epoch 3/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6874 - acc: 0.5696 - val_loss: 0.6564 - val_acc: 0.6276\n",
      "Epoch 4/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6650 - acc: 0.6002 - val_loss: 0.6391 - val_acc: 0.6295\n",
      "Epoch 5/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6419 - acc: 0.6240 - val_loss: 0.6105 - val_acc: 0.6278\n",
      "Epoch 6/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6115 - acc: 0.6380 - val_loss: 0.5779 - val_acc: 0.6418\n",
      "Epoch 7/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5801 - acc: 0.6490 - val_loss: 0.5497 - val_acc: 0.6481\n",
      "Epoch 8/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5511 - acc: 0.6588 - val_loss: 0.5281 - val_acc: 0.6558\n",
      "Epoch 9/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5275 - acc: 0.6675 - val_loss: 0.5101 - val_acc: 0.6749\n",
      "Epoch 10/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5106 - acc: 0.6728 - val_loss: 0.4976 - val_acc: 0.6831\n",
      "Epoch 11/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4996 - acc: 0.6764 - val_loss: 0.4900 - val_acc: 0.6892\n",
      "Epoch 12/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4927 - acc: 0.6788 - val_loss: 0.4850 - val_acc: 0.6944\n",
      "Epoch 13/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4881 - acc: 0.6805 - val_loss: 0.4813 - val_acc: 0.6956\n",
      "Epoch 14/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4851 - acc: 0.6815 - val_loss: 0.4787 - val_acc: 0.6962\n",
      "Epoch 15/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4827 - acc: 0.6821 - val_loss: 0.4765 - val_acc: 0.6968\n",
      "Epoch 16/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4813 - acc: 0.6824 - val_loss: 0.4746 - val_acc: 0.6976\n",
      "Epoch 17/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4799 - acc: 0.6834 - val_loss: 0.4742 - val_acc: 0.6979\n",
      "Epoch 18/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4790 - acc: 0.6841 - val_loss: 0.4731 - val_acc: 0.6984\n",
      "Epoch 19/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4785 - acc: 0.6839 - val_loss: 0.4720 - val_acc: 0.6983\n",
      "Epoch 20/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4775 - acc: 0.6852 - val_loss: 0.4711 - val_acc: 0.6989\n",
      "Epoch 21/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4770 - acc: 0.6857 - val_loss: 0.4708 - val_acc: 0.7002\n",
      "Epoch 22/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4764 - acc: 0.6862 - val_loss: 0.4704 - val_acc: 0.7029\n",
      "Epoch 23/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4756 - acc: 0.6863 - val_loss: 0.4698 - val_acc: 0.7064\n",
      "Epoch 24/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4751 - acc: 0.6870 - val_loss: 0.4682 - val_acc: 0.7066\n",
      "Epoch 25/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4744 - acc: 0.6886 - val_loss: 0.4666 - val_acc: 0.7100\n",
      "Epoch 26/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4732 - acc: 0.6907 - val_loss: 0.4661 - val_acc: 0.7099\n",
      "Epoch 27/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4726 - acc: 0.6936 - val_loss: 0.4647 - val_acc: 0.7106\n",
      "Epoch 28/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4720 - acc: 0.6965 - val_loss: 0.4641 - val_acc: 0.7109\n",
      "Epoch 29/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4714 - acc: 0.6976 - val_loss: 0.4637 - val_acc: 0.7108\n",
      "Epoch 30/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4709 - acc: 0.6987 - val_loss: 0.4632 - val_acc: 0.7109\n",
      "Epoch 31/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4706 - acc: 0.6993 - val_loss: 0.4624 - val_acc: 0.7112\n",
      "Epoch 32/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.7000 - val_loss: 0.4625 - val_acc: 0.7113\n",
      "Epoch 33/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4694 - acc: 0.7004 - val_loss: 0.4620 - val_acc: 0.7114\n",
      "Epoch 34/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4694 - acc: 0.7006 - val_loss: 0.4617 - val_acc: 0.7111\n",
      "Epoch 35/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4691 - acc: 0.7011 - val_loss: 0.4615 - val_acc: 0.7110\n",
      "Epoch 36/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.7015 - val_loss: 0.4609 - val_acc: 0.7109\n",
      "Epoch 37/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.7018 - val_loss: 0.4607 - val_acc: 0.7110\n",
      "Epoch 38/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.7015 - val_loss: 0.4606 - val_acc: 0.7109\n",
      "Epoch 39/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4679 - acc: 0.7020 - val_loss: 0.4601 - val_acc: 0.7110\n",
      "Epoch 40/40\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4679 - acc: 0.7020 - val_loss: 0.4603 - val_acc: 0.7106\n",
      "[[0.49983734]\n",
      " [0.49077764]\n",
      " [0.51038516]\n",
      " ...\n",
      " [0.2266979 ]\n",
      " [0.45847005]\n",
      " [0.711262  ]]\n",
      "254801 0.7106230477465417\n",
      "trend_test_acc:\n",
      "116250 0.6484270414993306\n",
      "vol_test_acc:\n",
      "138551 0.7728190539937528\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.49597672]\n",
      " [0.49077764]\n",
      " [0.61569566]\n",
      " ...\n",
      " [0.51284975]\n",
      " [0.3388927 ]\n",
      " [0.3388927 ]]\n",
      "141103 0.393526885319054\n",
      "trend_test_acc:\n",
      "108740 0.6065372601517179\n",
      "vol_test_acc:\n",
      "108717 0.6064089692101741\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7693189424364123\n",
      "loss:\n",
      "0.46786474629202013\n",
      "val_loss:\n",
      "0.4603124835936952\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPW9//HX58xMMglJQEJENAqoqChSRMR9aRULtHW57lvrrYr1tr9qra1621rt797+bO9ta1v3rWrd12oVBVEU6w4IiqACbgQQEA1LyDYz398f58xkshIhkxk47+fjMY8525x8cjTz5nuW79ecc4iIiAB4+S5AREQKh0JBREQyFAoiIpKhUBARkQyFgoiIZCgUREQkQ6EgIiIZCgUREclQKIiISEY03wV8VQMGDHBDhgzJdxkiIluUWbNmfe6cq9rYdltcKAwZMoSZM2fmuwwRkS2KmX3Sne10+khERDIUCiIikqFQEBGRjC3umoKIyKZobm6mpqaGhoaGfJeSU/F4nOrqamKx2CZ9XqEgIqFQU1NDeXk5Q4YMwczyXU5OOOdYvXo1NTU1DB06dJP2odNHIhIKDQ0NVFZWbrWBAGBmVFZWblZrSKEgIqGxNQdC2ub+jqEJhfc/W8f/TnmfL+qa8l2KiEjBCk0ofPT5eq6dvojP1mzdF5lEpDDV1tZy/fXXf+XPTZw4kdra2hxU1LHQhEJ53L8Sv66hOc+ViEgYdRYKyWSyy89NnjyZfv365aqsdkJz91F53P9V1zYk8lyJiITRZZddxuLFixk1ahSxWIyysjIGDRrEnDlzmD9/PscddxxLliyhoaGBCy+8kEmTJgEtXfusX7+eCRMmcMghh/DKK6+www478Pjjj1NSUtKjdYYmFCrUUhCRwFX/fJf5y9b26D733L6CX39nr07XX3311cybN485c+bwwgsv8K1vfYt58+Zlbh29/fbb6d+/P/X19ey3336ccMIJVFZWttrHwoULue+++7jllls4+eSTeeSRRzjzzDN79PcITSikWwrr1FIQkQIwduzYVs8S/OUvf+Gxxx4DYMmSJSxcuLBdKAwdOpRRo0YBsO+++/Lxxx/3eF0hCgW1FETE19W/6HtLnz59MtMvvPAC06ZN49VXX6W0tJQjjjiiw2cNiouLM9ORSIT6+voerys0F5qLoh7xmKdrCiKSF+Xl5axbt67DdWvWrGGbbbahtLSU9957j9dee62Xq2sRmpYC+K0FtRREJB8qKys5+OCDGTFiBCUlJQwcODCzbvz48dx4442MHDmS3XffnQMOOCBvdYYsFKJqKYhI3tx7770dLi8uLubpp5/ucF36usGAAQOYN29eZvkll1zS4/VBiE4fgX8H0tp6tRRERDoTqlAoj0d195GISBdCFQoVuqYgItKlcIVCia4piIh0JVShoLuPRES6Fq5QKI7S0JyiKZHKdykiIgUpXKGQ6epCrQUR6V2b2nU2wDXXXMOGDRt6uKKOhSoUKkrSXV3ouoKI9K4tJRRC9vCaQkFE8iO76+xx48ax7bbb8uCDD9LY2Mjxxx/PVVddRV1dHSeffDI1NTUkk0l+9atfsWLFCpYtW8bXv/51BgwYwPTp03NaZ8hCIT2mgk4fiYTa05fBZ+/07D632xsmXN3p6uyus6dOncrDDz/MG2+8gXOOY445hhkzZrBq1Sq23357nnrqKcDvE6lv37788Y9/ZPr06QwYMKBna+5AuE4fqadUESkAU6dOZerUqeyzzz6MHj2a9957j4ULF7L33nszbdo0Lr30Ul566SX69u3b67WFtKWg00ciodbFv+h7g3OOyy+/nPPPP7/dulmzZjF58mQuv/xyjj76aK644operS2kLQWFgoj0ruyus7/5zW9y++23s379egCWLl3KypUrWbZsGaWlpZx55plccsklzJ49u91ncy1nLQUzux34NrDSOTeig/VnAJcGs+uBC5xzc3NVD0BZuqWgTvFEpJdld509YcIETj/9dA488EAAysrKuPvuu1m0aBE/+9nP8DyPWCzGDTfcAMCkSZOYMGECgwYNyvmFZnPO5WbHZofhf9nf1UkoHAQscM59aWYTgCudc/tvbL9jxoxxM2fO3OS6Rvx6CieP2ZErvrPnJu9DRLY8CxYsYPjw4fkuo1d09Lua2Szn3JiNfTZnLQXn3AwzG9LF+leyZl8DqnNVSza/p1S1FEREOlIo1xTOAToeYaKHVcRjuiVVRKQTeb/7yMy+jh8Kh3SxzSRgEsBOO+20WT9PYyqIhJdzDjPLdxk5tbmXBPLaUjCzkcCtwLHOudWdbeecu9k5N8Y5N6aqqmqzfqZCQSSc4vE4q1ev3uwvzULmnGP16tXE4/FN3kfeWgpmthPwKHCWc+6D3vq55fEYH35e11s/TkQKRHV1NTU1NaxatSrfpeRUPB6nunrTL9Hm8pbU+4AjgAFmVgP8GogBOOduBK4AKoHrg+ZcojtXxjdXRYlaCiJhFIvFGDp0aL7LKHi5vPvotI2sPxc4N1c/vzPpgXbCcG5RROSrKpS7j3pNeTxKc9LR0KyBdkRE2gpdKKhTPBGRzoUuFNQpnohI50IXCumWgh5gExFpL3yhUJIep1ktBRGRtkIXCuW6piAi0qkQhoJaCiIinQldKGSuKWhMBRGRdkIXCqVFESKeqaUgItKB0IWCmVFWrDEVREQ6ErpQAP+6gp5TEBFpL5ShUBH0fyQiIq2FMhTUUhAR6VhIQyGmu49ERDoQylDQmAoiIh0LZyjomoKISIdCGQrl8SjrGhOkUlvvWK0iIpsilKFQEY/hHNQ16RSSiEi2UIaC+j8SEelYSEMh3VOqQkFEJFsoQyE9poIG2hERaS2UoaAxFUREOhbSUNA1BRGRjoQ6FPRUs4hIa6EMhcxAO2opiIi0EspQiMciFEU8nT4SEWkjlKEA6Z5SdfpIRCRbaEOhoiSmloKISBuhDYXyuIbkFBFpK9ShoLuPRERaC20o+N1n6/SRiEi20IaCf/pIoSAiki3EoRDT3UciIm2ENhQq4jE2NCVJJFP5LkVEpGCENhTSXV2sb9QpJBGRtNCHgq4riIi0CHEo+P0frdFtqSIiGaENhfRAO2opiIi0yFkomNntZrbSzOZ1st7M7C9mtsjM3jaz0bmqpSMVGmhHRKSdXLYU7gDGd7F+AjAseE0CbshhLe1kxlRQS0FEJCNnoeCcmwF80cUmxwJ3Od9rQD8zG5SretpSS0FEpL18XlPYAViSNV8TLGvHzCaZ2Uwzm7lq1aoe+eFluvtIRKSdfIaCdbDMdbShc+5m59wY59yYqqqqHvnhsYhHSSyiTvFERLLkMxRqgB2z5quBZb1ZQEWJ+j8SEcmWz1B4AvhucBfSAcAa59zy3iygPB5jXaNaCiIiadFc7djM7gOOAAaYWQ3wayAG4Jy7EZgMTAQWARuAf89VLZ3xx1RQS0FEJC1noeCcO20j6x3ww1z9/O6oiMeo3dCUzxJERApKaJ9oBo2pICLSVshDQWMqiIhkC3UoVMSjeqJZRCRLuEOhJEZTIkVjIpnvUkRECkKoQ0FjKoiItKZQAD3VLCISCHUotHSKp5aCiAiEPBTKFQoiIq2EPBTSYyro9JGICIQ8FCpKNKaCiEi2UIeC7j4SEWkt1KFQVhTFTHcfiYikhToUPM8oK9ZTzSIiaaEOBfBvS9XpIxERX7dCwcwuNLOKYECc28xstpkdneviekN5PKq7j0REAt1tKXzfObcWOBqowh8Q5+qcVdWL/O6zFQoiItD9ULDgfSLwN+fc3KxlWzSdPhIRadHdUJhlZlPxQ2GKmZUDqdyV1Xs00I6ISIvuDsd5DjAK+NA5t8HM+pOHMZVzQQPtiIi06G5L4UDgfedcrZmdCfwSWJO7snpPRYnfUvCHjBYRCbfuhsINwAYz+xrwc+AT4K6cVdWLyuMxkilHfbMG2hER6W4oJJz/T+ljgT875/4MlOeurN7TMqaCriuIiHQ3FNaZ2eXAWcBTZhYBYrkrq/e0jKmg6woiIt0NhVOARvznFT4DdgD+J2dV9aKW7rPVUhAR6VYoBEFwD9DXzL4NNDjntpprCqAxFUREoPvdXJwMvAGcBJwMvG5mJ+aysN5Soe6zRUQyuvucwi+A/ZxzKwHMrAqYBjycq8J6iwbaERFp0d1rCl46EAKrv8JnC5ruPhIRadHdlsIzZjYFuC+YPwWYnJuSeldJLELEM7UUREToZig4535mZicAB+N3hHezc+6xnFbWS8yMCvV/JCICdL+lgHPuEeCRHNaSN+r/SETE12UomNk6oKNOgQxwzrmKnFTVy9RTqoiIr8tQcM5tFV1ZbIw/poJaCiIiW8UdRJtLLQUREZ9CgeCaQr1aCiIiCgVaxlQQEQk7hQJ+S2F9U4JUSgPtiEi45TQUzGy8mb1vZovM7LIO1u9kZtPN7C0ze9vMJuayns5UxKM4B+sa1VoQkXDLWSgEYy5cB0wA9gROM7M922z2S+BB59w+wKnA9bmqpyvlmU7xdF1BRMItly2FscAi59yHzrkm4H78kduyOSD9rENfYFkO6+lUy0A7aimISLh1+4nmTbADsCRrvgbYv802VwJTzez/AH2Ao3JYT6cyYyroDiQRCblcthSsg2Vtr+SeBtzhnKsGJgJ/N7N2NZnZJDObaWYzV61a1eOFlmtMBRERILehUAPsmDVfTfvTQ+cADwI4514F4sCAtjtyzt3snBvjnBtTVVXV44VmxlRoVEtBRMItl6HwJjDMzIaaWRH+heQn2mzzKXAkgJkNxw+Fnm8KbITGVBAR8eUsFJxzCeBHwBRgAf5dRu+a2W/M7Jhgs58C55nZXPyxGs52zvX6wwK6+0hExJfLC8045ybTZjAe59wVWdPz8cdoyKviaITiqKdrCiISenqiOeCPqaBQEJFwUygEKuJRDbQjIqGnUAiUl8R0+khEQk+hEPDHaVZLQUTCTaEQKI9H9USziISeQiFQXqzTRyIiCoVA39IYtfXNbGhSMIhIeCkUAt/cazuaEilufemjfJciIpI3CoXAvoO3Yfxe23HTi4tZta4x3+WIiOSFQiHLz8fvTkMixV+eW5jvUkRE8kKhkGXnqjJOH7sT977xKYtXrc93OSIivU6h0MaPjxxGPOrx+2fey3cpIiK9TqHQRlV5MecfvgtT3l3BzI+/yHc5IiK9SqHQgXMPHUpVeTG/nbyAPPTkLSKSNwqFDpQWRbl43G7M/rSWKe9+lu9yRER6jUKhEyftW82u25bxu2fepzmZync5IiK9QqHQiWjE47Lxe/DR53Xc/8an+S5HRKRXhCcUnIMV736ljxw5fFvGDu3PNdMWsr5R3V+IyNYvPKEw9z644WCYdiUkmrr1ETPjPycOZ3VdEze/uDi39YmIFIDwhMLw78A+Z8K//gS3fANWzO/Wx0bt2I9vjxzELS99xIq1DTkuUkQkv8ITCsXlcOy1cOp9sP4zuPlweOWvkNr4ReSffXN3EqkUv/rHPOp0GklEtmLhCYW0PSbCBa/CruNg6i/hzu9AbdcXkgdX9uGnR+/OswtWMP7PM3jtw9W9VKyISO8KXygAlFXBqffAsdfB8rn+tYY59/oXozvxg8N34YFJB+KZcerNr3HlE+9q7AUR2eqEMxQAzPxrDBe8DANHwD8ugPvPgNWdX1AeO7Q/T194KGcfNIQ7XvmYCX9+iTc+UlcYIrL1CG8opG0zGM5+Esb9Bj6cDteNhSd/Aus6fpK5tCjKlcfsxf2TDsA5OOXmV/nNP+dT35Ts5cJFRHqebWl9+4wZM8bNnDkzNztftwJm/B5m3QGRIjjgP+DgH0O8b4eb1zUm+N0z73HXq58wdEAf/vekkew7uH9uahMR2QxmNss5N2aj2ykUOrB6MTz/X/Duo1DSHw79Kex3LsTiHW7+yuLP+fnDb7Ostp7zD9+Fi44aRnE0ktsaRUS+AoVCT1j2Fky7yj+t1HdHOOIyGHkqRKLtNl3fmOC/npzP/W8uYY/tyvnTKaMYPqiid+oUEdmI7oaCril0Zft94Lv/gO8+Dn0GwOM/hOsPgHmPtHu+oaw4ytUnjOS2743h8/VNHHPtv7jhhcUkU1tW6IpIuCkUumPnI+C86XDK3eBF4eHvw02HwnuT293GeuTwgUz9yWEcNXwgv3vmPU656VU+WV2Xl7JFRL4qhUJ3mfldZVzwMvzbrdC8Ae4/DW49EhY/3yoc+vcp4vozRnPNKaN4f8U6Jvz5Je55/RMN2CMiBU+h8FV5ERh5EvzwDTjmr/4dS38/Hu74Nnz2TmYzM+O4fXZgykWHMXqnbfjFY/O45KG3NTaDiBQ0hcKmisRg9Hfhx7Nhwv/A5+/DbUfDe0+12mz7fiXc9f2xXHTUMB6ZXcOku2bqSWgRKVgKhc0VLYb9J8EPXoaqPfynol+5ttXpJM8zLjpqN/77+BG8+MEqzrj1db6s61733SIivUmh0FPKB8LZT/nXHab+wn8qOtncapMz9h/M9WeM5t2laznxxldYWlufp2JFRDqmUOhJRaVw0p1wyE9g1t/g3pOhYU2rTcaPGMRd54xl5dpGTrzhFT5YsS5PxYqItKdQ6GmeB0ddCcdcCx/N8K8zfPlJq00O2LmSB84/kETKcdKNrzLrE3WqJyKFQaGQK6PPgrMeg3XL/dtWl7zZavWe21fw6AUHsU1pjNNveZ1p81fkqVARkRY5DQUzG29m75vZIjO7rJNtTjaz+Wb2rpndm8t6et3Qw+CcaVBUBnd8q92dSTv2L+XhCw5it4HlnH/3LB6ZVZOnQkVEfDkLBTOLANcBE4A9gdPMbM822wwDLgcOds7tBVyUq3rypmo3OPc52G4EPPhdePexVqsHlBVz36QD2H9of3760FzuePmjPBUqIpLblsJYYJFz7kPnXBNwP3Bsm23OA65zzn0J4JxbmcN68qdPJZz1D9hhjN9FxtsPtlpdVhzl9rP3Y9yeA7nyn/P563ML9fSziORFLkNhB2BJ1nxNsCzbbsBuZvaymb1mZuM72pGZTTKzmWY2c9WqVTkqN8fiFXDmIzD4YHh0Erx1d+vVsQjXnzGa4/fZgT88+wG/nbxAwSAiva59H9A9xzpY1vZbLgoMA44AqoGXzGyEc6621Yecuxm4Gfyus3u+1F5SXAanPwgPnOH3uJpsgjHfz6yORTz+cNLXKI9HueWlj1hbn+C3/7Y3Ea+jQyki0vNyGQo1wI5Z89XAsg62ec051wx8ZGbv44fEm2ytikrh1Pvgoe/5D7glmuCAH2RWe55x1TF7URGPce30RaxvTPCnU0ZRFNWNYiKSe7n8pnkTGGZmQ82sCDgVeKLNNv8Avg5gZgPwTyd9mMOaCkMsDif/3X/6+ZlL4eU/t1ptZlzyzd35z4l78NQ7yznvrpkaA1pEekXOQsE5lwB+BEwBFgAPOufeNbPfmNkxwWZTgNVmNh+YDvzMObc6VzUVlGgRnPg3GHECPHsFvPj7dmMzTDpsF/7fv+3NjIWr+O7tr1O7Qf0liUhuaTjOfEsl/esLc++DMefAhN/5PbBmefLtZVz8wFyq+5fwt7P3Y3BlnzwVKyJbKg3HuaXwInDs9XDwRTDzNrjnRKj/stUm3x65PXefuz9f1DVx/PWvMPvTLzvZmYjI5lEoFALPg3FX+eHw8ctw6zhYvbjVJmOH9ufRCw6iPB7ltJtf4+l3luepWBHZmikUCsk+Z8D3noANq+GWb8BHL7VavXNVGY9ecBB7bV/Bf9w7m5tnLNazDCLSoxQKhWbwQXDe81A2EP5+HMy6s9XqyrJi7j3vACaOGMRvJ7/Hrx6fR0JDfIpID1EoFKL+Q+HcZ2Ho4fDPH8OUX/gXpAPxWIS/nrYPPzh8F+5+7VPOu2smdY0a4lNENp9CoVDF+/pPP489H169Fu49BdZ9llntecZlE/bgt8fvzYyFn3P89S8zf9naPBYsIlsDhUIhi0Rh4u/hW3/0B+y5bizM/nur5xlO338n7vz3sXy5oZnjrnuZW2Z8SCql6wwismkUCluC/c6BC16BbfeCJ37kX2v4oqWL7UOGDWDKRYdxxO5V/PfkBZxx6+ss0/jPIrIJFApbigG7wtlP+a2Gmllww0Hw6nWZaw39+xRx01n78vsTRjK3ppbx18zgibltu5oSEemaQmFL4nl+q+GHr8GQQ2HKf8Jt42DFfMDvM+nk/Xbk6QsPZZdty/jxfW9x4f1vsaa+Oc+Fi8iWQqGwJepbDac/ACfcBl9+DDcdBtOuggb/QvPgyj48dP6BXDxuN558ezkTrpnBcwtW6JkGEdko9X20patbDVMuh7cfgJL+cNglfh9KsTgAb336JT99cC4ffl7H13bsx8XjduOwYQMw0xgNImHS3b6PFApbi6Wz4bnfwIfToe+OcMTl8LVTwYvQlEjxyOwarn1+EUtr6xm9Uz8uHrc7B+9aqXAQCQmFQlh9+AJMuxKWvQVVw+HIK2D3CWBGUyLFgzOXcN30RSxf08B+Q7bhJ+N246BdBuS7ahHJMYVCmDkH8x+H5/8vrF4EOx4Ah14MuxwJkSiNiSQPvOmHw4q1jew/tD9nHjCYb+yxLX2KczkYn4jki0JBIJmAOXfDC1fDuuV+f0ojT4FRp8O2w2loTnLfG59y44uLWbG2keKox+G7VTFh7+04cvhAKuKxjf8MEdkiKBSkRaIJFk6FOffCwimQSsD2o/1wGHECyfg2zPrkSya/s5xn5n3GZ2sbKIp4HDJsABNGbMe4PQfSr7Qo37+FiGwGhYJ0bP0qeOchmHsvfPYORIpgt/EwbBxUjyVVOYy3atbyzLzlTH7nM5bW1uMZDNu2nL2r+zKyui9779CX4YMqiMci+f5tRKSbFAqyccvf9ocBfechqFvlL4v3her9YMf9cdVjedd25dnFG3i7ppa3a9awus4fJzrqGbtvV87I6r7suX1fhlSWMqSyD4P6xolG9PiLSKFRKEj3OeeP9Lbkdah5A5a8ASsXAA7M8+9iqtodV7krtaWDWdA0kDfXbcPMz5K8s3QNtRtanpiOekb1NiUMruzD4MpSdupfSvU2JQwoK/Zf5cX0KYroVliRXqZQkM3TsAZqZkLNm7B0Fnz+AdR+Ci5rQJ+ygbjKXakvH8zqSBXLU/35uLkfH9SX887aUuZ/YaxrTLbbdTzmUdnHD4iqsiL69ymiX2kRfUti9CuN0a+kiH6lMfqW+K+y4iglRRGKo57CRGQTKRSk5yUa/d5ZVy+Ezxf6rYvVC/1ldSvbbe5ifUiVD6K+uIr6aF/WeRXUUs7nqTJWJctY2lRKTWMpSxuLWVpfzMrmYlJd9LziGZQW+QFRWhShJOa/Zy8rLYoG7xFKgm2Koh5FES/zHgum/Xcj4nlEzIh4RjRieGZEPX8++7PFUU+nxmSL1d1Q0E3p0n3RYth2D//VVqIJ1n8Ga5fB2qWwdjm2dhmRtUspW7+SsvqPqdqw2h9/2nUwfGjEf6WKK0jGKmiKVdAQLafeK2ODV0a9V0qdlbKOUta5Uta6EmqTJaxJFVNXBxtqjTUJx7JmqGt21DVDEo9mF6GRWPAq6jJ0usMzKI4GQRP1iHlGJGJEPc8PlSBMop7heUbM84hGjGjE3zZ7OhbxQ6Yo4k/H0kEVaVmX3l8s4odXR/PR4Of77y3TxVGPkqII8WiEeMxvaXmeWlrSNYWC9IxoEfTbyX91JZWCxjWw4Qs/IOo+h4ZaqK+Fhlq8+lq8hjXEGmrpU18LDUthw1poXAdN67pXiwcUd7zKeVFSkWJSXjGpSDHJYDoZKSbpFZPwikl6RSQi/nSCIposSrOL0kSMJqI0uihNLkKDi5JwHkk8Es5IOiPhjITz5xMJo9kFr5RHIgXNKaPJGU0poz5lNKQ8GpIejSmPxqTHhpSRcBGaidJIjA3Eae7BP9PiqEc8FiEe8wMoFkkHi0csHSqZ6Y5CJ3uZH0x+QLXMt6wL3rPCLf3uWfrd7903YobngWctLTUv2DbiWaYll95HOjCLoq3rj0X8Vt/GzjLqNGTnFArSuzwPSrbxX5W7fLXPppJ+ODSug8a1fq+wTev95y5SCX+9S/rv6elkMySboLkeEo1YooFIopFIosE/HZbwl6fX+/O1UF8P6W2SzZBs9KfJ0elWj077LHZezD8VFyslFbwno31IRktJpF8R/9UcKaEpUkqzF6fRxWhMmf9KetSnPBqTRkMyCCRXRB1xNrgiNqSKqaOIxlSU5mSKhuYUiVSSRDJFIulIpFIkUq5lOulIupb5ZMrRnNxyTkWb+QGUDhA/jMgEUTSrVZbdAot4LYGTeccy84YfOOmwy35PB57nGRGjdTgGwbexhtw3hg/kmK9tn7sDg0JBtiReBEr6+a98SSb8gEg2+afMXNI/HZYOIeeC6VRLQLmk30JqNZ8OrQSkmv3gSYdbstlflmiEpjoseHlNdX4IZt5Xwvq6YD5Ytrmh5UUhVgrROERi/jGPxCAW9del5y3SMm9eZtqZR8qiOPNwFgmmI6TMI0WElPkvZ1FSXhEpL0bSi+KsiKQX9ectiks5Ug5SLoVzjlQqRcqBS6VIuuDUIBG/VeX86WbnkXAezXiA4dLvZjgMZ/58Co+UM5L4+0k5I4Xfwks6SDiPJiCR8oJWHTQ5j+aUR2PK31/KeTjwT0eaZbqlTzn/v4BzjpRz/v8Ozv9dkkF4Jh2kUo5kyt/GX+ZIpdxG/+sNH1Sxef99u0GhIPJVRKL+iz75rqQ95/wWT1Odf6otmchqRTUHLah08DRBcwM0b/C3b66H5jpo2hBMb2i9fYevdLg1ZbY1lySS3VLLbJdov79kk/++VTA/HM38aWhpSmTPZwWo/x7z3yPBvJn/39GlgODdpYKkSYH3feCnOf1NFAoiWwszKCr1X1Tlu5ruSaWCllJTy6m+ZFOwMvvL1Wh1oSA7mDKtrKzgc46WL9XsL9fs5amsVl6b+XbhlxWuznXyxZ1eBpkWW+buzuA9/bMyLcLs/Qf7ToeLef6LrOn+X/GU6yZQKIhI/ngeeMX+nW1SEHTTtYiIZCgUREQkQ6EgIiIZCgUREclQKIiISIZCQUREMhQKIiKSoVAQEZGMLW48BTNbBXyyiR9FtbCjAAAF+ElEQVQfAHzeg+X0JNW2aQq5Nijs+lTbptlSaxvsnNvoo+5bXChsDjOb2Z1BJvJBtW2aQq4NCrs+1bZptvbadPpIREQyFAoiIpIRtlC4Od8FdEG1bZpCrg0Kuz7Vtmm26tpCdU1BRES6FraWgoiIdCE0oWBm483sfTNbZGaX5buebGb2sZm9Y2ZzzGxmnmu53cxWmtm8rGX9zexZM1sYvG9TQLVdaWZLg2M3x8wm5qm2Hc1supktMLN3zezCYHnej10XteX92JlZ3MzeMLO5QW1XBcuHmtnrwXF7wMyKCqi2O8zso6zjNqq3a8uqMWJmb5nZk8H85h8359xW/wIiwGJgZ6AImAvsme+6sur7GBiQ7zqCWg4DRgPzspb9HrgsmL4M+F0B1XYlcEkBHLdBwOhguhz4ANizEI5dF7Xl/djhD69WFkzHgNeBA4AHgVOD5TcCFxRQbXcAJ+b7/7mgrouBe4Eng/nNPm5haSmMBRY55z50zjUB9wPH5rmmguScmwF80WbxscCdwfSdwHG9WlSgk9oKgnNuuXNudjC9DlgA7EABHLsuass751sfzMaClwO+ATwcLM/XceustoJgZtXAt4Bbg3mjB45bWEJhB2BJ1nwNBfJHEXDAVDObZWaT8l1MBwY655aD/wUDbJvnetr6kZm9HZxeysuprWxmNgTYB/9flgV17NrUBgVw7IJTIHOAlcCz+K36WudcItgkb3+vbWtzzqWP238Hx+1PZpavsUSvAX4OpAeGrqQHjltYQsE6WFYwiQ8c7JwbDUwAfmhmh+W7oC3IDcAuwChgOfCHfBZjZmXAI8BFzrm1+aylrQ5qK4hj55xLOudGAdX4rfrhHW3Wu1UFP7RNbWY2Argc2APYD+gPXNrbdZnZt4GVzrlZ2Ys72PQrH7ewhEINsGPWfDWwLE+1tOOcWxa8rwQew//DKCQrzGwQQPC+Ms/1ZDjnVgR/uCngFvJ47Mwshv+le49z7tFgcUEcu45qK6RjF9RTC7yAf96+n5lFg1V5/3vNqm18cDrOOecagb+Rn+N2MHCMmX2Mfzr8G/gth80+bmEJhTeBYcGV+SLgVOCJPNcEgJn1MbPy9DRwNDCv60/1uieA7wXT3wMez2MtraS/cAPHk6djF5zPvQ1Y4Jz7Y9aqvB+7zmorhGNnZlVm1i+YLgGOwr/mMR04MdgsX8eto9reywp5wz9n3+vHzTl3uXOu2jk3BP/77Hnn3Bn0xHHL99Xz3noBE/HvulgM/CLf9WTVtTP+3VBzgXfzXRtwH/6phGb8FtY5+OcqnwMWBu/9C6i2vwPvAG/jfwEPylNth+A31d8G5gSviYVw7LqoLe/HDhgJvBXUMA+4Ili+M/AGsAh4CCguoNqeD47bPOBugjuU8vUCjqDl7qPNPm56ollERDLCcvpIRES6QaEgIiIZCgUREclQKIiISIZCQUREMhQKIr3IzI5I92gpUogUCiIikqFQEOmAmZ0Z9KU/x8xuCjpGW29mfzCz2Wb2nJlVBduOMrPXgg7SHkt3LGdmu5rZtKA//tlmtkuw+zIze9jM3jOze4InY0UKgkJBpA0zGw6cgt9R4SggCZwB9AFmO7/zwheBXwcfuQu41Dk3Ev9J1/Tye4DrnHNfAw7Cfxob/F5KL8If02Bn/H5sRApCdOObiITOkcC+wJvBP+JL8DuySwEPBNvcDTxqZn2Bfs65F4PldwIPBf1Z7eCcewzAOdcAEOzvDedcTTA/BxgC/Cv3v5bIxikURNoz4E7n3OWtFpr9qs12XfUR09Upocas6ST6O5QCotNHIu09B5xoZttCZpzlwfh/L+keKE8H/uWcWwN8aWaHBsvPAl50/ngFNWZ2XLCPYjMr7dXfQmQT6F8oIm045+ab2S/xR8Pz8Htl/SFQB+xlZrOANfjXHcDvovjG4Ev/Q+Dfg+VnATeZ2W+CfZzUi7+GyCZRL6ki3WRm651zZfmuQySXdPpIREQy1FIQEZEMtRRERCRDoSAiIhkKBRERyVAoiIhIhkJBREQyFAoiIpLx/wHfQbUwyS+6vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "16\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2521 MiB, count=158, average=16.0 MiB\n",
      "(1530060, 36, 6)\n",
      "(1530060, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1132620 samples, validate on 393120 samples\n",
      "Epoch 1/40\n",
      "1132620/1132620 [==============================] - 17s 15us/step - loss: 0.9700 - acc: 0.5122 - val_loss: 0.6745 - val_acc: 0.6157\n",
      "Epoch 2/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.7238 - acc: 0.5420 - val_loss: 0.6558 - val_acc: 0.6327\n",
      "Epoch 3/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.6740 - acc: 0.5862 - val_loss: 0.6237 - val_acc: 0.6368\n",
      "Epoch 4/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.6318 - acc: 0.6255 - val_loss: 0.5771 - val_acc: 0.6469\n",
      "Epoch 5/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5864 - acc: 0.6498 - val_loss: 0.5346 - val_acc: 0.6504\n",
      "Epoch 6/40\n",
      "1132620/1132620 [==============================] - 11s 9us/step - loss: 0.5488 - acc: 0.6612 - val_loss: 0.5099 - val_acc: 0.6559\n",
      "Epoch 7/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5231 - acc: 0.6691 - val_loss: 0.4984 - val_acc: 0.6698\n",
      "Epoch 8/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5066 - acc: 0.6750 - val_loss: 0.4925 - val_acc: 0.6784\n",
      "Epoch 9/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4965 - acc: 0.6792 - val_loss: 0.4872 - val_acc: 0.6845\n",
      "Epoch 10/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4897 - acc: 0.6826 - val_loss: 0.4825 - val_acc: 0.6908\n",
      "Epoch 11/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4849 - acc: 0.6862 - val_loss: 0.4787 - val_acc: 0.6980\n",
      "Epoch 12/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4816 - acc: 0.6885 - val_loss: 0.4756 - val_acc: 0.7017\n",
      "Epoch 13/40\n",
      "1132620/1132620 [==============================] - 11s 9us/step - loss: 0.4788 - acc: 0.6910 - val_loss: 0.4737 - val_acc: 0.7016\n",
      "Epoch 14/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4767 - acc: 0.6929 - val_loss: 0.4713 - val_acc: 0.7026\n",
      "Epoch 15/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4748 - acc: 0.6946 - val_loss: 0.4694 - val_acc: 0.7018\n",
      "Epoch 16/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4733 - acc: 0.6960 - val_loss: 0.4680 - val_acc: 0.7025\n",
      "Epoch 17/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4717 - acc: 0.6974 - val_loss: 0.4667 - val_acc: 0.7024\n",
      "Epoch 18/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4709 - acc: 0.6989 - val_loss: 0.4652 - val_acc: 0.7039\n",
      "Epoch 19/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4697 - acc: 0.7003 - val_loss: 0.4643 - val_acc: 0.7043\n",
      "Epoch 20/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4689 - acc: 0.7007 - val_loss: 0.4637 - val_acc: 0.7045\n",
      "Epoch 21/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7018 - val_loss: 0.4627 - val_acc: 0.7044\n",
      "Epoch 22/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.7028 - val_loss: 0.4623 - val_acc: 0.7045\n",
      "Epoch 23/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7035 - val_loss: 0.4615 - val_acc: 0.7047\n",
      "Epoch 24/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4663 - acc: 0.7038 - val_loss: 0.4611 - val_acc: 0.7048\n",
      "Epoch 25/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7044 - val_loss: 0.4606 - val_acc: 0.7047\n",
      "Epoch 26/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4652 - acc: 0.7056 - val_loss: 0.4599 - val_acc: 0.7048\n",
      "Epoch 27/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4650 - acc: 0.7053 - val_loss: 0.4596 - val_acc: 0.7050\n",
      "Epoch 28/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4643 - acc: 0.7060 - val_loss: 0.4593 - val_acc: 0.7051\n",
      "Epoch 29/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4641 - acc: 0.7058 - val_loss: 0.4590 - val_acc: 0.7053\n",
      "Epoch 30/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4640 - acc: 0.7057 - val_loss: 0.4588 - val_acc: 0.7056\n",
      "Epoch 31/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4635 - acc: 0.7067 - val_loss: 0.4586 - val_acc: 0.7030\n",
      "Epoch 32/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4628 - acc: 0.7066 - val_loss: 0.4581 - val_acc: 0.7055\n",
      "Epoch 33/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4628 - acc: 0.7070 - val_loss: 0.4581 - val_acc: 0.7044\n",
      "Epoch 34/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4627 - acc: 0.7076 - val_loss: 0.4578 - val_acc: 0.7047\n",
      "Epoch 35/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4622 - acc: 0.7080 - val_loss: 0.4577 - val_acc: 0.7052\n",
      "Epoch 36/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4620 - acc: 0.7075 - val_loss: 0.4575 - val_acc: 0.7054\n",
      "Epoch 37/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4617 - acc: 0.7075 - val_loss: 0.4571 - val_acc: 0.7040\n",
      "Epoch 38/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4615 - acc: 0.7081 - val_loss: 0.4571 - val_acc: 0.7054\n",
      "Epoch 39/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4616 - acc: 0.7082 - val_loss: 0.4570 - val_acc: 0.7051\n",
      "Epoch 40/40\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4611 - acc: 0.7080 - val_loss: 0.4570 - val_acc: 0.7055\n",
      "[[0.46668547]\n",
      " [0.49360427]\n",
      " [0.49627233]\n",
      " ...\n",
      " [0.261766  ]\n",
      " [0.45753166]\n",
      " [0.68476343]]\n",
      "277358 0.705530118030118\n",
      "trend_test_acc:\n",
      "126507 0.6436050061050061\n",
      "vol_test_acc:\n",
      "150851 0.76745522995523\n",
      "Reverse section--------------------------------------------------------------------------\n",
      "[[0.46710765]\n",
      " [0.5234715 ]\n",
      " [0.57543236]\n",
      " ...\n",
      " [0.6614441 ]\n",
      " [0.02662381]\n",
      " [0.02662381]]\n",
      "174369 0.4435515873015873\n",
      "trend_test_acc:\n",
      "109043 0.5547568172568172\n",
      "vol_test_acc:\n",
      "109708 0.5581400081400082\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.768215811965812\n",
      "loss:\n",
      "0.4611157851014275\n",
      "val_loss:\n",
      "0.4569624080277576\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for z in range(head,tail,1):\n",
    "    \"\"\"\n",
    "     V\n",
    "    \"\"\"\n",
    "    n=daynum[tail]-daynum[head]\n",
    "    df = pd.read_csv('data/JPY_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    print(df.shape)  \n",
    "    jpy5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        jpy5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/EUR_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eur5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eur5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/AUD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    aud5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        aud5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/btc_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    btc5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        btc5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/eth_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eth5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eth5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/DASH_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    dash5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        dash5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    print('finish dataread')\n",
    "    Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "    for p in range(n-l+1):\n",
    "        Train_data[p,0,:]=btc5months[p:p+l]\n",
    "        Train_data[p,1,:]=dash5months[p:p+l]\n",
    "        Train_data[p,2,:]=eth5months[p:p+l]\n",
    "        Train_data[p,3,:]=jpy5months[p:p+l]\n",
    "        Train_data[p,4,:]=eur5months[p:p+l]\n",
    "        Train_data[p,5,:]=aud5months[p:p+l]\n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((currencynum,l,1))\n",
    "    visual_conv = ConvolutionNetworks([20,10],[(1,kn),(1,kn)])(visual_scene)\n",
    "    print(K.int_shape(visual_conv))\n",
    "    tag = build_tag(visual_conv)\n",
    "    visual_conv = Concatenate()([tag, visual_conv])\n",
    "    print(K.int_shape(visual_conv))\n",
    "    \n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    f = shapes[2]\n",
    "    features= []\n",
    "    #features = np.zeros(0)\n",
    "    for k1 in range(w):\n",
    "        for k2 in range(f):\n",
    "            def get_feature(t):\n",
    "                return t[:, k1, k2, :]\n",
    "            #get_feature_layer = Lambda(get_feature)\n",
    "            features.append(Lambda(get_feature)(visual_conv))\n",
    "    \n",
    "      \n",
    "    input2 = Input((16,))\n",
    "    onehot_encode_question = input2\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode_question]))    \n",
    "    \n",
    "     \n",
    "    g_MLP = get_MLP(4, get_dense(4,MLP_unit))\n",
    "    \n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "    \n",
    "    combined_relation = Add()(mid_relations)\n",
    "    \n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "    \n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "    \n",
    "    \n",
    "    #model = Model(inputs=[visual_scene])\n",
    "    model = Model(inputs=[visual_scene, input2, tag], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "    #model.summary()\n",
    "    print(z)\n",
    "    fit_show(Train_data,daynum[z],daynum[z+3],daynum[z+4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 6, 36, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 6, 32, 20)    120         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 6, 8, 20)     0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 6, 8, 20)     80          max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 6, 4, 10)     1010        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 6, 1, 10)     0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 6, 1, 7)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 6, 1, 10)     40          max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 6, 1, 17)     0           input_14[0][0]                   \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 17)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 50)           0           lambda_25[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64)           3264        concatenate_10[0][0]             \n",
      "                                                                 concatenate_10[1][0]             \n",
      "                                                                 concatenate_10[2][0]             \n",
      "                                                                 concatenate_10[3][0]             \n",
      "                                                                 concatenate_10[4][0]             \n",
      "                                                                 concatenate_10[5][0]             \n",
      "                                                                 concatenate_10[6][0]             \n",
      "                                                                 concatenate_10[7][0]             \n",
      "                                                                 concatenate_10[8][0]             \n",
      "                                                                 concatenate_10[9][0]             \n",
      "                                                                 concatenate_10[10][0]            \n",
      "                                                                 concatenate_10[11][0]            \n",
      "                                                                 concatenate_10[12][0]            \n",
      "                                                                 concatenate_10[13][0]            \n",
      "                                                                 concatenate_10[14][0]            \n",
      "                                                                 concatenate_10[15][0]            \n",
      "                                                                 concatenate_10[16][0]            \n",
      "                                                                 concatenate_10[17][0]            \n",
      "                                                                 concatenate_10[18][0]            \n",
      "                                                                 concatenate_10[19][0]            \n",
      "                                                                 concatenate_10[20][0]            \n",
      "                                                                 concatenate_10[21][0]            \n",
      "                                                                 concatenate_10[22][0]            \n",
      "                                                                 concatenate_10[23][0]            \n",
      "                                                                 concatenate_10[24][0]            \n",
      "                                                                 concatenate_10[25][0]            \n",
      "                                                                 concatenate_10[26][0]            \n",
      "                                                                 concatenate_10[27][0]            \n",
      "                                                                 concatenate_10[28][0]            \n",
      "                                                                 concatenate_10[29][0]            \n",
      "                                                                 concatenate_10[30][0]            \n",
      "                                                                 concatenate_10[31][0]            \n",
      "                                                                 concatenate_10[32][0]            \n",
      "                                                                 concatenate_10[33][0]            \n",
      "                                                                 concatenate_10[34][0]            \n",
      "                                                                 concatenate_10[35][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 64)           4160        dense_29[0][0]                   \n",
      "                                                                 dense_29[1][0]                   \n",
      "                                                                 dense_29[2][0]                   \n",
      "                                                                 dense_29[3][0]                   \n",
      "                                                                 dense_29[4][0]                   \n",
      "                                                                 dense_29[5][0]                   \n",
      "                                                                 dense_29[6][0]                   \n",
      "                                                                 dense_29[7][0]                   \n",
      "                                                                 dense_29[8][0]                   \n",
      "                                                                 dense_29[9][0]                   \n",
      "                                                                 dense_29[10][0]                  \n",
      "                                                                 dense_29[11][0]                  \n",
      "                                                                 dense_29[12][0]                  \n",
      "                                                                 dense_29[13][0]                  \n",
      "                                                                 dense_29[14][0]                  \n",
      "                                                                 dense_29[15][0]                  \n",
      "                                                                 dense_29[16][0]                  \n",
      "                                                                 dense_29[17][0]                  \n",
      "                                                                 dense_29[18][0]                  \n",
      "                                                                 dense_29[19][0]                  \n",
      "                                                                 dense_29[20][0]                  \n",
      "                                                                 dense_29[21][0]                  \n",
      "                                                                 dense_29[22][0]                  \n",
      "                                                                 dense_29[23][0]                  \n",
      "                                                                 dense_29[24][0]                  \n",
      "                                                                 dense_29[25][0]                  \n",
      "                                                                 dense_29[26][0]                  \n",
      "                                                                 dense_29[27][0]                  \n",
      "                                                                 dense_29[28][0]                  \n",
      "                                                                 dense_29[29][0]                  \n",
      "                                                                 dense_29[30][0]                  \n",
      "                                                                 dense_29[31][0]                  \n",
      "                                                                 dense_29[32][0]                  \n",
      "                                                                 dense_29[33][0]                  \n",
      "                                                                 dense_29[34][0]                  \n",
      "                                                                 dense_29[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 64)           4160        dense_30[0][0]                   \n",
      "                                                                 dense_30[1][0]                   \n",
      "                                                                 dense_30[2][0]                   \n",
      "                                                                 dense_30[3][0]                   \n",
      "                                                                 dense_30[4][0]                   \n",
      "                                                                 dense_30[5][0]                   \n",
      "                                                                 dense_30[6][0]                   \n",
      "                                                                 dense_30[7][0]                   \n",
      "                                                                 dense_30[8][0]                   \n",
      "                                                                 dense_30[9][0]                   \n",
      "                                                                 dense_30[10][0]                  \n",
      "                                                                 dense_30[11][0]                  \n",
      "                                                                 dense_30[12][0]                  \n",
      "                                                                 dense_30[13][0]                  \n",
      "                                                                 dense_30[14][0]                  \n",
      "                                                                 dense_30[15][0]                  \n",
      "                                                                 dense_30[16][0]                  \n",
      "                                                                 dense_30[17][0]                  \n",
      "                                                                 dense_30[18][0]                  \n",
      "                                                                 dense_30[19][0]                  \n",
      "                                                                 dense_30[20][0]                  \n",
      "                                                                 dense_30[21][0]                  \n",
      "                                                                 dense_30[22][0]                  \n",
      "                                                                 dense_30[23][0]                  \n",
      "                                                                 dense_30[24][0]                  \n",
      "                                                                 dense_30[25][0]                  \n",
      "                                                                 dense_30[26][0]                  \n",
      "                                                                 dense_30[27][0]                  \n",
      "                                                                 dense_30[28][0]                  \n",
      "                                                                 dense_30[29][0]                  \n",
      "                                                                 dense_30[30][0]                  \n",
      "                                                                 dense_30[31][0]                  \n",
      "                                                                 dense_30[32][0]                  \n",
      "                                                                 dense_30[33][0]                  \n",
      "                                                                 dense_30[34][0]                  \n",
      "                                                                 dense_30[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 64)           4160        dense_31[0][0]                   \n",
      "                                                                 dense_31[1][0]                   \n",
      "                                                                 dense_31[2][0]                   \n",
      "                                                                 dense_31[3][0]                   \n",
      "                                                                 dense_31[4][0]                   \n",
      "                                                                 dense_31[5][0]                   \n",
      "                                                                 dense_31[6][0]                   \n",
      "                                                                 dense_31[7][0]                   \n",
      "                                                                 dense_31[8][0]                   \n",
      "                                                                 dense_31[9][0]                   \n",
      "                                                                 dense_31[10][0]                  \n",
      "                                                                 dense_31[11][0]                  \n",
      "                                                                 dense_31[12][0]                  \n",
      "                                                                 dense_31[13][0]                  \n",
      "                                                                 dense_31[14][0]                  \n",
      "                                                                 dense_31[15][0]                  \n",
      "                                                                 dense_31[16][0]                  \n",
      "                                                                 dense_31[17][0]                  \n",
      "                                                                 dense_31[18][0]                  \n",
      "                                                                 dense_31[19][0]                  \n",
      "                                                                 dense_31[20][0]                  \n",
      "                                                                 dense_31[21][0]                  \n",
      "                                                                 dense_31[22][0]                  \n",
      "                                                                 dense_31[23][0]                  \n",
      "                                                                 dense_31[24][0]                  \n",
      "                                                                 dense_31[25][0]                  \n",
      "                                                                 dense_31[26][0]                  \n",
      "                                                                 dense_31[27][0]                  \n",
      "                                                                 dense_31[28][0]                  \n",
      "                                                                 dense_31[29][0]                  \n",
      "                                                                 dense_31[30][0]                  \n",
      "                                                                 dense_31[31][0]                  \n",
      "                                                                 dense_31[32][0]                  \n",
      "                                                                 dense_31[33][0]                  \n",
      "                                                                 dense_31[34][0]                  \n",
      "                                                                 dense_31[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64)           0           dense_32[0][0]                   \n",
      "                                                                 dense_32[1][0]                   \n",
      "                                                                 dense_32[2][0]                   \n",
      "                                                                 dense_32[3][0]                   \n",
      "                                                                 dense_32[4][0]                   \n",
      "                                                                 dense_32[5][0]                   \n",
      "                                                                 dense_32[6][0]                   \n",
      "                                                                 dense_32[7][0]                   \n",
      "                                                                 dense_32[8][0]                   \n",
      "                                                                 dense_32[9][0]                   \n",
      "                                                                 dense_32[10][0]                  \n",
      "                                                                 dense_32[11][0]                  \n",
      "                                                                 dense_32[12][0]                  \n",
      "                                                                 dense_32[13][0]                  \n",
      "                                                                 dense_32[14][0]                  \n",
      "                                                                 dense_32[15][0]                  \n",
      "                                                                 dense_32[16][0]                  \n",
      "                                                                 dense_32[17][0]                  \n",
      "                                                                 dense_32[18][0]                  \n",
      "                                                                 dense_32[19][0]                  \n",
      "                                                                 dense_32[20][0]                  \n",
      "                                                                 dense_32[21][0]                  \n",
      "                                                                 dense_32[22][0]                  \n",
      "                                                                 dense_32[23][0]                  \n",
      "                                                                 dense_32[24][0]                  \n",
      "                                                                 dense_32[25][0]                  \n",
      "                                                                 dense_32[26][0]                  \n",
      "                                                                 dense_32[27][0]                  \n",
      "                                                                 dense_32[28][0]                  \n",
      "                                                                 dense_32[29][0]                  \n",
      "                                                                 dense_32[30][0]                  \n",
      "                                                                 dense_32[31][0]                  \n",
      "                                                                 dense_32[32][0]                  \n",
      "                                                                 dense_32[33][0]                  \n",
      "                                                                 dense_32[34][0]                  \n",
      "                                                                 dense_32[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 64)           4160        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64)           0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64)           0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 64)           4160        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64)           0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 64)           0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            65          activation_10[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 25,379\n",
      "Trainable params: 25,319\n",
      "Non-trainable params: 60\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "此處開始寫rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(0,m3-m1-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(v.shape)\n",
    "    v = v.reshape(len(v),currencynum,l,1)\n",
    "    print(v.shape)\n",
    "    print(\"[Training model......]\")\n",
    "    \n",
    "    #Train_v=v[:]\n",
    "    \n",
    "    Train_v=v[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:((m2-m1)-l*2)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[((m2-m1))*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    history = model.fit([Train_v, Train_q], Train_a,validation_data=([Test_v,Test_q],Test_a),batch_size=batch_size ,epochs = epochs,shuffle=False)\n",
    "    pred = model.predict([Test_v, Test_q])\n",
    "    count = 0\n",
    "    print(pred)\n",
    "    \n",
    "    \"\"\"\n",
    "    benchmark1\n",
    "    \"\"\"\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]):\n",
    "        benchacc=benchacc+Test_a[i]\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"猜答案多的那邊 benchacc1:\")\n",
    "    print(benchacc)\n",
    "\n",
    "    \"\"\"\n",
    "    benchmark2\n",
    "    \"\"\"\n",
    "    #第一個直接猜1\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]-1):\n",
    "        if(Test_a[i]!=Test_a[i+1]):\n",
    "            benchacc=benchacc+1\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"參考前一個答案 benchacc2:\")\n",
    "    print(benchacc)    \n",
    "\n",
    "        \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)\n",
    "    plt.show()    \n",
    "def ConvolutionNetworks(fn,ks):\n",
    "    def conv(model):\n",
    "        for i in range(len(fn)):\n",
    "            model = (Conv2D(filters=fn[i],kernel_size=ks[i],padding='valid',activation='relu'))(model)  \n",
    "            model = (MaxPooling2D(pool_size=(1,pl)))(model)\n",
    "            model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_tag(conv):\n",
    "    d = K.int_shape(conv)[1]\n",
    "    d2 = K.int_shape(conv)[2]\n",
    "    tag = np.zeros((d,d2,d+1))\n",
    "    for i in range(d):\n",
    "        for j in range(d2):\n",
    "            tag[i,j,d] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[i,j,i] = 1\n",
    "    tag = K.variable(tag)\n",
    "    tag = K.expand_dims(tag,axis=0)\n",
    "    batch_size = K.shape(conv)[0]\n",
    "    tag = K.tile(tag,[batch_size,1,1,1])\n",
    "    print(K.int_shape(tag))\n",
    "    return Input(tensor=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "9\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2429 MiB, count=2, average=1215 MiB\n",
      "(1474200, 36, 6)\n",
      "(1474200, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1099080 samples, validate on 370800 samples\n",
      "Epoch 1/70\n",
      "1099080/1099080 [==============================] - 16s 15us/step - loss: 1.6733 - acc: 0.5075 - val_loss: 0.7117 - val_acc: 0.5471\n",
      "Epoch 2/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.7992 - acc: 0.5257 - val_loss: 0.6681 - val_acc: 0.6111\n",
      "Epoch 3/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.7129 - acc: 0.5453 - val_loss: 0.6588 - val_acc: 0.6343\n",
      "Epoch 4/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.6860 - acc: 0.5683 - val_loss: 0.6439 - val_acc: 0.6511\n",
      "Epoch 5/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.6673 - acc: 0.5916 - val_loss: 0.6231 - val_acc: 0.6627\n",
      "Epoch 6/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.6488 - acc: 0.6082 - val_loss: 0.5979 - val_acc: 0.6693\n",
      "Epoch 7/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.6252 - acc: 0.6236 - val_loss: 0.5669 - val_acc: 0.6735\n",
      "Epoch 8/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.5993 - acc: 0.6351 - val_loss: 0.5373 - val_acc: 0.6766\n",
      "Epoch 9/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.5738 - acc: 0.6470 - val_loss: 0.5130 - val_acc: 0.6770\n",
      "Epoch 10/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.5520 - acc: 0.6558 - val_loss: 0.4966 - val_acc: 0.6798\n",
      "Epoch 11/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.5348 - acc: 0.6655 - val_loss: 0.4869 - val_acc: 0.6837\n",
      "Epoch 12/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.5226 - acc: 0.6731 - val_loss: 0.4818 - val_acc: 0.6868\n",
      "Epoch 13/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.5143 - acc: 0.6785 - val_loss: 0.4795 - val_acc: 0.6905\n",
      "Epoch 14/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.5083 - acc: 0.6822 - val_loss: 0.4777 - val_acc: 0.6947\n",
      "Epoch 15/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.5034 - acc: 0.6849 - val_loss: 0.4771 - val_acc: 0.6962\n",
      "Epoch 16/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4995 - acc: 0.6868 - val_loss: 0.4764 - val_acc: 0.6979\n",
      "Epoch 17/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4962 - acc: 0.6888 - val_loss: 0.4760 - val_acc: 0.6997\n",
      "Epoch 18/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4937 - acc: 0.6904 - val_loss: 0.4755 - val_acc: 0.7009\n",
      "Epoch 19/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4917 - acc: 0.6908 - val_loss: 0.4755 - val_acc: 0.7027\n",
      "Epoch 20/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4900 - acc: 0.6917 - val_loss: 0.4756 - val_acc: 0.7040\n",
      "Epoch 21/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4884 - acc: 0.6924 - val_loss: 0.4757 - val_acc: 0.7043\n",
      "Epoch 22/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4872 - acc: 0.6934 - val_loss: 0.4760 - val_acc: 0.7050\n",
      "Epoch 23/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4863 - acc: 0.6939 - val_loss: 0.4760 - val_acc: 0.7053\n",
      "Epoch 24/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4852 - acc: 0.6940 - val_loss: 0.4761 - val_acc: 0.7054\n",
      "Epoch 25/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4840 - acc: 0.6946 - val_loss: 0.4765 - val_acc: 0.7065\n",
      "Epoch 26/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4832 - acc: 0.6952 - val_loss: 0.4768 - val_acc: 0.7069\n",
      "Epoch 27/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4827 - acc: 0.6957 - val_loss: 0.4765 - val_acc: 0.7069\n",
      "Epoch 28/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4820 - acc: 0.6960 - val_loss: 0.4767 - val_acc: 0.7076\n",
      "Epoch 29/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4815 - acc: 0.6959 - val_loss: 0.4763 - val_acc: 0.7074\n",
      "Epoch 30/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4812 - acc: 0.6964 - val_loss: 0.4767 - val_acc: 0.7080\n",
      "Epoch 31/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4802 - acc: 0.6971 - val_loss: 0.4769 - val_acc: 0.7085\n",
      "Epoch 32/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4793 - acc: 0.6971 - val_loss: 0.4765 - val_acc: 0.7088\n",
      "Epoch 33/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4785 - acc: 0.6976 - val_loss: 0.4763 - val_acc: 0.7091\n",
      "Epoch 34/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4774 - acc: 0.6974 - val_loss: 0.4765 - val_acc: 0.7090\n",
      "Epoch 35/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4763 - acc: 0.6978 - val_loss: 0.4764 - val_acc: 0.7092\n",
      "Epoch 36/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4758 - acc: 0.6978 - val_loss: 0.4772 - val_acc: 0.7096\n",
      "Epoch 37/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4751 - acc: 0.6982 - val_loss: 0.4776 - val_acc: 0.7091\n",
      "Epoch 38/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4740 - acc: 0.6983 - val_loss: 0.4785 - val_acc: 0.7094\n",
      "Epoch 39/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.6984 - val_loss: 0.4787 - val_acc: 0.7096\n",
      "Epoch 40/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4725 - acc: 0.6989 - val_loss: 0.4791 - val_acc: 0.7095\n",
      "Epoch 41/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4721 - acc: 0.6988 - val_loss: 0.4798 - val_acc: 0.7098\n",
      "Epoch 42/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4717 - acc: 0.6989 - val_loss: 0.4801 - val_acc: 0.7098\n",
      "Epoch 43/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4714 - acc: 0.6987 - val_loss: 0.4804 - val_acc: 0.7098\n",
      "Epoch 44/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4709 - acc: 0.6992 - val_loss: 0.4806 - val_acc: 0.7098\n",
      "Epoch 45/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6992 - val_loss: 0.4808 - val_acc: 0.7101\n",
      "Epoch 46/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4706 - acc: 0.6991 - val_loss: 0.4806 - val_acc: 0.7099\n",
      "Epoch 47/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4703 - acc: 0.6994 - val_loss: 0.4810 - val_acc: 0.7102\n",
      "Epoch 48/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4697 - acc: 0.6997 - val_loss: 0.4812 - val_acc: 0.7098\n",
      "Epoch 49/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4695 - acc: 0.6997 - val_loss: 0.4812 - val_acc: 0.7100\n",
      "Epoch 50/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4696 - acc: 0.6996 - val_loss: 0.4811 - val_acc: 0.7101\n",
      "Epoch 51/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4693 - acc: 0.7000 - val_loss: 0.4813 - val_acc: 0.7104\n",
      "Epoch 52/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4692 - acc: 0.6998 - val_loss: 0.4813 - val_acc: 0.7101\n",
      "Epoch 53/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4687 - acc: 0.6999 - val_loss: 0.4812 - val_acc: 0.7104\n",
      "Epoch 54/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4688 - acc: 0.7000 - val_loss: 0.4815 - val_acc: 0.7102\n",
      "Epoch 55/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4686 - acc: 0.7001 - val_loss: 0.4815 - val_acc: 0.7100\n",
      "Epoch 56/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4686 - acc: 0.7000 - val_loss: 0.4816 - val_acc: 0.7103\n",
      "Epoch 57/70\n",
      "1099080/1099080 [==============================] - 9s 8us/step - loss: 0.4685 - acc: 0.7004 - val_loss: 0.4814 - val_acc: 0.7100\n",
      "Epoch 58/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7005 - val_loss: 0.4812 - val_acc: 0.7103\n",
      "Epoch 59/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4680 - acc: 0.7004 - val_loss: 0.4815 - val_acc: 0.7103\n",
      "Epoch 60/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.7003 - val_loss: 0.4816 - val_acc: 0.7103\n",
      "Epoch 61/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4677 - acc: 0.7005 - val_loss: 0.4818 - val_acc: 0.7103\n",
      "Epoch 62/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.7009 - val_loss: 0.4819 - val_acc: 0.7107\n",
      "Epoch 63/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7004 - val_loss: 0.4818 - val_acc: 0.7105\n",
      "Epoch 64/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.7001 - val_loss: 0.4814 - val_acc: 0.7110\n",
      "Epoch 65/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7007 - val_loss: 0.4815 - val_acc: 0.7105\n",
      "Epoch 66/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4672 - acc: 0.7005 - val_loss: 0.4817 - val_acc: 0.7107\n",
      "Epoch 67/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7012 - val_loss: 0.4820 - val_acc: 0.7106\n",
      "Epoch 68/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4668 - acc: 0.7008 - val_loss: 0.4817 - val_acc: 0.7109\n",
      "Epoch 69/70\n",
      "1099080/1099080 [==============================] - 9s 9us/step - loss: 0.4668 - acc: 0.7004 - val_loss: 0.4818 - val_acc: 0.7111\n",
      "Epoch 70/70\n",
      "1099080/1099080 [==============================] - 10s 9us/step - loss: 0.4668 - acc: 0.7003 - val_loss: 0.4819 - val_acc: 0.7114\n",
      "[[0.49308202]\n",
      " [0.49308202]\n",
      " [0.49308202]\n",
      " ...\n",
      " [0.28152594]\n",
      " [0.47456583]\n",
      " [0.5362905 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7847626752966559\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "10\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2415 MiB, count=155, average=15.6 MiB\n",
      "(1465440, 36, 6)\n",
      "(1465440, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1120920 samples, validate on 340200 samples\n",
      "Epoch 1/70\n",
      "1120920/1120920 [==============================] - 16s 15us/step - loss: 1.6443 - acc: 0.5052 - val_loss: 0.6960 - val_acc: 0.5527\n",
      "Epoch 2/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.8050 - acc: 0.5152 - val_loss: 0.6812 - val_acc: 0.5678\n",
      "Epoch 3/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.7206 - acc: 0.5319 - val_loss: 0.6658 - val_acc: 0.6279\n",
      "Epoch 4/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.6906 - acc: 0.5590 - val_loss: 0.6495 - val_acc: 0.6566\n",
      "Epoch 5/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.6711 - acc: 0.5854 - val_loss: 0.6266 - val_acc: 0.6668\n",
      "Epoch 6/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.6500 - acc: 0.6089 - val_loss: 0.5937 - val_acc: 0.6691\n",
      "Epoch 7/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.6208 - acc: 0.6270 - val_loss: 0.5559 - val_acc: 0.6726\n",
      "Epoch 8/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5878 - acc: 0.6396 - val_loss: 0.5203 - val_acc: 0.6772\n",
      "Epoch 9/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5574 - acc: 0.6489 - val_loss: 0.4982 - val_acc: 0.6827\n",
      "Epoch 10/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5369 - acc: 0.6574 - val_loss: 0.4870 - val_acc: 0.6915\n",
      "Epoch 11/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5235 - acc: 0.6656 - val_loss: 0.4805 - val_acc: 0.6943\n",
      "Epoch 12/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5145 - acc: 0.6728 - val_loss: 0.4760 - val_acc: 0.6973\n",
      "Epoch 13/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5074 - acc: 0.6780 - val_loss: 0.4724 - val_acc: 0.6994\n",
      "Epoch 14/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.5010 - acc: 0.6821 - val_loss: 0.4699 - val_acc: 0.7005\n",
      "Epoch 15/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4954 - acc: 0.6848 - val_loss: 0.4684 - val_acc: 0.6983\n",
      "Epoch 16/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4908 - acc: 0.6872 - val_loss: 0.4676 - val_acc: 0.7013\n",
      "Epoch 17/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4875 - acc: 0.6888 - val_loss: 0.4678 - val_acc: 0.7013\n",
      "Epoch 18/70\n",
      "1120920/1120920 [==============================] - 9s 8us/step - loss: 0.4845 - acc: 0.6894 - val_loss: 0.4683 - val_acc: 0.7029\n",
      "Epoch 19/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4821 - acc: 0.6910 - val_loss: 0.4691 - val_acc: 0.7030\n",
      "Epoch 20/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4801 - acc: 0.6912 - val_loss: 0.4702 - val_acc: 0.7027\n",
      "Epoch 21/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4785 - acc: 0.6922 - val_loss: 0.4712 - val_acc: 0.7010\n",
      "Epoch 22/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4772 - acc: 0.6928 - val_loss: 0.4722 - val_acc: 0.7034\n",
      "Epoch 23/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4760 - acc: 0.6938 - val_loss: 0.4733 - val_acc: 0.6996\n",
      "Epoch 24/70\n",
      "1120920/1120920 [==============================] - 9s 8us/step - loss: 0.4747 - acc: 0.6938 - val_loss: 0.4748 - val_acc: 0.7007\n",
      "Epoch 25/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4737 - acc: 0.6940 - val_loss: 0.4757 - val_acc: 0.6993\n",
      "Epoch 26/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4731 - acc: 0.6953 - val_loss: 0.4770 - val_acc: 0.6990\n",
      "Epoch 27/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4720 - acc: 0.6965 - val_loss: 0.4780 - val_acc: 0.7053\n",
      "Epoch 28/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4715 - acc: 0.6964 - val_loss: 0.4783 - val_acc: 0.7055\n",
      "Epoch 29/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6968 - val_loss: 0.4792 - val_acc: 0.7059\n",
      "Epoch 30/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4702 - acc: 0.6979 - val_loss: 0.4797 - val_acc: 0.7084\n",
      "Epoch 31/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4700 - acc: 0.6983 - val_loss: 0.4801 - val_acc: 0.7058\n",
      "Epoch 32/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4693 - acc: 0.6982 - val_loss: 0.4806 - val_acc: 0.7061\n",
      "Epoch 33/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.6989 - val_loss: 0.4809 - val_acc: 0.7061\n",
      "Epoch 34/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.6992 - val_loss: 0.4805 - val_acc: 0.7060\n",
      "Epoch 35/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.7002 - val_loss: 0.4812 - val_acc: 0.7057\n",
      "Epoch 36/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4677 - acc: 0.7006 - val_loss: 0.4810 - val_acc: 0.7058\n",
      "Epoch 37/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7005 - val_loss: 0.4810 - val_acc: 0.7059\n",
      "Epoch 38/70\n",
      "1120920/1120920 [==============================] - 9s 8us/step - loss: 0.4672 - acc: 0.7010 - val_loss: 0.4817 - val_acc: 0.7057\n",
      "Epoch 39/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7009 - val_loss: 0.4816 - val_acc: 0.7056\n",
      "Epoch 40/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4668 - acc: 0.7011 - val_loss: 0.4818 - val_acc: 0.7056\n",
      "Epoch 41/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4666 - acc: 0.7017 - val_loss: 0.4815 - val_acc: 0.7058\n",
      "Epoch 42/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4663 - acc: 0.7017 - val_loss: 0.4818 - val_acc: 0.7057\n",
      "Epoch 43/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4661 - acc: 0.7017 - val_loss: 0.4819 - val_acc: 0.7057\n",
      "Epoch 44/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7014 - val_loss: 0.4819 - val_acc: 0.7055\n",
      "Epoch 45/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7018 - val_loss: 0.4820 - val_acc: 0.7055\n",
      "Epoch 46/70\n",
      "1120920/1120920 [==============================] - 9s 8us/step - loss: 0.4657 - acc: 0.7013 - val_loss: 0.4818 - val_acc: 0.7058\n",
      "Epoch 47/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4656 - acc: 0.7020 - val_loss: 0.4820 - val_acc: 0.7057\n",
      "Epoch 48/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4652 - acc: 0.7021 - val_loss: 0.4819 - val_acc: 0.7057\n",
      "Epoch 49/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4649 - acc: 0.7019 - val_loss: 0.4821 - val_acc: 0.7057\n",
      "Epoch 50/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4649 - acc: 0.7022 - val_loss: 0.4822 - val_acc: 0.7057\n",
      "Epoch 51/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4649 - acc: 0.7023 - val_loss: 0.4820 - val_acc: 0.7058\n",
      "Epoch 52/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4649 - acc: 0.7026 - val_loss: 0.4818 - val_acc: 0.7056\n",
      "Epoch 53/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4648 - acc: 0.7022 - val_loss: 0.4816 - val_acc: 0.7061\n",
      "Epoch 54/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4647 - acc: 0.7018 - val_loss: 0.4818 - val_acc: 0.7058\n",
      "Epoch 55/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4645 - acc: 0.7024 - val_loss: 0.4820 - val_acc: 0.7105\n",
      "Epoch 56/70\n",
      "1120920/1120920 [==============================] - 10s 8us/step - loss: 0.4644 - acc: 0.7026 - val_loss: 0.4817 - val_acc: 0.7110\n",
      "Epoch 57/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4642 - acc: 0.7027 - val_loss: 0.4820 - val_acc: 0.7107\n",
      "Epoch 58/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4642 - acc: 0.7025 - val_loss: 0.4818 - val_acc: 0.7108\n",
      "Epoch 59/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4643 - acc: 0.7023 - val_loss: 0.4820 - val_acc: 0.7108\n",
      "Epoch 60/70\n",
      "1120920/1120920 [==============================] - 9s 8us/step - loss: 0.4641 - acc: 0.7030 - val_loss: 0.4817 - val_acc: 0.7110\n",
      "Epoch 61/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4641 - acc: 0.7031 - val_loss: 0.4815 - val_acc: 0.7109\n",
      "Epoch 62/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4637 - acc: 0.7033 - val_loss: 0.4820 - val_acc: 0.7109\n",
      "Epoch 63/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4638 - acc: 0.7034 - val_loss: 0.4819 - val_acc: 0.7109\n",
      "Epoch 64/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4637 - acc: 0.7036 - val_loss: 0.4816 - val_acc: 0.7109\n",
      "Epoch 65/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4637 - acc: 0.7038 - val_loss: 0.4820 - val_acc: 0.7108\n",
      "Epoch 66/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4637 - acc: 0.7037 - val_loss: 0.4820 - val_acc: 0.7110\n",
      "Epoch 67/70\n",
      "1120920/1120920 [==============================] - 9s 8us/step - loss: 0.4636 - acc: 0.7034 - val_loss: 0.4823 - val_acc: 0.7109\n",
      "Epoch 68/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4634 - acc: 0.7036 - val_loss: 0.4819 - val_acc: 0.7109\n",
      "Epoch 69/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4634 - acc: 0.7040 - val_loss: 0.4819 - val_acc: 0.7109\n",
      "Epoch 70/70\n",
      "1120920/1120920 [==============================] - 10s 9us/step - loss: 0.4634 - acc: 0.7034 - val_loss: 0.4821 - val_acc: 0.7109\n",
      "[[0.49358505]\n",
      " [0.49358505]\n",
      " [0.49358505]\n",
      " ...\n",
      " [0.36023286]\n",
      " [0.45866546]\n",
      " [0.59026486]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7851969429747208\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "11\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2459 MiB, count=157, average=15.7 MiB\n",
      "(1492260, 36, 6)\n",
      "(1492260, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1095240 samples, validate on 392700 samples\n",
      "Epoch 1/70\n",
      "1095240/1095240 [==============================] - 16s 15us/step - loss: 1.0179 - acc: 0.5070 - val_loss: 0.6930 - val_acc: 0.5361\n",
      "Epoch 2/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.7308 - acc: 0.5204 - val_loss: 0.6790 - val_acc: 0.6038\n",
      "Epoch 3/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.6891 - acc: 0.5525 - val_loss: 0.6514 - val_acc: 0.6432\n",
      "Epoch 4/70\n",
      "1095240/1095240 [==============================] - 9s 9us/step - loss: 0.6616 - acc: 0.5942 - val_loss: 0.6129 - val_acc: 0.6580\n",
      "Epoch 5/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.6315 - acc: 0.6220 - val_loss: 0.5687 - val_acc: 0.6668\n",
      "Epoch 6/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.5954 - acc: 0.6419 - val_loss: 0.5298 - val_acc: 0.6750\n",
      "Epoch 7/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.5628 - acc: 0.6546 - val_loss: 0.5042 - val_acc: 0.6839\n",
      "Epoch 8/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.5387 - acc: 0.6637 - val_loss: 0.4887 - val_acc: 0.6943\n",
      "Epoch 9/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.5234 - acc: 0.6696 - val_loss: 0.4804 - val_acc: 0.7003\n",
      "Epoch 10/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.5132 - acc: 0.6741 - val_loss: 0.4747 - val_acc: 0.7039\n",
      "Epoch 11/70\n",
      "1095240/1095240 [==============================] - 9s 9us/step - loss: 0.5055 - acc: 0.6791 - val_loss: 0.4712 - val_acc: 0.7072\n",
      "Epoch 12/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4990 - acc: 0.6831 - val_loss: 0.4696 - val_acc: 0.7075\n",
      "Epoch 13/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4933 - acc: 0.6884 - val_loss: 0.4689 - val_acc: 0.7077\n",
      "Epoch 14/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4893 - acc: 0.6912 - val_loss: 0.4691 - val_acc: 0.7076\n",
      "Epoch 15/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4852 - acc: 0.6937 - val_loss: 0.4698 - val_acc: 0.7064\n",
      "Epoch 16/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4823 - acc: 0.6947 - val_loss: 0.4708 - val_acc: 0.7074\n",
      "Epoch 17/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4803 - acc: 0.6969 - val_loss: 0.4719 - val_acc: 0.7072\n",
      "Epoch 18/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4785 - acc: 0.6976 - val_loss: 0.4725 - val_acc: 0.7079\n",
      "Epoch 19/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4771 - acc: 0.6986 - val_loss: 0.4736 - val_acc: 0.7092\n",
      "Epoch 20/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4761 - acc: 0.6990 - val_loss: 0.4744 - val_acc: 0.7100\n",
      "Epoch 21/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4747 - acc: 0.6997 - val_loss: 0.4753 - val_acc: 0.7105\n",
      "Epoch 22/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4740 - acc: 0.6996 - val_loss: 0.4758 - val_acc: 0.7108\n",
      "Epoch 23/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4731 - acc: 0.7001 - val_loss: 0.4760 - val_acc: 0.7107\n",
      "Epoch 24/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4724 - acc: 0.7008 - val_loss: 0.4767 - val_acc: 0.7110\n",
      "Epoch 25/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4719 - acc: 0.7009 - val_loss: 0.4770 - val_acc: 0.7107\n",
      "Epoch 26/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4715 - acc: 0.7013 - val_loss: 0.4771 - val_acc: 0.7113\n",
      "Epoch 27/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.7013 - val_loss: 0.4774 - val_acc: 0.7113\n",
      "Epoch 28/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4706 - acc: 0.7023 - val_loss: 0.4777 - val_acc: 0.7116\n",
      "Epoch 29/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4702 - acc: 0.7024 - val_loss: 0.4781 - val_acc: 0.7117\n",
      "Epoch 30/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4697 - acc: 0.7018 - val_loss: 0.4782 - val_acc: 0.7114\n",
      "Epoch 31/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.7026 - val_loss: 0.4782 - val_acc: 0.7112\n",
      "Epoch 32/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.7030 - val_loss: 0.4784 - val_acc: 0.7112\n",
      "Epoch 33/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4690 - acc: 0.7029 - val_loss: 0.4785 - val_acc: 0.7112\n",
      "Epoch 34/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4683 - acc: 0.7031 - val_loss: 0.4791 - val_acc: 0.7114\n",
      "Epoch 35/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.7032 - val_loss: 0.4794 - val_acc: 0.7114\n",
      "Epoch 36/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7036 - val_loss: 0.4792 - val_acc: 0.7113\n",
      "Epoch 37/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7034 - val_loss: 0.4790 - val_acc: 0.7112\n",
      "Epoch 38/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7038 - val_loss: 0.4790 - val_acc: 0.7113\n",
      "Epoch 39/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4674 - acc: 0.7035 - val_loss: 0.4792 - val_acc: 0.7110\n",
      "Epoch 40/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7036 - val_loss: 0.4795 - val_acc: 0.7111\n",
      "Epoch 41/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7037 - val_loss: 0.4796 - val_acc: 0.7109\n",
      "Epoch 42/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4666 - acc: 0.7036 - val_loss: 0.4798 - val_acc: 0.7107\n",
      "Epoch 43/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7046 - val_loss: 0.4801 - val_acc: 0.7106\n",
      "Epoch 44/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7041 - val_loss: 0.4800 - val_acc: 0.7106\n",
      "Epoch 45/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4665 - acc: 0.7043 - val_loss: 0.4799 - val_acc: 0.7105\n",
      "Epoch 46/70\n",
      "1095240/1095240 [==============================] - 9s 9us/step - loss: 0.4664 - acc: 0.7046 - val_loss: 0.4803 - val_acc: 0.7107\n",
      "Epoch 47/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7045 - val_loss: 0.4807 - val_acc: 0.7106\n",
      "Epoch 48/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4658 - acc: 0.7047 - val_loss: 0.4804 - val_acc: 0.7106\n",
      "Epoch 49/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4660 - acc: 0.7052 - val_loss: 0.4806 - val_acc: 0.7107\n",
      "Epoch 50/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4656 - acc: 0.7049 - val_loss: 0.4807 - val_acc: 0.7106\n",
      "Epoch 51/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4656 - acc: 0.7045 - val_loss: 0.4808 - val_acc: 0.7103\n",
      "Epoch 52/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4655 - acc: 0.7050 - val_loss: 0.4808 - val_acc: 0.7104\n",
      "Epoch 53/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4652 - acc: 0.7055 - val_loss: 0.4814 - val_acc: 0.7103\n",
      "Epoch 54/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4653 - acc: 0.7050 - val_loss: 0.4822 - val_acc: 0.7101\n",
      "Epoch 55/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4651 - acc: 0.7056 - val_loss: 0.4822 - val_acc: 0.7099\n",
      "Epoch 56/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4648 - acc: 0.7056 - val_loss: 0.4830 - val_acc: 0.7100\n",
      "Epoch 57/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4646 - acc: 0.7060 - val_loss: 0.4833 - val_acc: 0.7103\n",
      "Epoch 58/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4645 - acc: 0.7062 - val_loss: 0.4833 - val_acc: 0.7100\n",
      "Epoch 59/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4645 - acc: 0.7061 - val_loss: 0.4836 - val_acc: 0.7100\n",
      "Epoch 60/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4642 - acc: 0.7056 - val_loss: 0.4838 - val_acc: 0.7100\n",
      "Epoch 61/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4644 - acc: 0.7060 - val_loss: 0.4840 - val_acc: 0.7102\n",
      "Epoch 62/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4642 - acc: 0.7060 - val_loss: 0.4843 - val_acc: 0.7100\n",
      "Epoch 63/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4640 - acc: 0.7062 - val_loss: 0.4843 - val_acc: 0.7101\n",
      "Epoch 64/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4640 - acc: 0.7058 - val_loss: 0.4844 - val_acc: 0.7100\n",
      "Epoch 65/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4640 - acc: 0.7064 - val_loss: 0.4847 - val_acc: 0.7103\n",
      "Epoch 66/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4639 - acc: 0.7061 - val_loss: 0.4844 - val_acc: 0.7103\n",
      "Epoch 67/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4636 - acc: 0.7064 - val_loss: 0.4847 - val_acc: 0.7104\n",
      "Epoch 68/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4638 - acc: 0.7064 - val_loss: 0.4851 - val_acc: 0.7103\n",
      "Epoch 69/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4635 - acc: 0.7063 - val_loss: 0.4849 - val_acc: 0.7102\n",
      "Epoch 70/70\n",
      "1095240/1095240 [==============================] - 10s 9us/step - loss: 0.4634 - acc: 0.7066 - val_loss: 0.4852 - val_acc: 0.7102\n",
      "[[0.502304  ]\n",
      " [0.502304  ]\n",
      " [0.502304  ]\n",
      " ...\n",
      " [0.37464476]\n",
      " [0.46578056]\n",
      " [0.58528405]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.784489432136491\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "12\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2405 MiB, count=158, average=15.2 MiB\n",
      "(1459380, 36, 6)\n",
      "(1459380, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1112340 samples, validate on 342720 samples\n",
      "Epoch 1/70\n",
      "1112340/1112340 [==============================] - 17s 15us/step - loss: 1.1351 - acc: 0.5035 - val_loss: 0.7007 - val_acc: 0.5224\n",
      "Epoch 2/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.7441 - acc: 0.5142 - val_loss: 0.6822 - val_acc: 0.5752\n",
      "Epoch 3/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.6963 - acc: 0.5366 - val_loss: 0.6657 - val_acc: 0.6190\n",
      "Epoch 4/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.6742 - acc: 0.5709 - val_loss: 0.6347 - val_acc: 0.6502\n",
      "Epoch 5/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.6448 - acc: 0.6045 - val_loss: 0.5836 - val_acc: 0.6633\n",
      "Epoch 6/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.6025 - acc: 0.6259 - val_loss: 0.5318 - val_acc: 0.6655\n",
      "Epoch 7/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5641 - acc: 0.6423 - val_loss: 0.5029 - val_acc: 0.6824\n",
      "Epoch 8/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5409 - acc: 0.6574 - val_loss: 0.4890 - val_acc: 0.6913\n",
      "Epoch 9/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5278 - acc: 0.6669 - val_loss: 0.4813 - val_acc: 0.6957\n",
      "Epoch 10/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5193 - acc: 0.6738 - val_loss: 0.4762 - val_acc: 0.6985\n",
      "Epoch 11/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5124 - acc: 0.6780 - val_loss: 0.4726 - val_acc: 0.7004\n",
      "Epoch 12/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5070 - acc: 0.6817 - val_loss: 0.4701 - val_acc: 0.7015\n",
      "Epoch 13/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.5018 - acc: 0.6848 - val_loss: 0.4685 - val_acc: 0.7016\n",
      "Epoch 14/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4973 - acc: 0.6869 - val_loss: 0.4675 - val_acc: 0.7022\n",
      "Epoch 15/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4931 - acc: 0.6876 - val_loss: 0.4678 - val_acc: 0.7022\n",
      "Epoch 16/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4896 - acc: 0.6892 - val_loss: 0.4688 - val_acc: 0.7025\n",
      "Epoch 17/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4872 - acc: 0.6899 - val_loss: 0.4700 - val_acc: 0.7043\n",
      "Epoch 18/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4850 - acc: 0.6915 - val_loss: 0.4711 - val_acc: 0.7062\n",
      "Epoch 19/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4832 - acc: 0.6920 - val_loss: 0.4723 - val_acc: 0.7078\n",
      "Epoch 20/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4819 - acc: 0.6931 - val_loss: 0.4736 - val_acc: 0.7073\n",
      "Epoch 21/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4808 - acc: 0.6935 - val_loss: 0.4743 - val_acc: 0.7076\n",
      "Epoch 22/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4796 - acc: 0.6939 - val_loss: 0.4752 - val_acc: 0.7085\n",
      "Epoch 23/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4787 - acc: 0.6942 - val_loss: 0.4764 - val_acc: 0.7084\n",
      "Epoch 24/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4773 - acc: 0.6949 - val_loss: 0.4774 - val_acc: 0.7094\n",
      "Epoch 25/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4767 - acc: 0.6956 - val_loss: 0.4780 - val_acc: 0.7100\n",
      "Epoch 26/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4757 - acc: 0.6959 - val_loss: 0.4787 - val_acc: 0.7126\n",
      "Epoch 27/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4748 - acc: 0.6968 - val_loss: 0.4790 - val_acc: 0.7132\n",
      "Epoch 28/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4743 - acc: 0.6966 - val_loss: 0.4796 - val_acc: 0.7138\n",
      "Epoch 29/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4738 - acc: 0.6971 - val_loss: 0.4801 - val_acc: 0.7137\n",
      "Epoch 30/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4733 - acc: 0.6975 - val_loss: 0.4806 - val_acc: 0.7141\n",
      "Epoch 31/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4729 - acc: 0.6973 - val_loss: 0.4810 - val_acc: 0.7141\n",
      "Epoch 32/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4724 - acc: 0.6974 - val_loss: 0.4813 - val_acc: 0.7139\n",
      "Epoch 33/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4718 - acc: 0.6977 - val_loss: 0.4816 - val_acc: 0.7141\n",
      "Epoch 34/70\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4717 - acc: 0.6982 - val_loss: 0.4816 - val_acc: 0.7137\n",
      "Epoch 35/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4711 - acc: 0.6979 - val_loss: 0.4822 - val_acc: 0.7141\n",
      "Epoch 36/70\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4708 - acc: 0.6983 - val_loss: 0.4824 - val_acc: 0.7139\n",
      "Epoch 37/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4706 - acc: 0.6979 - val_loss: 0.4825 - val_acc: 0.7138\n",
      "Epoch 38/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.6985 - val_loss: 0.4824 - val_acc: 0.7135\n",
      "Epoch 39/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.6986 - val_loss: 0.4824 - val_acc: 0.7138\n",
      "Epoch 40/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4697 - acc: 0.6983 - val_loss: 0.4825 - val_acc: 0.7134\n",
      "Epoch 41/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4698 - acc: 0.6983 - val_loss: 0.4829 - val_acc: 0.7137\n",
      "Epoch 42/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.6989 - val_loss: 0.4829 - val_acc: 0.7134\n",
      "Epoch 43/70\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4691 - acc: 0.6990 - val_loss: 0.4832 - val_acc: 0.7136\n",
      "Epoch 44/70\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4688 - acc: 0.6980 - val_loss: 0.4832 - val_acc: 0.7138\n",
      "Epoch 45/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.6992 - val_loss: 0.4836 - val_acc: 0.7138\n",
      "Epoch 46/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.6991 - val_loss: 0.4834 - val_acc: 0.7139\n",
      "Epoch 47/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.6994 - val_loss: 0.4835 - val_acc: 0.7139\n",
      "Epoch 48/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.6989 - val_loss: 0.4837 - val_acc: 0.7138\n",
      "Epoch 49/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.6992 - val_loss: 0.4837 - val_acc: 0.7138\n",
      "Epoch 50/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.6988 - val_loss: 0.4837 - val_acc: 0.7136\n",
      "Epoch 51/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.6994 - val_loss: 0.4840 - val_acc: 0.7137\n",
      "Epoch 52/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4676 - acc: 0.6994 - val_loss: 0.4841 - val_acc: 0.7135\n",
      "Epoch 53/70\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4674 - acc: 0.6994 - val_loss: 0.4840 - val_acc: 0.7138\n",
      "Epoch 54/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4676 - acc: 0.6995 - val_loss: 0.4840 - val_acc: 0.7135\n",
      "Epoch 55/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4672 - acc: 0.6992 - val_loss: 0.4839 - val_acc: 0.7136\n",
      "Epoch 56/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.6995 - val_loss: 0.4841 - val_acc: 0.7134\n",
      "Epoch 57/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.6995 - val_loss: 0.4840 - val_acc: 0.7135\n",
      "Epoch 58/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.6990 - val_loss: 0.4844 - val_acc: 0.7135\n",
      "Epoch 59/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4668 - acc: 0.6995 - val_loss: 0.4844 - val_acc: 0.7136\n",
      "Epoch 60/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4665 - acc: 0.6996 - val_loss: 0.4840 - val_acc: 0.7135\n",
      "Epoch 61/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.6999 - val_loss: 0.4843 - val_acc: 0.7133\n",
      "Epoch 62/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.6998 - val_loss: 0.4844 - val_acc: 0.7135\n",
      "Epoch 63/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.6998 - val_loss: 0.4841 - val_acc: 0.7134\n",
      "Epoch 64/70\n",
      "1112340/1112340 [==============================] - 9s 8us/step - loss: 0.4662 - acc: 0.6998 - val_loss: 0.4844 - val_acc: 0.7134\n",
      "Epoch 65/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4660 - acc: 0.6994 - val_loss: 0.4840 - val_acc: 0.7135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.6997 - val_loss: 0.4844 - val_acc: 0.7133\n",
      "Epoch 67/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.6994 - val_loss: 0.4844 - val_acc: 0.7133\n",
      "Epoch 68/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7004 - val_loss: 0.4843 - val_acc: 0.7134\n",
      "Epoch 69/70\n",
      "1112340/1112340 [==============================] - 9s 9us/step - loss: 0.4655 - acc: 0.6999 - val_loss: 0.4844 - val_acc: 0.7134\n",
      "Epoch 70/70\n",
      "1112340/1112340 [==============================] - 10s 9us/step - loss: 0.4656 - acc: 0.7001 - val_loss: 0.4848 - val_acc: 0.7132\n",
      "[[0.5039002 ]\n",
      " [0.5039002 ]\n",
      " [0.5039002 ]\n",
      " ...\n",
      " [0.33842912]\n",
      " [0.45843926]\n",
      " [0.57887036]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7848243464052287\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "13\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2436 MiB, count=158, average=15.4 MiB\n",
      "(1478160, 36, 6)\n",
      "(1478160, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1084260 samples, validate on 389580 samples\n",
      "Epoch 1/70\n",
      "1084260/1084260 [==============================] - 17s 16us/step - loss: 1.3999 - acc: 0.5061 - val_loss: 0.6901 - val_acc: 0.5578\n",
      "Epoch 2/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.7551 - acc: 0.5114 - val_loss: 0.6846 - val_acc: 0.5640\n",
      "Epoch 3/70\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.7025 - acc: 0.5221 - val_loss: 0.6808 - val_acc: 0.5825\n",
      "Epoch 4/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6894 - acc: 0.5374 - val_loss: 0.6680 - val_acc: 0.6144\n",
      "Epoch 5/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6746 - acc: 0.5676 - val_loss: 0.6375 - val_acc: 0.6450\n",
      "Epoch 6/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6516 - acc: 0.5982 - val_loss: 0.5997 - val_acc: 0.6583\n",
      "Epoch 7/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.6252 - acc: 0.6180 - val_loss: 0.5668 - val_acc: 0.6648\n",
      "Epoch 8/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5990 - acc: 0.6314 - val_loss: 0.5391 - val_acc: 0.6692\n",
      "Epoch 9/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5757 - acc: 0.6423 - val_loss: 0.5186 - val_acc: 0.6735\n",
      "Epoch 10/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5565 - acc: 0.6509 - val_loss: 0.5029 - val_acc: 0.6802\n",
      "Epoch 11/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5426 - acc: 0.6580 - val_loss: 0.4935 - val_acc: 0.6908\n",
      "Epoch 12/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5313 - acc: 0.6642 - val_loss: 0.4871 - val_acc: 0.6950\n",
      "Epoch 13/70\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.5236 - acc: 0.6684 - val_loss: 0.4819 - val_acc: 0.6997\n",
      "Epoch 14/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5173 - acc: 0.6716 - val_loss: 0.4786 - val_acc: 0.7033\n",
      "Epoch 15/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5132 - acc: 0.6733 - val_loss: 0.4757 - val_acc: 0.7050\n",
      "Epoch 16/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5094 - acc: 0.6741 - val_loss: 0.4738 - val_acc: 0.7063\n",
      "Epoch 17/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5061 - acc: 0.6766 - val_loss: 0.4720 - val_acc: 0.7069\n",
      "Epoch 18/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5031 - acc: 0.6782 - val_loss: 0.4708 - val_acc: 0.7069\n",
      "Epoch 19/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.5010 - acc: 0.6799 - val_loss: 0.4700 - val_acc: 0.7064\n",
      "Epoch 20/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4990 - acc: 0.6818 - val_loss: 0.4691 - val_acc: 0.7062\n",
      "Epoch 21/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4968 - acc: 0.6832 - val_loss: 0.4687 - val_acc: 0.7071\n",
      "Epoch 22/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4946 - acc: 0.6847 - val_loss: 0.4682 - val_acc: 0.7075\n",
      "Epoch 23/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4936 - acc: 0.6853 - val_loss: 0.4677 - val_acc: 0.7082\n",
      "Epoch 24/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4922 - acc: 0.6859 - val_loss: 0.4672 - val_acc: 0.7086\n",
      "Epoch 25/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4911 - acc: 0.6866 - val_loss: 0.4674 - val_acc: 0.7077\n",
      "Epoch 26/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4897 - acc: 0.6867 - val_loss: 0.4674 - val_acc: 0.7086\n",
      "Epoch 27/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4884 - acc: 0.6885 - val_loss: 0.4678 - val_acc: 0.7079\n",
      "Epoch 28/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4865 - acc: 0.6899 - val_loss: 0.4679 - val_acc: 0.7073\n",
      "Epoch 29/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4843 - acc: 0.6911 - val_loss: 0.4689 - val_acc: 0.7065\n",
      "Epoch 30/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4828 - acc: 0.6923 - val_loss: 0.4696 - val_acc: 0.7059\n",
      "Epoch 31/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4812 - acc: 0.6918 - val_loss: 0.4701 - val_acc: 0.7056\n",
      "Epoch 32/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4800 - acc: 0.6925 - val_loss: 0.4712 - val_acc: 0.7057\n",
      "Epoch 33/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4790 - acc: 0.6926 - val_loss: 0.4716 - val_acc: 0.7061\n",
      "Epoch 34/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4782 - acc: 0.6936 - val_loss: 0.4721 - val_acc: 0.7063\n",
      "Epoch 35/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4777 - acc: 0.6932 - val_loss: 0.4730 - val_acc: 0.7054\n",
      "Epoch 36/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4771 - acc: 0.6936 - val_loss: 0.4735 - val_acc: 0.7047\n",
      "Epoch 37/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4762 - acc: 0.6946 - val_loss: 0.4740 - val_acc: 0.7045\n",
      "Epoch 38/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4753 - acc: 0.6949 - val_loss: 0.4756 - val_acc: 0.7051\n",
      "Epoch 39/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4742 - acc: 0.6953 - val_loss: 0.4762 - val_acc: 0.7065\n",
      "Epoch 40/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4737 - acc: 0.6959 - val_loss: 0.4773 - val_acc: 0.7059\n",
      "Epoch 41/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4728 - acc: 0.6961 - val_loss: 0.4776 - val_acc: 0.7054\n",
      "Epoch 42/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4723 - acc: 0.6970 - val_loss: 0.4783 - val_acc: 0.7059\n",
      "Epoch 43/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4719 - acc: 0.6974 - val_loss: 0.4785 - val_acc: 0.7058\n",
      "Epoch 44/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4715 - acc: 0.6973 - val_loss: 0.4790 - val_acc: 0.7055\n",
      "Epoch 45/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4709 - acc: 0.6975 - val_loss: 0.4799 - val_acc: 0.7050\n",
      "Epoch 46/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6978 - val_loss: 0.4799 - val_acc: 0.7062\n",
      "Epoch 47/70\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4702 - acc: 0.6985 - val_loss: 0.4801 - val_acc: 0.7080\n",
      "Epoch 48/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.6983 - val_loss: 0.4805 - val_acc: 0.7063\n",
      "Epoch 49/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.6987 - val_loss: 0.4808 - val_acc: 0.7053\n",
      "Epoch 50/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4699 - acc: 0.6988 - val_loss: 0.4812 - val_acc: 0.7063\n",
      "Epoch 51/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4694 - acc: 0.6985 - val_loss: 0.4809 - val_acc: 0.7053\n",
      "Epoch 52/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4691 - acc: 0.6985 - val_loss: 0.4814 - val_acc: 0.7098\n",
      "Epoch 53/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4689 - acc: 0.6988 - val_loss: 0.4816 - val_acc: 0.7085\n",
      "Epoch 54/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.6994 - val_loss: 0.4818 - val_acc: 0.7105\n",
      "Epoch 55/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4684 - acc: 0.6996 - val_loss: 0.4818 - val_acc: 0.7107\n",
      "Epoch 56/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.6993 - val_loss: 0.4818 - val_acc: 0.7108\n",
      "Epoch 57/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.6993 - val_loss: 0.4817 - val_acc: 0.7111\n",
      "Epoch 58/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.6998 - val_loss: 0.4819 - val_acc: 0.7111\n",
      "Epoch 59/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.6998 - val_loss: 0.4821 - val_acc: 0.7112\n",
      "Epoch 60/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4677 - acc: 0.7004 - val_loss: 0.4821 - val_acc: 0.7112\n",
      "Epoch 61/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7001 - val_loss: 0.4822 - val_acc: 0.7112\n",
      "Epoch 62/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4674 - acc: 0.6998 - val_loss: 0.4822 - val_acc: 0.7112\n",
      "Epoch 63/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.6998 - val_loss: 0.4823 - val_acc: 0.7112\n",
      "Epoch 64/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7002 - val_loss: 0.4824 - val_acc: 0.7114\n",
      "Epoch 65/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.6997 - val_loss: 0.4831 - val_acc: 0.7111\n",
      "Epoch 66/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4668 - acc: 0.7000 - val_loss: 0.4826 - val_acc: 0.7114\n",
      "Epoch 67/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4666 - acc: 0.7004 - val_loss: 0.4830 - val_acc: 0.7114\n",
      "Epoch 68/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7007 - val_loss: 0.4830 - val_acc: 0.7114\n",
      "Epoch 69/70\n",
      "1084260/1084260 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7009 - val_loss: 0.4830 - val_acc: 0.7116\n",
      "Epoch 70/70\n",
      "1084260/1084260 [==============================] - 9s 9us/step - loss: 0.4664 - acc: 0.7008 - val_loss: 0.4834 - val_acc: 0.7116\n",
      "[[0.5014927]\n",
      " [0.5014927]\n",
      " [0.5014927]\n",
      " ...\n",
      " [0.371034 ]\n",
      " [0.4752978]\n",
      " [0.5347491]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7839673494532573\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "14\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2495 MiB, count=158, average=15.8 MiB\n",
      "(1513800, 36, 6)\n",
      "(1513800, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1133640 samples, validate on 375840 samples\n",
      "Epoch 1/70\n",
      "1133640/1133640 [==============================] - 18s 16us/step - loss: 1.9840 - acc: 0.5071 - val_loss: 0.6895 - val_acc: 0.5510\n",
      "Epoch 2/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.8569 - acc: 0.5166 - val_loss: 0.6696 - val_acc: 0.6093\n",
      "Epoch 3/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.7272 - acc: 0.5329 - val_loss: 0.6613 - val_acc: 0.6348\n",
      "Epoch 4/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6916 - acc: 0.5539 - val_loss: 0.6447 - val_acc: 0.6489\n",
      "Epoch 5/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6689 - acc: 0.5786 - val_loss: 0.6189 - val_acc: 0.6571\n",
      "Epoch 6/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6461 - acc: 0.6000 - val_loss: 0.5862 - val_acc: 0.6622\n",
      "Epoch 7/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.6171 - acc: 0.6195 - val_loss: 0.5447 - val_acc: 0.6661\n",
      "Epoch 8/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5839 - acc: 0.6369 - val_loss: 0.5139 - val_acc: 0.6712\n",
      "Epoch 9/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5559 - acc: 0.6497 - val_loss: 0.4955 - val_acc: 0.6789\n",
      "Epoch 10/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5369 - acc: 0.6593 - val_loss: 0.4850 - val_acc: 0.6895\n",
      "Epoch 11/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5252 - acc: 0.6672 - val_loss: 0.4790 - val_acc: 0.6947\n",
      "Epoch 12/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5173 - acc: 0.6740 - val_loss: 0.4747 - val_acc: 0.6993\n",
      "Epoch 13/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5118 - acc: 0.6795 - val_loss: 0.4715 - val_acc: 0.7043\n",
      "Epoch 14/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5074 - acc: 0.6836 - val_loss: 0.4693 - val_acc: 0.7061\n",
      "Epoch 15/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5035 - acc: 0.6865 - val_loss: 0.4678 - val_acc: 0.7078\n",
      "Epoch 16/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.5010 - acc: 0.6885 - val_loss: 0.4670 - val_acc: 0.7090\n",
      "Epoch 17/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4984 - acc: 0.6903 - val_loss: 0.4665 - val_acc: 0.7107\n",
      "Epoch 18/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4965 - acc: 0.6915 - val_loss: 0.4662 - val_acc: 0.7117\n",
      "Epoch 19/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4941 - acc: 0.6930 - val_loss: 0.4660 - val_acc: 0.7127\n",
      "Epoch 20/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4925 - acc: 0.6939 - val_loss: 0.4656 - val_acc: 0.7118\n",
      "Epoch 21/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4903 - acc: 0.6944 - val_loss: 0.4655 - val_acc: 0.7118\n",
      "Epoch 22/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4887 - acc: 0.6951 - val_loss: 0.4657 - val_acc: 0.7121\n",
      "Epoch 23/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4873 - acc: 0.6952 - val_loss: 0.4656 - val_acc: 0.7112\n",
      "Epoch 24/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4858 - acc: 0.6959 - val_loss: 0.4656 - val_acc: 0.7107\n",
      "Epoch 25/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4850 - acc: 0.6962 - val_loss: 0.4652 - val_acc: 0.7108\n",
      "Epoch 26/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4837 - acc: 0.6967 - val_loss: 0.4652 - val_acc: 0.7108\n",
      "Epoch 27/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4826 - acc: 0.6977 - val_loss: 0.4655 - val_acc: 0.7108\n",
      "Epoch 28/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4818 - acc: 0.6977 - val_loss: 0.4655 - val_acc: 0.7107\n",
      "Epoch 29/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4811 - acc: 0.6985 - val_loss: 0.4659 - val_acc: 0.7109\n",
      "Epoch 30/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4809 - acc: 0.6984 - val_loss: 0.4661 - val_acc: 0.7106\n",
      "Epoch 31/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4802 - acc: 0.6990 - val_loss: 0.4662 - val_acc: 0.7110\n",
      "Epoch 32/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4795 - acc: 0.6993 - val_loss: 0.4665 - val_acc: 0.7121\n",
      "Epoch 33/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4793 - acc: 0.6989 - val_loss: 0.4668 - val_acc: 0.7119\n",
      "Epoch 34/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4786 - acc: 0.7000 - val_loss: 0.4666 - val_acc: 0.7124\n",
      "Epoch 35/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4786 - acc: 0.7000 - val_loss: 0.4671 - val_acc: 0.7131\n",
      "Epoch 36/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4783 - acc: 0.6999 - val_loss: 0.4672 - val_acc: 0.7131\n",
      "Epoch 37/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4780 - acc: 0.7003 - val_loss: 0.4674 - val_acc: 0.7120\n",
      "Epoch 38/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4776 - acc: 0.7001 - val_loss: 0.4677 - val_acc: 0.7122\n",
      "Epoch 39/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4776 - acc: 0.7006 - val_loss: 0.4678 - val_acc: 0.7122\n",
      "Epoch 40/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4772 - acc: 0.7010 - val_loss: 0.4678 - val_acc: 0.7111\n",
      "Epoch 41/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4768 - acc: 0.7013 - val_loss: 0.4682 - val_acc: 0.7122\n",
      "Epoch 42/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4766 - acc: 0.7016 - val_loss: 0.4682 - val_acc: 0.7125\n",
      "Epoch 43/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4766 - acc: 0.7011 - val_loss: 0.4684 - val_acc: 0.7123\n",
      "Epoch 44/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4761 - acc: 0.7018 - val_loss: 0.4687 - val_acc: 0.7120\n",
      "Epoch 45/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4761 - acc: 0.7018 - val_loss: 0.4685 - val_acc: 0.7120\n",
      "Epoch 46/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4758 - acc: 0.7022 - val_loss: 0.4688 - val_acc: 0.7119\n",
      "Epoch 47/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4758 - acc: 0.7022 - val_loss: 0.4686 - val_acc: 0.7117\n",
      "Epoch 48/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4755 - acc: 0.7029 - val_loss: 0.4688 - val_acc: 0.7122\n",
      "Epoch 49/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4753 - acc: 0.7026 - val_loss: 0.4689 - val_acc: 0.7122\n",
      "Epoch 50/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4752 - acc: 0.7032 - val_loss: 0.4690 - val_acc: 0.7123\n",
      "Epoch 51/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4752 - acc: 0.7032 - val_loss: 0.4689 - val_acc: 0.7124\n",
      "Epoch 52/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4750 - acc: 0.7030 - val_loss: 0.4693 - val_acc: 0.7121\n",
      "Epoch 53/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4749 - acc: 0.7031 - val_loss: 0.4694 - val_acc: 0.7123\n",
      "Epoch 54/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4746 - acc: 0.7032 - val_loss: 0.4695 - val_acc: 0.7126\n",
      "Epoch 55/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4747 - acc: 0.7030 - val_loss: 0.4697 - val_acc: 0.7125\n",
      "Epoch 56/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4744 - acc: 0.7041 - val_loss: 0.4694 - val_acc: 0.7123\n",
      "Epoch 57/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4743 - acc: 0.7038 - val_loss: 0.4698 - val_acc: 0.7129\n",
      "Epoch 58/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4741 - acc: 0.7038 - val_loss: 0.4699 - val_acc: 0.7131\n",
      "Epoch 59/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4741 - acc: 0.7041 - val_loss: 0.4697 - val_acc: 0.7129\n",
      "Epoch 60/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4742 - acc: 0.7040 - val_loss: 0.4703 - val_acc: 0.7130\n",
      "Epoch 61/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4739 - acc: 0.7043 - val_loss: 0.4705 - val_acc: 0.7134\n",
      "Epoch 62/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4737 - acc: 0.7044 - val_loss: 0.4704 - val_acc: 0.7132\n",
      "Epoch 63/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4739 - acc: 0.7040 - val_loss: 0.4698 - val_acc: 0.7133\n",
      "Epoch 64/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4740 - acc: 0.7048 - val_loss: 0.4704 - val_acc: 0.7138\n",
      "Epoch 65/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4737 - acc: 0.7051 - val_loss: 0.4705 - val_acc: 0.7138\n",
      "Epoch 66/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4738 - acc: 0.7045 - val_loss: 0.4703 - val_acc: 0.7133\n",
      "Epoch 67/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.7051 - val_loss: 0.4704 - val_acc: 0.7137\n",
      "Epoch 68/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4731 - acc: 0.7053 - val_loss: 0.4704 - val_acc: 0.7137\n",
      "Epoch 69/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4737 - acc: 0.7046 - val_loss: 0.4711 - val_acc: 0.7139\n",
      "Epoch 70/70\n",
      "1133640/1133640 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.7048 - val_loss: 0.4707 - val_acc: 0.7136\n",
      "[[0.52556276]\n",
      " [0.5147029 ]\n",
      " [0.53410757]\n",
      " ...\n",
      " [0.2686796 ]\n",
      " [0.45881468]\n",
      " [0.5449825 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7866379310344828\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "15\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2438 MiB, count=158, average=15.4 MiB\n",
      "(1479660, 36, 6)\n",
      "(1479660, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1116780 samples, validate on 358560 samples\n",
      "Epoch 1/70\n",
      "1116780/1116780 [==============================] - 18s 16us/step - loss: 1.1477 - acc: 0.5022 - val_loss: 0.6938 - val_acc: 0.5373\n",
      "Epoch 2/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.7436 - acc: 0.5146 - val_loss: 0.6804 - val_acc: 0.5820\n",
      "Epoch 3/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.7016 - acc: 0.5335 - val_loss: 0.6667 - val_acc: 0.6205\n",
      "Epoch 4/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6815 - acc: 0.5635 - val_loss: 0.6421 - val_acc: 0.6472\n",
      "Epoch 5/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6577 - acc: 0.5938 - val_loss: 0.6070 - val_acc: 0.6601\n",
      "Epoch 6/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.6251 - acc: 0.6176 - val_loss: 0.5629 - val_acc: 0.6628\n",
      "Epoch 7/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5893 - acc: 0.6347 - val_loss: 0.5265 - val_acc: 0.6650\n",
      "Epoch 8/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5612 - acc: 0.6451 - val_loss: 0.5040 - val_acc: 0.6695\n",
      "Epoch 9/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5426 - acc: 0.6531 - val_loss: 0.4928 - val_acc: 0.6766\n",
      "Epoch 10/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5314 - acc: 0.6587 - val_loss: 0.4873 - val_acc: 0.6857\n",
      "Epoch 11/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5242 - acc: 0.6643 - val_loss: 0.4841 - val_acc: 0.6890\n",
      "Epoch 12/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5192 - acc: 0.6684 - val_loss: 0.4817 - val_acc: 0.6945\n",
      "Epoch 13/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5154 - acc: 0.6729 - val_loss: 0.4794 - val_acc: 0.6984\n",
      "Epoch 14/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5118 - acc: 0.6770 - val_loss: 0.4772 - val_acc: 0.7005\n",
      "Epoch 15/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5086 - acc: 0.6802 - val_loss: 0.4754 - val_acc: 0.7028\n",
      "Epoch 16/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5062 - acc: 0.6826 - val_loss: 0.4741 - val_acc: 0.7052\n",
      "Epoch 17/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5035 - acc: 0.6857 - val_loss: 0.4733 - val_acc: 0.7072\n",
      "Epoch 18/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.5012 - acc: 0.6873 - val_loss: 0.4728 - val_acc: 0.7077\n",
      "Epoch 19/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4993 - acc: 0.6888 - val_loss: 0.4724 - val_acc: 0.7075\n",
      "Epoch 20/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4977 - acc: 0.6897 - val_loss: 0.4722 - val_acc: 0.7087\n",
      "Epoch 21/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4960 - acc: 0.6908 - val_loss: 0.4721 - val_acc: 0.7093\n",
      "Epoch 22/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4947 - acc: 0.6916 - val_loss: 0.4721 - val_acc: 0.7090\n",
      "Epoch 23/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4937 - acc: 0.6927 - val_loss: 0.4722 - val_acc: 0.7099\n",
      "Epoch 24/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4926 - acc: 0.6938 - val_loss: 0.4725 - val_acc: 0.7099\n",
      "Epoch 25/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4920 - acc: 0.6944 - val_loss: 0.4725 - val_acc: 0.7106\n",
      "Epoch 26/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4912 - acc: 0.6952 - val_loss: 0.4726 - val_acc: 0.7107\n",
      "Epoch 27/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4907 - acc: 0.6958 - val_loss: 0.4728 - val_acc: 0.7109\n",
      "Epoch 28/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4898 - acc: 0.6967 - val_loss: 0.4728 - val_acc: 0.7119\n",
      "Epoch 29/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4890 - acc: 0.6972 - val_loss: 0.4731 - val_acc: 0.7117\n",
      "Epoch 30/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4882 - acc: 0.6977 - val_loss: 0.4736 - val_acc: 0.7110\n",
      "Epoch 31/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4875 - acc: 0.6985 - val_loss: 0.4734 - val_acc: 0.7112\n",
      "Epoch 32/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4871 - acc: 0.6988 - val_loss: 0.4738 - val_acc: 0.7113\n",
      "Epoch 33/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4869 - acc: 0.6992 - val_loss: 0.4738 - val_acc: 0.7117\n",
      "Epoch 34/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4864 - acc: 0.6995 - val_loss: 0.4739 - val_acc: 0.7119\n",
      "Epoch 35/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4862 - acc: 0.6997 - val_loss: 0.4740 - val_acc: 0.7116\n",
      "Epoch 36/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4854 - acc: 0.6998 - val_loss: 0.4738 - val_acc: 0.7120\n",
      "Epoch 37/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4854 - acc: 0.7000 - val_loss: 0.4740 - val_acc: 0.7109\n",
      "Epoch 38/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4847 - acc: 0.7006 - val_loss: 0.4740 - val_acc: 0.7111\n",
      "Epoch 39/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4845 - acc: 0.7004 - val_loss: 0.4741 - val_acc: 0.7115\n",
      "Epoch 40/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4843 - acc: 0.7007 - val_loss: 0.4741 - val_acc: 0.7113\n",
      "Epoch 41/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4842 - acc: 0.7010 - val_loss: 0.4741 - val_acc: 0.7113\n",
      "Epoch 42/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4837 - acc: 0.7014 - val_loss: 0.4739 - val_acc: 0.7118\n",
      "Epoch 43/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4835 - acc: 0.7011 - val_loss: 0.4740 - val_acc: 0.7113\n",
      "Epoch 44/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4832 - acc: 0.7018 - val_loss: 0.4740 - val_acc: 0.7113\n",
      "Epoch 45/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4827 - acc: 0.7019 - val_loss: 0.4738 - val_acc: 0.7109\n",
      "Epoch 46/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4824 - acc: 0.7025 - val_loss: 0.4737 - val_acc: 0.7105\n",
      "Epoch 47/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4824 - acc: 0.7021 - val_loss: 0.4736 - val_acc: 0.7106\n",
      "Epoch 48/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4820 - acc: 0.7023 - val_loss: 0.4736 - val_acc: 0.7105\n",
      "Epoch 49/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4821 - acc: 0.7027 - val_loss: 0.4735 - val_acc: 0.7106\n",
      "Epoch 50/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4816 - acc: 0.7028 - val_loss: 0.4738 - val_acc: 0.7097\n",
      "Epoch 51/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4811 - acc: 0.7029 - val_loss: 0.4737 - val_acc: 0.7103\n",
      "Epoch 52/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4808 - acc: 0.7038 - val_loss: 0.4738 - val_acc: 0.7096\n",
      "Epoch 53/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4807 - acc: 0.7034 - val_loss: 0.4739 - val_acc: 0.7096\n",
      "Epoch 54/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4803 - acc: 0.7040 - val_loss: 0.4738 - val_acc: 0.7098\n",
      "Epoch 55/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4802 - acc: 0.7035 - val_loss: 0.4739 - val_acc: 0.7101\n",
      "Epoch 56/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4800 - acc: 0.7039 - val_loss: 0.4738 - val_acc: 0.7103\n",
      "Epoch 57/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4798 - acc: 0.7039 - val_loss: 0.4738 - val_acc: 0.7103\n",
      "Epoch 58/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4797 - acc: 0.7037 - val_loss: 0.4739 - val_acc: 0.7108\n",
      "Epoch 59/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4792 - acc: 0.7036 - val_loss: 0.4742 - val_acc: 0.7112\n",
      "Epoch 60/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4793 - acc: 0.7037 - val_loss: 0.4740 - val_acc: 0.7114\n",
      "Epoch 61/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4789 - acc: 0.7044 - val_loss: 0.4741 - val_acc: 0.7116\n",
      "Epoch 62/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4787 - acc: 0.7040 - val_loss: 0.4742 - val_acc: 0.7120\n",
      "Epoch 63/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4784 - acc: 0.7040 - val_loss: 0.4741 - val_acc: 0.7122\n",
      "Epoch 64/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4782 - acc: 0.7046 - val_loss: 0.4743 - val_acc: 0.7122\n",
      "Epoch 65/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4779 - acc: 0.7048 - val_loss: 0.4742 - val_acc: 0.7121\n",
      "Epoch 66/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4778 - acc: 0.7047 - val_loss: 0.4745 - val_acc: 0.7120\n",
      "Epoch 67/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4775 - acc: 0.7050 - val_loss: 0.4745 - val_acc: 0.7121\n",
      "Epoch 68/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4776 - acc: 0.7044 - val_loss: 0.4745 - val_acc: 0.7123\n",
      "Epoch 69/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4772 - acc: 0.7051 - val_loss: 0.4748 - val_acc: 0.7122\n",
      "Epoch 70/70\n",
      "1116780/1116780 [==============================] - 10s 9us/step - loss: 0.4769 - acc: 0.7054 - val_loss: 0.4748 - val_acc: 0.7124\n",
      "[[0.5275703 ]\n",
      " [0.53304654]\n",
      " [0.54017955]\n",
      " ...\n",
      " [0.32134098]\n",
      " [0.4477046 ]\n",
      " [0.6179233 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.785059683177153\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "16\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2521 MiB, count=158, average=16.0 MiB\n",
      "(1530060, 36, 6)\n",
      "(1530060, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1132620 samples, validate on 393120 samples\n",
      "Epoch 1/70\n",
      "1132620/1132620 [==============================] - 19s 16us/step - loss: 1.9021 - acc: 0.5059 - val_loss: 0.6930 - val_acc: 0.5509\n",
      "Epoch 2/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132620/1132620 [==============================] - 10s 8us/step - loss: 0.8348 - acc: 0.5146 - val_loss: 0.6767 - val_acc: 0.5808\n",
      "Epoch 3/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.7178 - acc: 0.5306 - val_loss: 0.6643 - val_acc: 0.6257\n",
      "Epoch 4/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.6883 - acc: 0.5540 - val_loss: 0.6461 - val_acc: 0.6465\n",
      "Epoch 5/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.6686 - acc: 0.5787 - val_loss: 0.6171 - val_acc: 0.6551\n",
      "Epoch 6/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.6440 - acc: 0.6011 - val_loss: 0.5780 - val_acc: 0.6642\n",
      "Epoch 7/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.6127 - acc: 0.6211 - val_loss: 0.5384 - val_acc: 0.6707\n",
      "Epoch 8/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5795 - acc: 0.6368 - val_loss: 0.5083 - val_acc: 0.6778\n",
      "Epoch 9/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5503 - acc: 0.6511 - val_loss: 0.4891 - val_acc: 0.6892\n",
      "Epoch 10/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5305 - acc: 0.6632 - val_loss: 0.4793 - val_acc: 0.6929\n",
      "Epoch 11/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5178 - acc: 0.6724 - val_loss: 0.4737 - val_acc: 0.7007\n",
      "Epoch 12/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5091 - acc: 0.6776 - val_loss: 0.4702 - val_acc: 0.7041\n",
      "Epoch 13/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.5025 - acc: 0.6812 - val_loss: 0.4683 - val_acc: 0.7060\n",
      "Epoch 14/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4977 - acc: 0.6835 - val_loss: 0.4671 - val_acc: 0.7068\n",
      "Epoch 15/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4939 - acc: 0.6850 - val_loss: 0.4665 - val_acc: 0.7079\n",
      "Epoch 16/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4911 - acc: 0.6864 - val_loss: 0.4666 - val_acc: 0.7082\n",
      "Epoch 17/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4883 - acc: 0.6880 - val_loss: 0.4668 - val_acc: 0.7082\n",
      "Epoch 18/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4858 - acc: 0.6891 - val_loss: 0.4671 - val_acc: 0.7084\n",
      "Epoch 19/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4844 - acc: 0.6895 - val_loss: 0.4676 - val_acc: 0.7091\n",
      "Epoch 20/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4826 - acc: 0.6913 - val_loss: 0.4680 - val_acc: 0.7087\n",
      "Epoch 21/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4810 - acc: 0.6920 - val_loss: 0.4688 - val_acc: 0.7101\n",
      "Epoch 22/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4796 - acc: 0.6935 - val_loss: 0.4695 - val_acc: 0.7108\n",
      "Epoch 23/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4785 - acc: 0.6938 - val_loss: 0.4699 - val_acc: 0.7098\n",
      "Epoch 24/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4775 - acc: 0.6955 - val_loss: 0.4705 - val_acc: 0.7111\n",
      "Epoch 25/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4766 - acc: 0.6956 - val_loss: 0.4711 - val_acc: 0.7114\n",
      "Epoch 26/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4755 - acc: 0.6964 - val_loss: 0.4714 - val_acc: 0.7123\n",
      "Epoch 27/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4752 - acc: 0.6970 - val_loss: 0.4716 - val_acc: 0.7118\n",
      "Epoch 28/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4743 - acc: 0.6969 - val_loss: 0.4719 - val_acc: 0.7122\n",
      "Epoch 29/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4737 - acc: 0.6974 - val_loss: 0.4722 - val_acc: 0.7107\n",
      "Epoch 30/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4733 - acc: 0.6976 - val_loss: 0.4725 - val_acc: 0.7133\n",
      "Epoch 31/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4728 - acc: 0.6979 - val_loss: 0.4727 - val_acc: 0.7130\n",
      "Epoch 32/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4721 - acc: 0.6993 - val_loss: 0.4729 - val_acc: 0.7140\n",
      "Epoch 33/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4718 - acc: 0.6985 - val_loss: 0.4734 - val_acc: 0.7126\n",
      "Epoch 34/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4713 - acc: 0.6989 - val_loss: 0.4733 - val_acc: 0.7130\n",
      "Epoch 35/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6994 - val_loss: 0.4737 - val_acc: 0.7123\n",
      "Epoch 36/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6995 - val_loss: 0.4737 - val_acc: 0.7126\n",
      "Epoch 37/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4704 - acc: 0.7001 - val_loss: 0.4740 - val_acc: 0.7128\n",
      "Epoch 38/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.6992 - val_loss: 0.4743 - val_acc: 0.7123\n",
      "Epoch 39/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4698 - acc: 0.7001 - val_loss: 0.4746 - val_acc: 0.7126\n",
      "Epoch 40/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4694 - acc: 0.6996 - val_loss: 0.4746 - val_acc: 0.7152\n",
      "Epoch 41/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.6997 - val_loss: 0.4749 - val_acc: 0.7152\n",
      "Epoch 42/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4693 - acc: 0.7003 - val_loss: 0.4746 - val_acc: 0.7128\n",
      "Epoch 43/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4689 - acc: 0.7008 - val_loss: 0.4748 - val_acc: 0.7122\n",
      "Epoch 44/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.7000 - val_loss: 0.4748 - val_acc: 0.7130\n",
      "Epoch 45/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.7008 - val_loss: 0.4749 - val_acc: 0.7149\n",
      "Epoch 46/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4680 - acc: 0.7005 - val_loss: 0.4749 - val_acc: 0.7153\n",
      "Epoch 47/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.7007 - val_loss: 0.4755 - val_acc: 0.7152\n",
      "Epoch 48/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4680 - acc: 0.7007 - val_loss: 0.4755 - val_acc: 0.7152\n",
      "Epoch 49/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4678 - acc: 0.7010 - val_loss: 0.4754 - val_acc: 0.7153\n",
      "Epoch 50/70\n",
      "1132620/1132620 [==============================] - 10s 8us/step - loss: 0.4676 - acc: 0.7003 - val_loss: 0.4757 - val_acc: 0.7154\n",
      "Epoch 51/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7006 - val_loss: 0.4759 - val_acc: 0.7151\n",
      "Epoch 52/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4674 - acc: 0.7005 - val_loss: 0.4759 - val_acc: 0.7154\n",
      "Epoch 53/70\n",
      "1132620/1132620 [==============================] - 10s 8us/step - loss: 0.4674 - acc: 0.7010 - val_loss: 0.4755 - val_acc: 0.7154\n",
      "Epoch 54/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7012 - val_loss: 0.4758 - val_acc: 0.7155\n",
      "Epoch 55/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7010 - val_loss: 0.4757 - val_acc: 0.7154\n",
      "Epoch 56/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7014 - val_loss: 0.4755 - val_acc: 0.7154\n",
      "Epoch 57/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7017 - val_loss: 0.4757 - val_acc: 0.7155\n",
      "Epoch 58/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7010 - val_loss: 0.4757 - val_acc: 0.7153\n",
      "Epoch 59/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4665 - acc: 0.7016 - val_loss: 0.4758 - val_acc: 0.7154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7011 - val_loss: 0.4762 - val_acc: 0.7153\n",
      "Epoch 61/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7009 - val_loss: 0.4761 - val_acc: 0.7153\n",
      "Epoch 62/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7013 - val_loss: 0.4760 - val_acc: 0.7153\n",
      "Epoch 63/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7017 - val_loss: 0.4763 - val_acc: 0.7134\n",
      "Epoch 64/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4660 - acc: 0.7016 - val_loss: 0.4762 - val_acc: 0.7134\n",
      "Epoch 65/70\n",
      "1132620/1132620 [==============================] - 10s 8us/step - loss: 0.4661 - acc: 0.7016 - val_loss: 0.4762 - val_acc: 0.7133\n",
      "Epoch 66/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7019 - val_loss: 0.4762 - val_acc: 0.7155\n",
      "Epoch 67/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7016 - val_loss: 0.4763 - val_acc: 0.7130\n",
      "Epoch 68/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4658 - acc: 0.7020 - val_loss: 0.4763 - val_acc: 0.7155\n",
      "Epoch 69/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4658 - acc: 0.7013 - val_loss: 0.4763 - val_acc: 0.7153\n",
      "Epoch 70/70\n",
      "1132620/1132620 [==============================] - 10s 9us/step - loss: 0.4656 - acc: 0.7018 - val_loss: 0.4764 - val_acc: 0.7155\n",
      "[[0.49991348]\n",
      " [0.49991348]\n",
      " [0.49991348]\n",
      " ...\n",
      " [0.31053808]\n",
      " [0.4704605 ]\n",
      " [0.5777299 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7856049043549044\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "17\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2476 MiB, count=158, average=15.7 MiB\n",
      "(1502580, 36, 6)\n",
      "(1502580, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1136160 samples, validate on 362100 samples\n",
      "Epoch 1/70\n",
      "1136160/1136160 [==============================] - 18s 16us/step - loss: 1.1081 - acc: 0.5026 - val_loss: 0.6964 - val_acc: 0.5334\n",
      "Epoch 2/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.7406 - acc: 0.5172 - val_loss: 0.6778 - val_acc: 0.5920\n",
      "Epoch 3/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.6950 - acc: 0.5427 - val_loss: 0.6583 - val_acc: 0.6280\n",
      "Epoch 4/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.6690 - acc: 0.5814 - val_loss: 0.6190 - val_acc: 0.6596\n",
      "Epoch 5/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.6375 - acc: 0.6162 - val_loss: 0.5696 - val_acc: 0.6683\n",
      "Epoch 6/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.6029 - acc: 0.6356 - val_loss: 0.5294 - val_acc: 0.6741\n",
      "Epoch 7/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5701 - acc: 0.6520 - val_loss: 0.5029 - val_acc: 0.6813\n",
      "Epoch 8/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5442 - acc: 0.6655 - val_loss: 0.4861 - val_acc: 0.6910\n",
      "Epoch 9/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5275 - acc: 0.6746 - val_loss: 0.4772 - val_acc: 0.7030\n",
      "Epoch 10/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5170 - acc: 0.6812 - val_loss: 0.4724 - val_acc: 0.7085\n",
      "Epoch 11/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5107 - acc: 0.6845 - val_loss: 0.4702 - val_acc: 0.7125\n",
      "Epoch 12/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5055 - acc: 0.6873 - val_loss: 0.4684 - val_acc: 0.7133\n",
      "Epoch 13/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.5018 - acc: 0.6880 - val_loss: 0.4677 - val_acc: 0.7139\n",
      "Epoch 14/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4976 - acc: 0.6898 - val_loss: 0.4661 - val_acc: 0.7145\n",
      "Epoch 15/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4940 - acc: 0.6910 - val_loss: 0.4651 - val_acc: 0.7154\n",
      "Epoch 16/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4899 - acc: 0.6930 - val_loss: 0.4642 - val_acc: 0.7157\n",
      "Epoch 17/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4866 - acc: 0.6940 - val_loss: 0.4645 - val_acc: 0.7158\n",
      "Epoch 18/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4836 - acc: 0.6950 - val_loss: 0.4651 - val_acc: 0.7157\n",
      "Epoch 19/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4815 - acc: 0.6961 - val_loss: 0.4662 - val_acc: 0.7156\n",
      "Epoch 20/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4796 - acc: 0.6967 - val_loss: 0.4669 - val_acc: 0.7158\n",
      "Epoch 21/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4781 - acc: 0.6976 - val_loss: 0.4679 - val_acc: 0.7155\n",
      "Epoch 22/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4770 - acc: 0.6983 - val_loss: 0.4686 - val_acc: 0.7155\n",
      "Epoch 23/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4759 - acc: 0.6990 - val_loss: 0.4691 - val_acc: 0.7156\n",
      "Epoch 24/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4752 - acc: 0.6999 - val_loss: 0.4695 - val_acc: 0.7154\n",
      "Epoch 25/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4745 - acc: 0.7008 - val_loss: 0.4701 - val_acc: 0.7158\n",
      "Epoch 26/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.7012 - val_loss: 0.4713 - val_acc: 0.7157\n",
      "Epoch 27/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4727 - acc: 0.7012 - val_loss: 0.4720 - val_acc: 0.7155\n",
      "Epoch 28/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4724 - acc: 0.7017 - val_loss: 0.4726 - val_acc: 0.7155\n",
      "Epoch 29/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4718 - acc: 0.7019 - val_loss: 0.4734 - val_acc: 0.7122\n",
      "Epoch 30/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4712 - acc: 0.7025 - val_loss: 0.4740 - val_acc: 0.7122\n",
      "Epoch 31/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4711 - acc: 0.7024 - val_loss: 0.4742 - val_acc: 0.7157\n",
      "Epoch 32/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4704 - acc: 0.7026 - val_loss: 0.4746 - val_acc: 0.7124\n",
      "Epoch 33/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4702 - acc: 0.7032 - val_loss: 0.4748 - val_acc: 0.7125\n",
      "Epoch 34/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4698 - acc: 0.7031 - val_loss: 0.4751 - val_acc: 0.7123\n",
      "Epoch 35/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4694 - acc: 0.7033 - val_loss: 0.4751 - val_acc: 0.7122\n",
      "Epoch 36/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4691 - acc: 0.7032 - val_loss: 0.4752 - val_acc: 0.7122\n",
      "Epoch 37/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.7030 - val_loss: 0.4754 - val_acc: 0.7123\n",
      "Epoch 38/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4688 - acc: 0.7033 - val_loss: 0.4755 - val_acc: 0.7120\n",
      "Epoch 39/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.7040 - val_loss: 0.4752 - val_acc: 0.7119\n",
      "Epoch 40/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.7040 - val_loss: 0.4756 - val_acc: 0.7120\n",
      "Epoch 41/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4682 - acc: 0.7041 - val_loss: 0.4754 - val_acc: 0.7122\n",
      "Epoch 42/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4680 - acc: 0.7040 - val_loss: 0.4756 - val_acc: 0.7118\n",
      "Epoch 43/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7042 - val_loss: 0.4757 - val_acc: 0.7119\n",
      "Epoch 44/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4679 - acc: 0.7043 - val_loss: 0.4760 - val_acc: 0.7123\n",
      "Epoch 45/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4677 - acc: 0.7042 - val_loss: 0.4757 - val_acc: 0.7120\n",
      "Epoch 46/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4676 - acc: 0.7043 - val_loss: 0.4758 - val_acc: 0.7120\n",
      "Epoch 47/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4676 - acc: 0.7048 - val_loss: 0.4758 - val_acc: 0.7119\n",
      "Epoch 48/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.7050 - val_loss: 0.4764 - val_acc: 0.7120\n",
      "Epoch 49/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7045 - val_loss: 0.4760 - val_acc: 0.7115\n",
      "Epoch 50/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7047 - val_loss: 0.4761 - val_acc: 0.7117\n",
      "Epoch 51/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7048 - val_loss: 0.4758 - val_acc: 0.7122\n",
      "Epoch 52/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7053 - val_loss: 0.4765 - val_acc: 0.7121\n",
      "Epoch 53/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7054 - val_loss: 0.4763 - val_acc: 0.7120\n",
      "Epoch 54/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7045 - val_loss: 0.4761 - val_acc: 0.7120\n",
      "Epoch 55/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4666 - acc: 0.7052 - val_loss: 0.4762 - val_acc: 0.7114\n",
      "Epoch 56/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7050 - val_loss: 0.4766 - val_acc: 0.7118\n",
      "Epoch 57/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.7051 - val_loss: 0.4766 - val_acc: 0.7121\n",
      "Epoch 58/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4665 - acc: 0.7052 - val_loss: 0.4765 - val_acc: 0.7119\n",
      "Epoch 59/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.7055 - val_loss: 0.4763 - val_acc: 0.7116\n",
      "Epoch 60/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7053 - val_loss: 0.4765 - val_acc: 0.7121\n",
      "Epoch 61/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4662 - acc: 0.7056 - val_loss: 0.4765 - val_acc: 0.7119\n",
      "Epoch 62/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4663 - acc: 0.7054 - val_loss: 0.4766 - val_acc: 0.7118\n",
      "Epoch 63/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4661 - acc: 0.7057 - val_loss: 0.4764 - val_acc: 0.7121\n",
      "Epoch 64/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4661 - acc: 0.7055 - val_loss: 0.4764 - val_acc: 0.7123\n",
      "Epoch 65/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4660 - acc: 0.7060 - val_loss: 0.4769 - val_acc: 0.7121\n",
      "Epoch 66/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4661 - acc: 0.7055 - val_loss: 0.4765 - val_acc: 0.7121\n",
      "Epoch 67/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4658 - acc: 0.7053 - val_loss: 0.4766 - val_acc: 0.7119\n",
      "Epoch 68/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7057 - val_loss: 0.4764 - val_acc: 0.7119\n",
      "Epoch 69/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4658 - acc: 0.7055 - val_loss: 0.4769 - val_acc: 0.7121\n",
      "Epoch 70/70\n",
      "1136160/1136160 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7057 - val_loss: 0.4768 - val_acc: 0.7123\n",
      "[[0.51797265]\n",
      " [0.51797265]\n",
      " [0.51797265]\n",
      " ...\n",
      " [0.3805198 ]\n",
      " [0.4414391 ]\n",
      " [0.51797265]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7861032863849765\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "18\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2470 MiB, count=158, average=15.6 MiB\n",
      "(1498980, 36, 6)\n",
      "(1498980, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1122420 samples, validate on 372240 samples\n",
      "Epoch 1/70\n",
      "1122420/1122420 [==============================] - 19s 17us/step - loss: 2.2198 - acc: 0.5030 - val_loss: 0.7098 - val_acc: 0.5371\n",
      "Epoch 2/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.8176 - acc: 0.5179 - val_loss: 0.6753 - val_acc: 0.5712\n",
      "Epoch 3/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.7121 - acc: 0.5344 - val_loss: 0.6658 - val_acc: 0.5978\n",
      "Epoch 4/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.6871 - acc: 0.5509 - val_loss: 0.6517 - val_acc: 0.6281\n",
      "Epoch 5/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.6726 - acc: 0.5681 - val_loss: 0.6314 - val_acc: 0.6433\n",
      "Epoch 6/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.6577 - acc: 0.5853 - val_loss: 0.6066 - val_acc: 0.6507\n",
      "Epoch 7/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.6392 - acc: 0.6038 - val_loss: 0.5768 - val_acc: 0.6583\n",
      "Epoch 8/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.6159 - acc: 0.6186 - val_loss: 0.5459 - val_acc: 0.6657\n",
      "Epoch 9/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5915 - acc: 0.6303 - val_loss: 0.5187 - val_acc: 0.6682\n",
      "Epoch 10/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5689 - acc: 0.6416 - val_loss: 0.5004 - val_acc: 0.6763\n",
      "Epoch 11/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5509 - acc: 0.6523 - val_loss: 0.4892 - val_acc: 0.6888\n",
      "Epoch 12/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5379 - acc: 0.6610 - val_loss: 0.4813 - val_acc: 0.6972\n",
      "Epoch 13/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5279 - acc: 0.6689 - val_loss: 0.4768 - val_acc: 0.7033\n",
      "Epoch 14/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5206 - acc: 0.6740 - val_loss: 0.4742 - val_acc: 0.7071\n",
      "Epoch 15/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5150 - acc: 0.6789 - val_loss: 0.4728 - val_acc: 0.7088\n",
      "Epoch 16/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5102 - acc: 0.6812 - val_loss: 0.4715 - val_acc: 0.7106\n",
      "Epoch 17/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5061 - acc: 0.6845 - val_loss: 0.4704 - val_acc: 0.7105\n",
      "Epoch 18/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.5015 - acc: 0.6865 - val_loss: 0.4688 - val_acc: 0.7110\n",
      "Epoch 19/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4978 - acc: 0.6885 - val_loss: 0.4677 - val_acc: 0.7121\n",
      "Epoch 20/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4945 - acc: 0.6899 - val_loss: 0.4675 - val_acc: 0.7125\n",
      "Epoch 21/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4916 - acc: 0.6914 - val_loss: 0.4674 - val_acc: 0.7135\n",
      "Epoch 22/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4892 - acc: 0.6923 - val_loss: 0.4675 - val_acc: 0.7135\n",
      "Epoch 23/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4874 - acc: 0.6934 - val_loss: 0.4677 - val_acc: 0.7135\n",
      "Epoch 24/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4856 - acc: 0.6943 - val_loss: 0.4681 - val_acc: 0.7135\n",
      "Epoch 25/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4842 - acc: 0.6951 - val_loss: 0.4680 - val_acc: 0.7141\n",
      "Epoch 26/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4829 - acc: 0.6952 - val_loss: 0.4686 - val_acc: 0.7140\n",
      "Epoch 27/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4816 - acc: 0.6961 - val_loss: 0.4684 - val_acc: 0.7144\n",
      "Epoch 28/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4810 - acc: 0.6968 - val_loss: 0.4688 - val_acc: 0.7148\n",
      "Epoch 29/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4798 - acc: 0.6971 - val_loss: 0.4693 - val_acc: 0.7149\n",
      "Epoch 30/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4790 - acc: 0.6976 - val_loss: 0.4691 - val_acc: 0.7145\n",
      "Epoch 31/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4784 - acc: 0.6975 - val_loss: 0.4695 - val_acc: 0.7148\n",
      "Epoch 32/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4777 - acc: 0.6980 - val_loss: 0.4694 - val_acc: 0.7147\n",
      "Epoch 33/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4774 - acc: 0.6981 - val_loss: 0.4701 - val_acc: 0.7148\n",
      "Epoch 34/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4766 - acc: 0.6986 - val_loss: 0.4702 - val_acc: 0.7149\n",
      "Epoch 35/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4763 - acc: 0.6986 - val_loss: 0.4705 - val_acc: 0.7152\n",
      "Epoch 36/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4759 - acc: 0.6985 - val_loss: 0.4703 - val_acc: 0.7148\n",
      "Epoch 37/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4750 - acc: 0.6991 - val_loss: 0.4705 - val_acc: 0.7152\n",
      "Epoch 38/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4743 - acc: 0.6989 - val_loss: 0.4714 - val_acc: 0.7150\n",
      "Epoch 39/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.6984 - val_loss: 0.4718 - val_acc: 0.7150\n",
      "Epoch 40/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4728 - acc: 0.6987 - val_loss: 0.4719 - val_acc: 0.7152\n",
      "Epoch 41/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4724 - acc: 0.6986 - val_loss: 0.4726 - val_acc: 0.7151\n",
      "Epoch 42/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4717 - acc: 0.6991 - val_loss: 0.4730 - val_acc: 0.7150\n",
      "Epoch 43/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4717 - acc: 0.6989 - val_loss: 0.4730 - val_acc: 0.7152\n",
      "Epoch 44/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4712 - acc: 0.6990 - val_loss: 0.4734 - val_acc: 0.7151\n",
      "Epoch 45/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6988 - val_loss: 0.4733 - val_acc: 0.7153\n",
      "Epoch 46/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4706 - acc: 0.6993 - val_loss: 0.4735 - val_acc: 0.7153\n",
      "Epoch 47/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4699 - acc: 0.6996 - val_loss: 0.4740 - val_acc: 0.7154\n",
      "Epoch 48/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4699 - acc: 0.6991 - val_loss: 0.4747 - val_acc: 0.7154\n",
      "Epoch 49/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.6998 - val_loss: 0.4748 - val_acc: 0.7154\n",
      "Epoch 50/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4694 - acc: 0.6992 - val_loss: 0.4749 - val_acc: 0.7153\n",
      "Epoch 51/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.6991 - val_loss: 0.4749 - val_acc: 0.7153\n",
      "Epoch 52/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.6997 - val_loss: 0.4752 - val_acc: 0.7155\n",
      "Epoch 53/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4685 - acc: 0.6998 - val_loss: 0.4755 - val_acc: 0.7155\n",
      "Epoch 54/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.6988 - val_loss: 0.4757 - val_acc: 0.7154\n",
      "Epoch 55/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7002 - val_loss: 0.4753 - val_acc: 0.7156\n",
      "Epoch 56/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4680 - acc: 0.7003 - val_loss: 0.4753 - val_acc: 0.7157\n",
      "Epoch 57/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4677 - acc: 0.7004 - val_loss: 0.4752 - val_acc: 0.7156\n",
      "Epoch 58/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4677 - acc: 0.7003 - val_loss: 0.4753 - val_acc: 0.7159\n",
      "Epoch 59/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4672 - acc: 0.7016 - val_loss: 0.4752 - val_acc: 0.7159\n",
      "Epoch 60/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7011 - val_loss: 0.4752 - val_acc: 0.7166\n",
      "Epoch 61/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7014 - val_loss: 0.4754 - val_acc: 0.7157\n",
      "Epoch 62/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4666 - acc: 0.7017 - val_loss: 0.4752 - val_acc: 0.7158\n",
      "Epoch 63/70\n",
      "1122420/1122420 [==============================] - 10s 8us/step - loss: 0.4666 - acc: 0.7011 - val_loss: 0.4757 - val_acc: 0.7157\n",
      "Epoch 64/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4665 - acc: 0.7011 - val_loss: 0.4754 - val_acc: 0.7157\n",
      "Epoch 65/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.7014 - val_loss: 0.4757 - val_acc: 0.7158\n",
      "Epoch 66/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4663 - acc: 0.7017 - val_loss: 0.4757 - val_acc: 0.7158\n",
      "Epoch 67/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4663 - acc: 0.7014 - val_loss: 0.4760 - val_acc: 0.7156\n",
      "Epoch 68/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4661 - acc: 0.7019 - val_loss: 0.4760 - val_acc: 0.7160\n",
      "Epoch 69/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4659 - acc: 0.7017 - val_loss: 0.4763 - val_acc: 0.7158\n",
      "Epoch 70/70\n",
      "1122420/1122420 [==============================] - 10s 9us/step - loss: 0.4657 - acc: 0.7022 - val_loss: 0.4762 - val_acc: 0.7157\n",
      "[[0.50361156]\n",
      " [0.50361156]\n",
      " [0.50361156]\n",
      " ...\n",
      " [0.2985408 ]\n",
      " [0.4375495 ]\n",
      " [0.55158246]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7856973995271868\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "19\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2497 MiB, count=158, average=15.8 MiB\n",
      "(1515180, 36, 6)\n",
      "(1515180, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1136100 samples, validate on 374760 samples\n",
      "Epoch 1/70\n",
      "1136100/1136100 [==============================] - 19s 17us/step - loss: 1.2251 - acc: 0.5114 - val_loss: 0.6676 - val_acc: 0.6080\n",
      "Epoch 2/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.7533 - acc: 0.5336 - val_loss: 0.6553 - val_acc: 0.6368\n",
      "Epoch 3/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.6922 - acc: 0.5635 - val_loss: 0.6305 - val_acc: 0.6551\n",
      "Epoch 4/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.6618 - acc: 0.5939 - val_loss: 0.5968 - val_acc: 0.6608\n",
      "Epoch 5/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.6325 - acc: 0.6153 - val_loss: 0.5576 - val_acc: 0.6632\n",
      "Epoch 6/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5999 - acc: 0.6287 - val_loss: 0.5214 - val_acc: 0.6693\n",
      "Epoch 7/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5700 - acc: 0.6400 - val_loss: 0.4959 - val_acc: 0.6722\n",
      "Epoch 8/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5456 - acc: 0.6498 - val_loss: 0.4826 - val_acc: 0.6786\n",
      "Epoch 9/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5297 - acc: 0.6581 - val_loss: 0.4759 - val_acc: 0.6884\n",
      "Epoch 10/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5203 - acc: 0.6655 - val_loss: 0.4725 - val_acc: 0.6979\n",
      "Epoch 11/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5145 - acc: 0.6702 - val_loss: 0.4704 - val_acc: 0.7055\n",
      "Epoch 12/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5099 - acc: 0.6737 - val_loss: 0.4687 - val_acc: 0.7076\n",
      "Epoch 13/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5067 - acc: 0.6766 - val_loss: 0.4673 - val_acc: 0.7074\n",
      "Epoch 14/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5032 - acc: 0.6796 - val_loss: 0.4658 - val_acc: 0.7079\n",
      "Epoch 15/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.5003 - acc: 0.6812 - val_loss: 0.4646 - val_acc: 0.7085\n",
      "Epoch 16/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4971 - acc: 0.6841 - val_loss: 0.4635 - val_acc: 0.7094\n",
      "Epoch 17/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4948 - acc: 0.6866 - val_loss: 0.4627 - val_acc: 0.7099\n",
      "Epoch 18/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4926 - acc: 0.6880 - val_loss: 0.4625 - val_acc: 0.7104\n",
      "Epoch 19/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4904 - acc: 0.6900 - val_loss: 0.4629 - val_acc: 0.7100\n",
      "Epoch 20/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4889 - acc: 0.6907 - val_loss: 0.4632 - val_acc: 0.7103\n",
      "Epoch 21/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4871 - acc: 0.6922 - val_loss: 0.4634 - val_acc: 0.7111\n",
      "Epoch 22/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4856 - acc: 0.6933 - val_loss: 0.4636 - val_acc: 0.7114\n",
      "Epoch 23/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4842 - acc: 0.6932 - val_loss: 0.4643 - val_acc: 0.7113\n",
      "Epoch 24/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4830 - acc: 0.6940 - val_loss: 0.4645 - val_acc: 0.7120\n",
      "Epoch 25/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4816 - acc: 0.6946 - val_loss: 0.4650 - val_acc: 0.7121\n",
      "Epoch 26/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4806 - acc: 0.6944 - val_loss: 0.4652 - val_acc: 0.7123\n",
      "Epoch 27/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4797 - acc: 0.6950 - val_loss: 0.4653 - val_acc: 0.7128\n",
      "Epoch 28/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4790 - acc: 0.6953 - val_loss: 0.4659 - val_acc: 0.7127\n",
      "Epoch 29/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4784 - acc: 0.6963 - val_loss: 0.4657 - val_acc: 0.7135\n",
      "Epoch 30/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4771 - acc: 0.6965 - val_loss: 0.4659 - val_acc: 0.7139\n",
      "Epoch 31/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4770 - acc: 0.6967 - val_loss: 0.4660 - val_acc: 0.7144\n",
      "Epoch 32/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4763 - acc: 0.6966 - val_loss: 0.4663 - val_acc: 0.7144\n",
      "Epoch 33/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4757 - acc: 0.6971 - val_loss: 0.4668 - val_acc: 0.7147\n",
      "Epoch 34/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4753 - acc: 0.6974 - val_loss: 0.4671 - val_acc: 0.7147\n",
      "Epoch 35/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4750 - acc: 0.6977 - val_loss: 0.4673 - val_acc: 0.7149\n",
      "Epoch 36/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4747 - acc: 0.6981 - val_loss: 0.4672 - val_acc: 0.7152\n",
      "Epoch 37/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4745 - acc: 0.6979 - val_loss: 0.4673 - val_acc: 0.7153\n",
      "Epoch 38/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4743 - acc: 0.6986 - val_loss: 0.4675 - val_acc: 0.7154\n",
      "Epoch 39/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4740 - acc: 0.6987 - val_loss: 0.4674 - val_acc: 0.7155\n",
      "Epoch 40/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.6991 - val_loss: 0.4676 - val_acc: 0.7158\n",
      "Epoch 41/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4735 - acc: 0.6986 - val_loss: 0.4679 - val_acc: 0.7159\n",
      "Epoch 42/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4733 - acc: 0.6992 - val_loss: 0.4680 - val_acc: 0.7159\n",
      "Epoch 43/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4729 - acc: 0.6993 - val_loss: 0.4682 - val_acc: 0.7159\n",
      "Epoch 44/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4729 - acc: 0.6995 - val_loss: 0.4681 - val_acc: 0.7162\n",
      "Epoch 45/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4730 - acc: 0.6993 - val_loss: 0.4683 - val_acc: 0.7160\n",
      "Epoch 46/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4724 - acc: 0.6996 - val_loss: 0.4683 - val_acc: 0.7162\n",
      "Epoch 47/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4722 - acc: 0.6999 - val_loss: 0.4685 - val_acc: 0.7162\n",
      "Epoch 48/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4721 - acc: 0.6996 - val_loss: 0.4687 - val_acc: 0.7161\n",
      "Epoch 49/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4721 - acc: 0.6998 - val_loss: 0.4688 - val_acc: 0.7162\n",
      "Epoch 50/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4719 - acc: 0.7001 - val_loss: 0.4692 - val_acc: 0.7162\n",
      "Epoch 51/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4717 - acc: 0.7004 - val_loss: 0.4692 - val_acc: 0.7167\n",
      "Epoch 52/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4716 - acc: 0.7005 - val_loss: 0.4691 - val_acc: 0.7164\n",
      "Epoch 53/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4713 - acc: 0.7005 - val_loss: 0.4697 - val_acc: 0.7160\n",
      "Epoch 54/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4714 - acc: 0.7010 - val_loss: 0.4698 - val_acc: 0.7166\n",
      "Epoch 55/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4705 - acc: 0.7009 - val_loss: 0.4696 - val_acc: 0.7170\n",
      "Epoch 56/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4702 - acc: 0.7010 - val_loss: 0.4699 - val_acc: 0.7167\n",
      "Epoch 57/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4698 - acc: 0.7009 - val_loss: 0.4704 - val_acc: 0.7169\n",
      "Epoch 58/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4696 - acc: 0.7010 - val_loss: 0.4705 - val_acc: 0.7166\n",
      "Epoch 59/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4695 - acc: 0.7013 - val_loss: 0.4704 - val_acc: 0.7165\n",
      "Epoch 60/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.7012 - val_loss: 0.4711 - val_acc: 0.7168\n",
      "Epoch 61/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4692 - acc: 0.7010 - val_loss: 0.4712 - val_acc: 0.7167\n",
      "Epoch 62/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4690 - acc: 0.7011 - val_loss: 0.4712 - val_acc: 0.7167\n",
      "Epoch 63/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4690 - acc: 0.7009 - val_loss: 0.4711 - val_acc: 0.7168\n",
      "Epoch 64/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.7010 - val_loss: 0.4712 - val_acc: 0.7163\n",
      "Epoch 65/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4684 - acc: 0.7012 - val_loss: 0.4716 - val_acc: 0.7164\n",
      "Epoch 66/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.7011 - val_loss: 0.4715 - val_acc: 0.7166\n",
      "Epoch 67/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.7013 - val_loss: 0.4718 - val_acc: 0.7166\n",
      "Epoch 68/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4684 - acc: 0.7012 - val_loss: 0.4720 - val_acc: 0.7162\n",
      "Epoch 69/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4686 - acc: 0.7012 - val_loss: 0.4718 - val_acc: 0.7162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/70\n",
      "1136100/1136100 [==============================] - 10s 9us/step - loss: 0.4684 - acc: 0.7013 - val_loss: 0.4719 - val_acc: 0.7165\n",
      "[[0.4905508 ]\n",
      " [0.4905508 ]\n",
      " [0.5416909 ]\n",
      " ...\n",
      " [0.30731702]\n",
      " [0.46409008]\n",
      " [0.5417673 ]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.786511367275056\n",
      "(148335, 1)\n",
      "finish dataread\n",
      "(None, 6, 1, 10)\n",
      "(None, 6, 1, 7)\n",
      "(None, 6, 1, 17)\n",
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "20\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2429 MiB, count=158, average=15.4 MiB\n",
      "(1473780, 36, 6)\n",
      "(1473780, 6, 36, 1)\n",
      "[Training model......]\n",
      "Train on 1117740 samples, validate on 351720 samples\n",
      "Epoch 1/70\n",
      "1117740/1117740 [==============================] - 20s 18us/step - loss: 1.5112 - acc: 0.5041 - val_loss: 0.7226 - val_acc: 0.5088\n",
      "Epoch 2/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.7960 - acc: 0.5133 - val_loss: 0.6906 - val_acc: 0.5397\n",
      "Epoch 3/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.7214 - acc: 0.5246 - val_loss: 0.6795 - val_acc: 0.5744\n",
      "Epoch 4/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.6937 - acc: 0.5489 - val_loss: 0.6601 - val_acc: 0.6240\n",
      "Epoch 5/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.6714 - acc: 0.5800 - val_loss: 0.6258 - val_acc: 0.6535\n",
      "Epoch 6/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.6415 - acc: 0.6099 - val_loss: 0.5821 - val_acc: 0.6633\n",
      "Epoch 7/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.6056 - acc: 0.6301 - val_loss: 0.5398 - val_acc: 0.6689\n",
      "Epoch 8/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5713 - acc: 0.6457 - val_loss: 0.5106 - val_acc: 0.6769\n",
      "Epoch 9/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5453 - acc: 0.6587 - val_loss: 0.4941 - val_acc: 0.6901\n",
      "Epoch 10/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5288 - acc: 0.6683 - val_loss: 0.4857 - val_acc: 0.6963\n",
      "Epoch 11/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5191 - acc: 0.6749 - val_loss: 0.4807 - val_acc: 0.7004\n",
      "Epoch 12/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5121 - acc: 0.6802 - val_loss: 0.4774 - val_acc: 0.7021\n",
      "Epoch 13/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5069 - acc: 0.6827 - val_loss: 0.4739 - val_acc: 0.7040\n",
      "Epoch 14/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.5013 - acc: 0.6867 - val_loss: 0.4707 - val_acc: 0.7045\n",
      "Epoch 15/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4961 - acc: 0.6890 - val_loss: 0.4687 - val_acc: 0.7079\n",
      "Epoch 16/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4919 - acc: 0.6915 - val_loss: 0.4672 - val_acc: 0.7102\n",
      "Epoch 17/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4892 - acc: 0.6920 - val_loss: 0.4670 - val_acc: 0.7102\n",
      "Epoch 18/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4863 - acc: 0.6931 - val_loss: 0.4673 - val_acc: 0.7111\n",
      "Epoch 19/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4844 - acc: 0.6935 - val_loss: 0.4676 - val_acc: 0.7115\n",
      "Epoch 20/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4826 - acc: 0.6940 - val_loss: 0.4685 - val_acc: 0.7110\n",
      "Epoch 21/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4812 - acc: 0.6945 - val_loss: 0.4689 - val_acc: 0.7115\n",
      "Epoch 22/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4802 - acc: 0.6953 - val_loss: 0.4700 - val_acc: 0.7118\n",
      "Epoch 23/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4788 - acc: 0.6959 - val_loss: 0.4707 - val_acc: 0.7120\n",
      "Epoch 24/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4779 - acc: 0.6965 - val_loss: 0.4715 - val_acc: 0.7118\n",
      "Epoch 25/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4770 - acc: 0.6970 - val_loss: 0.4717 - val_acc: 0.7124\n",
      "Epoch 26/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4764 - acc: 0.6971 - val_loss: 0.4725 - val_acc: 0.7129\n",
      "Epoch 27/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4758 - acc: 0.6971 - val_loss: 0.4731 - val_acc: 0.7123\n",
      "Epoch 28/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4749 - acc: 0.6980 - val_loss: 0.4738 - val_acc: 0.7127\n",
      "Epoch 29/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4751 - acc: 0.6979 - val_loss: 0.4736 - val_acc: 0.7132\n",
      "Epoch 30/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4744 - acc: 0.6981 - val_loss: 0.4742 - val_acc: 0.7132\n",
      "Epoch 31/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4742 - acc: 0.6978 - val_loss: 0.4746 - val_acc: 0.7131\n",
      "Epoch 32/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4736 - acc: 0.6982 - val_loss: 0.4750 - val_acc: 0.7136\n",
      "Epoch 33/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4731 - acc: 0.6985 - val_loss: 0.4754 - val_acc: 0.7121\n",
      "Epoch 34/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4731 - acc: 0.6989 - val_loss: 0.4758 - val_acc: 0.7120\n",
      "Epoch 35/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4726 - acc: 0.6988 - val_loss: 0.4756 - val_acc: 0.7130\n",
      "Epoch 36/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4720 - acc: 0.6991 - val_loss: 0.4764 - val_acc: 0.7099\n",
      "Epoch 37/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4717 - acc: 0.6992 - val_loss: 0.4764 - val_acc: 0.7096\n",
      "Epoch 38/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4716 - acc: 0.6998 - val_loss: 0.4766 - val_acc: 0.7097\n",
      "Epoch 39/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4716 - acc: 0.6996 - val_loss: 0.4762 - val_acc: 0.7097\n",
      "Epoch 40/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4710 - acc: 0.7002 - val_loss: 0.4764 - val_acc: 0.7100\n",
      "Epoch 41/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4709 - acc: 0.7001 - val_loss: 0.4766 - val_acc: 0.7095\n",
      "Epoch 42/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4708 - acc: 0.6999 - val_loss: 0.4771 - val_acc: 0.7095\n",
      "Epoch 43/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4705 - acc: 0.7000 - val_loss: 0.4766 - val_acc: 0.7094\n",
      "Epoch 44/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4701 - acc: 0.7005 - val_loss: 0.4767 - val_acc: 0.7099\n",
      "Epoch 45/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4699 - acc: 0.7006 - val_loss: 0.4770 - val_acc: 0.7094\n",
      "Epoch 46/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4699 - acc: 0.7002 - val_loss: 0.4772 - val_acc: 0.7094\n",
      "Epoch 47/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4695 - acc: 0.7005 - val_loss: 0.4771 - val_acc: 0.7093\n",
      "Epoch 48/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4693 - acc: 0.7008 - val_loss: 0.4778 - val_acc: 0.7093\n",
      "Epoch 49/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4693 - acc: 0.7004 - val_loss: 0.4781 - val_acc: 0.7091\n",
      "Epoch 50/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4687 - acc: 0.7010 - val_loss: 0.4779 - val_acc: 0.7076\n",
      "Epoch 51/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4683 - acc: 0.7007 - val_loss: 0.4780 - val_acc: 0.7068\n",
      "Epoch 52/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4683 - acc: 0.7011 - val_loss: 0.4787 - val_acc: 0.7065\n",
      "Epoch 53/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4679 - acc: 0.7013 - val_loss: 0.4783 - val_acc: 0.7070\n",
      "Epoch 54/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4681 - acc: 0.7013 - val_loss: 0.4783 - val_acc: 0.7068\n",
      "Epoch 55/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4680 - acc: 0.7015 - val_loss: 0.4785 - val_acc: 0.7070\n",
      "Epoch 56/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4676 - acc: 0.7014 - val_loss: 0.4783 - val_acc: 0.7069\n",
      "Epoch 57/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4674 - acc: 0.7019 - val_loss: 0.4789 - val_acc: 0.7065\n",
      "Epoch 58/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4675 - acc: 0.7015 - val_loss: 0.4786 - val_acc: 0.7068\n",
      "Epoch 59/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4673 - acc: 0.7016 - val_loss: 0.4786 - val_acc: 0.7068\n",
      "Epoch 60/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4671 - acc: 0.7015 - val_loss: 0.4789 - val_acc: 0.7067\n",
      "Epoch 61/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4670 - acc: 0.7020 - val_loss: 0.4789 - val_acc: 0.7066\n",
      "Epoch 62/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7023 - val_loss: 0.4790 - val_acc: 0.7068\n",
      "Epoch 63/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4672 - acc: 0.7015 - val_loss: 0.4788 - val_acc: 0.7068\n",
      "Epoch 64/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4669 - acc: 0.7026 - val_loss: 0.4789 - val_acc: 0.7068\n",
      "Epoch 65/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4668 - acc: 0.7027 - val_loss: 0.4787 - val_acc: 0.7073\n",
      "Epoch 66/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4665 - acc: 0.7025 - val_loss: 0.4787 - val_acc: 0.7069\n",
      "Epoch 67/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4667 - acc: 0.7022 - val_loss: 0.4787 - val_acc: 0.7102\n",
      "Epoch 68/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.7025 - val_loss: 0.4787 - val_acc: 0.7106\n",
      "Epoch 69/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4663 - acc: 0.7026 - val_loss: 0.4790 - val_acc: 0.7105\n",
      "Epoch 70/70\n",
      "1117740/1117740 [==============================] - 10s 9us/step - loss: 0.4664 - acc: 0.7029 - val_loss: 0.4785 - val_acc: 0.7109\n",
      "[[0.5037581 ]\n",
      " [0.50422263]\n",
      " [0.50422263]\n",
      " ...\n",
      " [0.3388986 ]\n",
      " [0.45761135]\n",
      " [0.55447036]]\n",
      "猜答案多的那邊 benchacc1:\n",
      "0.5\n",
      "參考前一個答案 benchacc2:\n",
      "0.7847662913681337\n"
     ]
    }
   ],
   "source": [
    "for z in range(head,tail,1):\n",
    "    \"\"\"\n",
    "     V\n",
    "    \"\"\"\n",
    "    n=daynum[tail]-daynum[head]\n",
    "    df = pd.read_csv('data/JPY_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    print(df.shape)  \n",
    "    jpy5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        jpy5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/EUR_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eur5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eur5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/AUD_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    aud5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        aud5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/btc_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    btc5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        btc5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/eth_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    eth5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        eth5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    df = pd.read_csv('data/DASH_data_ver2.0.csv',header=None)  # 讀取訓練數據\n",
    "    dash5months = np.zeros(n)\n",
    "    dt=0\n",
    "    for i in range(daynum[head],daynum[tail],1):\n",
    "        dash5months[dt] = df[0][i]  \n",
    "        dt=dt+1\n",
    "    print('finish dataread')\n",
    "    Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "    for p in range(n-l+1):\n",
    "        Train_data[p,0,:]=btc5months[p:p+l]\n",
    "        Train_data[p,1,:]=dash5months[p:p+l]\n",
    "        Train_data[p,2,:]=eth5months[p:p+l]\n",
    "        Train_data[p,3,:]=jpy5months[p:p+l]\n",
    "        Train_data[p,4,:]=eur5months[p:p+l]\n",
    "        Train_data[p,5,:]=aud5months[p:p+l]\n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((currencynum,l,1))\n",
    "    visual_conv = ConvolutionNetworks([20,10],[(1,kn),(1,kn)])(visual_scene)\n",
    "    print(K.int_shape(visual_conv))\n",
    "    tag = build_tag(visual_conv)\n",
    "    visual_conv = Concatenate()([tag, visual_conv])\n",
    "    print(K.int_shape(visual_conv))\n",
    "    \n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    f = shapes[2]\n",
    "    features= []\n",
    "    #features = np.zeros(0)\n",
    "    for k1 in range(w):\n",
    "        for k2 in range(f):\n",
    "            def get_feature(t):\n",
    "                return t[:, k1, k2, :]\n",
    "            #get_feature_layer = Lambda(get_feature)\n",
    "            features.append(Lambda(get_feature)(visual_conv))\n",
    "    \n",
    "      \n",
    "    input2 = Input((16,))\n",
    "    onehot_encode_question = input2\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode_question]))    \n",
    "    \n",
    "     \n",
    "    g_MLP = get_MLP(4, get_dense(4,MLP_unit))\n",
    "    \n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "    \n",
    "    combined_relation = Add()(mid_relations)\n",
    "    \n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "    \n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "    \n",
    "    \n",
    "    #model = Model(inputs=[visual_scene])\n",
    "    model = Model(inputs=[visual_scene, input2, tag], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "    #model.summary()\n",
    "    print(z)\n",
    "    fit_show(Train_data,daynum[z],daynum[z+3],daynum[z+4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
