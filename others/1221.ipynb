{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.65\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Flatten, Convolution1D, MaxPooling1D, Activation, BatchNormalization,\\\n",
    "Lambda, Concatenate, Add, Conv2D, Conv1D,TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "基本設定\n",
    "\"\"\"\n",
    "n=210527 #用2016 1~3月的資料 288x(31+29+31+30+31+30+...)=52416 210528-1(all) 差值在減一  \n",
    "l=36 #36個一組 (3小時一組)\n",
    "currencynum=5\n",
    "currency = [\"BTC\",\"DASH\",\"ETH\",\"LTC\",\"XMR\"]\n",
    "month = [0,31,60,91,121,152,182,213,244,274,305,335,366,397,425,456,486,517,547,578,609,639,670,700,731] #2016是閏年 366天\n",
    "#month = [0,10,20,30,40,50]\n",
    "month2 = [397,425,456,486,517,547,578,609,639,670,700,731] #2017 365天\n",
    "#currency = [\"BTC\",\"DASH\",\"ETH\",\"LTC\",\"JBY\",\"GBP\",\"EUR\",\"AUD\",\"US\"]\n",
    "question = [\"trand\",\"volatility\"]\n",
    "M=0\n",
    "\n",
    "# 控制要不要存檔，1 : Yes, 0 : No\n",
    "save = 1\n",
    "epochs = 400\n",
    "batch_size = 4096\n",
    "\n",
    "all_cur_pair = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "if(M==0):\n",
    "    all_cur_pair_P = list(combinations(currency,2))# (Cn取2) 問題沒有先後順序時使用\n",
    "elif(M==1):\n",
    "    all_cur_pair_P = list(permutations(currency,2))# (Pn取2) 問題有先後順序時使用  \n",
    "all_question= list(permutations(question,1))\n",
    "np.set_printoptions(suppress=True)#不要用科學符號輸出\n",
    "btc = 423.51\n",
    "DASH = 0.00778 * btc\n",
    "ETH = 0.0021652 * btc\n",
    "LTC = 0.00805792 * btc\n",
    "XMR = 0.00105001 * btc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210527, 1)\n",
      "(210527, 1)\n",
      "(210527, 1)\n",
      "(210527, 1)\n",
      "(210527, 1)\n",
      "finish dataread\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    " V\n",
    "\"\"\"\n",
    "df = pd.read_csv('btc_data_return_diff.csv',header=None)  # 讀取訓練數據\n",
    "print(df.shape)  \n",
    "btc5months = np.zeros(n)\n",
    "for i in range(n):\n",
    "    btc5months[i] = df[0][i]\n",
    "df = pd.read_csv('DASH_data_return_diff.csv',header=None)  # 讀取訓練數據\n",
    "print(df.shape)  \n",
    "dash5months = np.zeros(n)\n",
    "for i in range(n):\n",
    "    dash5months[i] = df[0][i]\n",
    "df = pd.read_csv('ETH_data_return_diff.csv',header=None)  # 讀取訓練數據\n",
    "print(df.shape)  \n",
    "eth5months = np.zeros(n)\n",
    "for i in range(n):\n",
    "    eth5months[i] = df[0][i]\n",
    "df = pd.read_csv('LTC_data_return_diff.csv',header=None)  # 讀取訓練數據\n",
    "print(df.shape)  \n",
    "ltc5months = np.zeros(n)\n",
    "for i in range(n):\n",
    "    ltc5months[i] = df[0][i]\n",
    "df = pd.read_csv('XMR_data_return_diff.csv',header=None)  # 讀取訓練數據\n",
    "print(df.shape)  \n",
    "xmr5months = np.zeros(n)\n",
    "for i in range(n):\n",
    "    xmr5months[i] = df[0][i]    \n",
    "\n",
    "print('finish dataread')\n",
    "Train_data=np.zeros(((n-l+1,len(currency),l)))\n",
    "for p in range(n-l+1):\n",
    "    Train_data[p,0,:]=btc5months[p:p+l]\n",
    "    Train_data[p,1,:]=dash5months[p:p+l]\n",
    "    Train_data[p,2,:]=eth5months[p:p+l]\n",
    "    Train_data[p,3,:]=ltc5months[p:p+l]\n",
    "    Train_data[p,4,:]=xmr5months[p:p+l]\n",
    "    \n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "str, onehotcode, company code轉換\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\n",
    "currencylist = {}\n",
    "questionlist = {}\n",
    "for i in range(len(currency)):\n",
    "    currencylist[i] = currency[i]\n",
    "\n",
    "for i in range(len(question)):\n",
    "    questionlist[i] = question[i]\n",
    "\n",
    "def str_to_currency(cur):\n",
    "    return {v: k for k, v in currencylist.items()}[cur]\n",
    "\n",
    "def str_to_question(q):\n",
    "    return {v: k for k, v in questionlist.items()}[q]\n",
    "\n",
    "\n",
    "def one_hot_currency(currencylist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(currencylist)))\n",
    "    for i in range(len(currencylist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "def one_hot_question(questionlist):\n",
    "    d = {}\n",
    "    temp = np.eye((len(questionlist)))\n",
    "    for i in range(len(questionlist)):\n",
    "        d[i] = temp[i]\n",
    "    return d\n",
    "\n",
    "onehotcurrency = one_hot_currency(currencylist)\n",
    "onehotquestion = one_hot_question(questionlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Q&A\n",
    "\"\"\"\n",
    "vol='volatility'\n",
    "def set_question(com1, com2, condition,typeq):\n",
    "    # set condition\n",
    "    def rise_of_fall():\n",
    "        #目前問題設定只有三種設定：前者大(big)-0、後者大(small)-1、一樣(s)-2\n",
    "        tmp = np.eye((3))\n",
    "        d = {0:tmp[0],1:tmp[1],2:tmp[2]}\n",
    "        return d\n",
    "    # 拼接問題\n",
    "    return np.concatenate((onehotquestion[str_to_question(typeq)],onehotcurrency[str_to_currency(com1)], onehotcurrency[str_to_currency(com2)], rise_of_fall()[condition]))\n",
    "\n",
    "qtype = ['big','small','same']\n",
    "HVqtype = ['big','small','same']\n",
    "\n",
    "\n",
    "\"\"\"比漲幅程度類問題\"\"\"\n",
    "def set_question_and_answer_pair(data, data2, n, all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_pair = {}\n",
    "    a_pair = {}\n",
    "    outcome=np.zeros((5,1))\n",
    "    #data_sum=np.sum(data,axis=1)\n",
    "    data2_sum=np.sum(data2,axis=1)\n",
    "    outcome[0]=(data2_sum[0])/(btc + np.sum(btc5months[:n]))\n",
    "    outcome[1]=(data2_sum[1])/(DASH + np.sum(dash5months[:n]))\n",
    "    outcome[2]=(data2_sum[2])/(ETH + np.sum(eth5months[:n]))\n",
    "    outcome[3]=(data2_sum[3])/(LTC + np.sum(ltc5months[:n]))\n",
    "    outcome[4]=(data2_sum[4])/(XMR + np.sum(xmr5months[:n]))\n",
    "\n",
    "    # question_type (目前有3種小問題：前者大、後者大或者一樣)\n",
    "    for i in range(3):\n",
    "        tmp_q = []\n",
    "        tmp_a = []\n",
    "            \n",
    "        #先塞question pairs\n",
    "        for j in range(len(all_cur_pair_P)):\n",
    "            tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1], i, all_question[0][0]))\n",
    "        q_pair[i] = tmp_q\n",
    "        \n",
    "        #再塞answer pairs\n",
    "        if(i == 0): #同漲\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] > outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "        elif(i == 1):\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] < outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "        else:\n",
    "             for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] == outcome[str_to_currency(all_cur_pair_P[j][1])] == 0):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)           \n",
    "        a_pair[i] = tmp_a\n",
    "        \n",
    "    return (data, q_pair, a_pair)\n",
    "\n",
    "\n",
    "\"\"\"History Volatility類問題\"\"\"\n",
    "def set_HVquestion_and_HVanswer_pair(data, data2,all_cur_pair_P): #比較波動率大小的問題\n",
    "    q_HVpair = {}\n",
    "    a_HVpair = {}\n",
    "    outcome=np.zeros((5,1))\n",
    "    #data_std=np.std(data,axis=1)\n",
    "    data2_std=np.std(data2,axis=1)    \n",
    "    outcome[0]=data2_std[0]\n",
    "    outcome[1]=data2_std[1]\n",
    "    outcome[2]=data2_std[2]\n",
    "    outcome[3]=data2_std[3]\n",
    "    outcome[4]=data2_std[4]\n",
    "     \n",
    "    # question_type (目前有3種小問題：前者大、後者大或者一樣)\n",
    "    for i in range(3):\n",
    "        tmp_q = []\n",
    "        tmp_a = []\n",
    "        \n",
    "        #先塞question pairs\n",
    "        for j in range(len(all_cur_pair_P)):\n",
    "            tmp_q.append(set_question(all_cur_pair_P[j][0], all_cur_pair_P[j][1], i,all_question[1][0]))\n",
    "        q_HVpair[i] = tmp_q\n",
    "        \n",
    "        #再塞answer pairs\n",
    "        if(i == 0): #前者大\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] > outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "        elif(i == 1):\n",
    "            for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] < outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)\n",
    "        else:\n",
    "             for j in range(len(all_cur_pair_P)):\n",
    "                if(outcome[str_to_currency(all_cur_pair_P[j][0])] == outcome[str_to_currency(all_cur_pair_P[j][1])]):\n",
    "                    tmp_a.append(1)\n",
    "                else:\n",
    "                    tmp_a.append(0)           \n",
    "        a_HVpair[i] = tmp_a\n",
    "        \n",
    "    return (data, q_HVpair, a_HVpair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "網路函數\n",
    "\"\"\"\n",
    "def ConvolutionNetworks(filter_num,kernel_size):\n",
    "    def conv(model):\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = Conv1D(filter_num, kernel_size, activation='relu')(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv\n",
    "\n",
    "def get_dense(n, MLP_unit):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        r.append(Dense(MLP_unit, activation='relu'))\n",
    "    return r\n",
    "\n",
    "def get_MLP(n, denses):\n",
    "    def g(x):\n",
    "        d = x\n",
    "        for k in range(n):\n",
    "            d = denses[k](d)\n",
    "        return d\n",
    "    return g\n",
    "\n",
    "def dropout_dense(x,MLP_unit):\n",
    "    y = Dense(MLP_unit)(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "此處開始寫rolling\n",
    "\"\"\"\n",
    "\n",
    "def fit_show(traindata,m1,m2,m3):\n",
    "    \n",
    "    vqa_pair = []\n",
    "    for i in range(m1*288,m3*288-l-l,1):\n",
    "        vqa_pair.append(set_question_and_answer_pair(Train_data[i],Train_data[i+l],i+l-1,all_cur_pair_P))  \n",
    "        vqa_pair.append(set_HVquestion_and_HVanswer_pair(Train_data[i],Train_data[i+l],all_cur_pair_P))\n",
    "        #這裡交叉塞資料才不會不小心預測未來\n",
    "            \n",
    "    # 處理訓練資料\n",
    "    v_tmp, q_tmp, a_tmp = [],[],[]\n",
    "    for i in range(len(vqa_pair)):\n",
    "        v_tmp.append(vqa_pair[i][0])\n",
    "        q_tmp.append(vqa_pair[i][1])\n",
    "        a_tmp.append(vqa_pair[i][2])\n",
    "    v = np.array(v_tmp)\n",
    "    \n",
    "    # 下面這行會導致memory error\n",
    "    import tracemalloc\n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    v = np.repeat(v, len(all_cur_pair_P)*len(qtype), axis = 0)\n",
    "    #這裡要注意問題的分類數量(2種:波動和漲跌)已經在前面vqa_pair變成兩倍過了\n",
    "    #這裡只要考慮貨幣組合數量(P的N取2)和問的問題數量(高低同)\n",
    "    snap = tracemalloc.take_snapshot()\n",
    "    # Evaluate result\n",
    "    stats = snap.statistics('lineno')\n",
    "    for stat in stats[:1]:\n",
    "        print(stat)\n",
    "    \n",
    "    q, a = [],[]\n",
    "    for i in range(len(q_tmp)):\n",
    "        for value in q_tmp[i].values():\n",
    "            q.append(value)\n",
    "    q = np.vstack(q)\n",
    "    q = np.vstack(q)\n",
    "    \n",
    "    for i in range(len(a_tmp)):\n",
    "        for value in a_tmp[i].values():\n",
    "            a.append(value)\n",
    "    a = np.vstack(a)\n",
    "    a = a.reshape(a.shape[0]*a.shape[1])\n",
    "    v=np.swapaxes(v,1,2)\n",
    "    print(\"[Training model......]\")\n",
    "\n",
    "    Train_v=v[:(m2*288-72)*len(all_cur_pair_P)*len(qtype)*2]# -1-72+1=-72\n",
    "    Train_q=q[:(m2*288-72)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Train_a=a[:(m2*288-72)*len(all_cur_pair_P)*len(qtype)*2]\n",
    "    Test_v=v[(m2*288)*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_q=q[(m2*288)*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    Test_a=a[(m2*288)*len(all_cur_pair_P)*len(qtype)*2:]\n",
    "    history = model.fit([Train_v, Train_q], Train_a, validation_data=([Test_v,Test_q],Test_a),batch_size=batch_size ,epochs = epochs,shuffle=False)\n",
    "    \n",
    "    \"\"\"\n",
    "    算benchmark\n",
    "    \"\"\"\n",
    "    benchacc=0\n",
    "    for i in range(Test_a.shape[0]):\n",
    "        benchacc=benchacc+Test_a[i]\n",
    "    benchacc=benchacc/Test_a.shape[0]    \n",
    "    if(benchacc<0.5):\n",
    "        benchacc=1-benchacc\n",
    "    print(\"benchacc:\")\n",
    "    print(benchacc)\n",
    "    \n",
    "    \"\"\"\n",
    "    畫圖\n",
    "    \"\"\"\n",
    "    benchfunction=np.ones(a.shape[0])\n",
    "    benchfunction=benchfunction*benchacc\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"loss\") \n",
    "    #plt.title(\"The Title\") \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    plt.plot(history.history['val_acc'], label='test')\n",
    "    plt.plot(np.arange(epochs),np.repeat(benchacc,epochs), label='benchmark')\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"acc\") \n",
    "    #plt.title(\"The Title\") \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "0\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2865 MiB, count=2, average=1433 MiB\n",
      "[Training model......]\n",
      "Train on 1568160 samples, validate on 514080 samples\n",
      "Epoch 1/400\n",
      "1568160/1568160 [==============================] - 602s 384us/step - loss: 5.8570 - acc: 0.6031 - val_loss: 2.4209 - val_acc: 0.7657\n",
      "Epoch 2/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 1.5241 - acc: 0.7395 - val_loss: 0.3482 - val_acc: 0.8022\n",
      "Epoch 3/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.4441 - acc: 0.7618 - val_loss: 0.3136 - val_acc: 0.8034\n",
      "Epoch 4/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3673 - acc: 0.7771 - val_loss: 0.2955 - val_acc: 0.8049\n",
      "Epoch 5/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3434 - acc: 0.7839 - val_loss: 0.2887 - val_acc: 0.8050\n",
      "Epoch 6/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3313 - acc: 0.7895 - val_loss: 0.2847 - val_acc: 0.8052\n",
      "Epoch 7/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3227 - acc: 0.7935 - val_loss: 0.2828 - val_acc: 0.8108\n",
      "Epoch 8/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3179 - acc: 0.7947 - val_loss: 0.2818 - val_acc: 0.8108\n",
      "Epoch 9/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3150 - acc: 0.7966 - val_loss: 0.2813 - val_acc: 0.8106\n",
      "Epoch 10/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.3134 - acc: 0.7976 - val_loss: 0.2812 - val_acc: 0.8102\n",
      "Epoch 11/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3117 - acc: 0.7987 - val_loss: 0.2812 - val_acc: 0.8099\n",
      "Epoch 12/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3104 - acc: 0.8004 - val_loss: 0.2813 - val_acc: 0.8092\n",
      "Epoch 13/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3097 - acc: 0.8008 - val_loss: 0.2813 - val_acc: 0.8092\n",
      "Epoch 14/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3089 - acc: 0.8007 - val_loss: 0.2812 - val_acc: 0.8105\n",
      "Epoch 15/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3078 - acc: 0.8007 - val_loss: 0.2812 - val_acc: 0.8107\n",
      "Epoch 16/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3077 - acc: 0.7999 - val_loss: 0.2813 - val_acc: 0.8115\n",
      "Epoch 17/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3073 - acc: 0.7988 - val_loss: 0.2811 - val_acc: 0.8114\n",
      "Epoch 18/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3069 - acc: 0.7990 - val_loss: 0.2817 - val_acc: 0.8101\n",
      "Epoch 19/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3063 - acc: 0.7995 - val_loss: 0.2820 - val_acc: 0.8064\n",
      "Epoch 20/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3057 - acc: 0.7999 - val_loss: 0.2822 - val_acc: 0.8058\n",
      "Epoch 21/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3058 - acc: 0.7996 - val_loss: 0.2821 - val_acc: 0.8058\n",
      "Epoch 22/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3058 - acc: 0.7990 - val_loss: 0.2820 - val_acc: 0.8059\n",
      "Epoch 23/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3052 - acc: 0.8003 - val_loss: 0.2821 - val_acc: 0.8059\n",
      "Epoch 24/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3055 - acc: 0.7985 - val_loss: 0.2820 - val_acc: 0.8065\n",
      "Epoch 25/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3051 - acc: 0.7997 - val_loss: 0.2819 - val_acc: 0.8059\n",
      "Epoch 26/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3047 - acc: 0.7995 - val_loss: 0.2818 - val_acc: 0.8052\n",
      "Epoch 27/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3038 - acc: 0.8006 - val_loss: 0.2816 - val_acc: 0.8054\n",
      "Epoch 28/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3040 - acc: 0.7996 - val_loss: 0.2817 - val_acc: 0.8058\n",
      "Epoch 29/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3030 - acc: 0.8017 - val_loss: 0.2818 - val_acc: 0.8060\n",
      "Epoch 30/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3031 - acc: 0.8005 - val_loss: 0.2822 - val_acc: 0.8057\n",
      "Epoch 31/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3028 - acc: 0.8023 - val_loss: 0.2823 - val_acc: 0.8052\n",
      "Epoch 32/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3028 - acc: 0.8033 - val_loss: 0.2814 - val_acc: 0.8102\n",
      "Epoch 33/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3018 - acc: 0.8047 - val_loss: 0.2813 - val_acc: 0.8111\n",
      "Epoch 34/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3016 - acc: 0.8039 - val_loss: 0.2818 - val_acc: 0.8112\n",
      "Epoch 35/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3007 - acc: 0.8072 - val_loss: 0.2822 - val_acc: 0.8113\n",
      "Epoch 36/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3020 - acc: 0.8026 - val_loss: 0.2822 - val_acc: 0.8110\n",
      "Epoch 37/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3021 - acc: 0.8027 - val_loss: 0.2830 - val_acc: 0.8105\n",
      "Epoch 38/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3021 - acc: 0.8016 - val_loss: 0.2844 - val_acc: 0.8056\n",
      "Epoch 39/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3016 - acc: 0.8010 - val_loss: 0.2838 - val_acc: 0.8054\n",
      "Epoch 40/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3015 - acc: 0.8010 - val_loss: 0.2840 - val_acc: 0.8053\n",
      "Epoch 41/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3014 - acc: 0.8019 - val_loss: 0.2843 - val_acc: 0.8061\n",
      "Epoch 42/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3012 - acc: 0.8027 - val_loss: 0.2846 - val_acc: 0.8055\n",
      "Epoch 43/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3007 - acc: 0.8029 - val_loss: 0.2845 - val_acc: 0.8055\n",
      "Epoch 44/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3005 - acc: 0.8024 - val_loss: 0.2843 - val_acc: 0.8056\n",
      "Epoch 45/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3007 - acc: 0.8029 - val_loss: 0.2848 - val_acc: 0.8052\n",
      "Epoch 46/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3009 - acc: 0.8028 - val_loss: 0.2853 - val_acc: 0.8046\n",
      "Epoch 47/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3009 - acc: 0.8028 - val_loss: 0.2858 - val_acc: 0.8049\n",
      "Epoch 48/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3004 - acc: 0.8031 - val_loss: 0.2863 - val_acc: 0.8054\n",
      "Epoch 49/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.3006 - acc: 0.8028 - val_loss: 0.2866 - val_acc: 0.8053\n",
      "Epoch 50/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.2997 - acc: 0.8024 - val_loss: 0.2851 - val_acc: 0.8056\n",
      "Epoch 51/400\n",
      "1568160/1568160 [==============================] - 124s 79us/step - loss: 0.2991 - acc: 0.8040 - val_loss: 0.2872 - val_acc: 0.8046\n",
      "Epoch 52/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3004 - acc: 0.8011 - val_loss: 0.2872 - val_acc: 0.8054\n",
      "Epoch 53/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3003 - acc: 0.8016 - val_loss: 0.2874 - val_acc: 0.8055\n",
      "Epoch 54/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2997 - acc: 0.8016 - val_loss: 0.2869 - val_acc: 0.8055\n",
      "Epoch 55/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.2995 - acc: 0.8026 - val_loss: 0.2868 - val_acc: 0.8054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.3000 - acc: 0.8023 - val_loss: 0.2865 - val_acc: 0.8055\n",
      "Epoch 57/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2992 - acc: 0.8027 - val_loss: 0.2845 - val_acc: 0.8058\n",
      "Epoch 58/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.2973 - acc: 0.8034 - val_loss: 0.2856 - val_acc: 0.8055\n",
      "Epoch 59/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2982 - acc: 0.8045 - val_loss: 0.2859 - val_acc: 0.8112\n",
      "Epoch 60/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2989 - acc: 0.8041 - val_loss: 0.2851 - val_acc: 0.8112\n",
      "Epoch 61/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.2982 - acc: 0.8030 - val_loss: 0.2860 - val_acc: 0.8055\n",
      "Epoch 62/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2984 - acc: 0.8042 - val_loss: 0.2862 - val_acc: 0.8108\n",
      "Epoch 63/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2980 - acc: 0.8032 - val_loss: 0.2847 - val_acc: 0.8112\n",
      "Epoch 64/400\n",
      "1568160/1568160 [==============================] - 123s 79us/step - loss: 0.2969 - acc: 0.8032 - val_loss: 0.2864 - val_acc: 0.8056\n",
      "Epoch 65/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2986 - acc: 0.8021 - val_loss: 0.2872 - val_acc: 0.8057\n",
      "Epoch 66/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2979 - acc: 0.8021 - val_loss: 0.2883 - val_acc: 0.8055\n",
      "Epoch 67/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2964 - acc: 0.8052 - val_loss: 0.2848 - val_acc: 0.8104\n",
      "Epoch 68/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2967 - acc: 0.8036 - val_loss: 0.2867 - val_acc: 0.8058\n",
      "Epoch 69/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2970 - acc: 0.8045 - val_loss: 0.2858 - val_acc: 0.8112\n",
      "Epoch 70/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2976 - acc: 0.8027 - val_loss: 0.2864 - val_acc: 0.8056\n",
      "Epoch 71/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2962 - acc: 0.8040 - val_loss: 0.2859 - val_acc: 0.8057\n",
      "Epoch 72/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2958 - acc: 0.8045 - val_loss: 0.2842 - val_acc: 0.8106\n",
      "Epoch 73/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2962 - acc: 0.8043 - val_loss: 0.2839 - val_acc: 0.8096\n",
      "Epoch 74/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2964 - acc: 0.8034 - val_loss: 0.2875 - val_acc: 0.8054\n",
      "Epoch 75/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2959 - acc: 0.8030 - val_loss: 0.2865 - val_acc: 0.8059\n",
      "Epoch 76/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2956 - acc: 0.8040 - val_loss: 0.2838 - val_acc: 0.8117\n",
      "Epoch 77/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2954 - acc: 0.8034 - val_loss: 0.2867 - val_acc: 0.8101\n",
      "Epoch 78/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2953 - acc: 0.8037 - val_loss: 0.2864 - val_acc: 0.8058\n",
      "Epoch 79/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2950 - acc: 0.8053 - val_loss: 0.2841 - val_acc: 0.8098\n",
      "Epoch 80/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2955 - acc: 0.8033 - val_loss: 0.2856 - val_acc: 0.8059\n",
      "Epoch 81/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2947 - acc: 0.8050 - val_loss: 0.2831 - val_acc: 0.8096\n",
      "Epoch 82/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2947 - acc: 0.8038 - val_loss: 0.2842 - val_acc: 0.8057\n",
      "Epoch 83/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2950 - acc: 0.8044 - val_loss: 0.2858 - val_acc: 0.8067\n",
      "Epoch 84/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2943 - acc: 0.8033 - val_loss: 0.2859 - val_acc: 0.8111\n",
      "Epoch 85/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2937 - acc: 0.8047 - val_loss: 0.2838 - val_acc: 0.8114\n",
      "Epoch 86/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2939 - acc: 0.8043 - val_loss: 0.2855 - val_acc: 0.8099\n",
      "Epoch 87/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2937 - acc: 0.8040 - val_loss: 0.2876 - val_acc: 0.8103\n",
      "Epoch 88/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2929 - acc: 0.8054 - val_loss: 0.2870 - val_acc: 0.8108\n",
      "Epoch 89/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2942 - acc: 0.8050 - val_loss: 0.2884 - val_acc: 0.8101\n",
      "Epoch 90/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2929 - acc: 0.8039 - val_loss: 0.2848 - val_acc: 0.8116\n",
      "Epoch 91/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2930 - acc: 0.8054 - val_loss: 0.2869 - val_acc: 0.8105\n",
      "Epoch 92/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2930 - acc: 0.8043 - val_loss: 0.2868 - val_acc: 0.8110\n",
      "Epoch 93/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2931 - acc: 0.8048 - val_loss: 0.2850 - val_acc: 0.8105\n",
      "Epoch 94/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2918 - acc: 0.8056 - val_loss: 0.2861 - val_acc: 0.8113\n",
      "Epoch 95/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2936 - acc: 0.8048 - val_loss: 0.2839 - val_acc: 0.8113\n",
      "Epoch 96/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2919 - acc: 0.8054 - val_loss: 0.2855 - val_acc: 0.8107\n",
      "Epoch 97/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2920 - acc: 0.8065 - val_loss: 0.2874 - val_acc: 0.8113\n",
      "Epoch 98/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2919 - acc: 0.8064 - val_loss: 0.2855 - val_acc: 0.8093\n",
      "Epoch 99/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2928 - acc: 0.8045 - val_loss: 0.2836 - val_acc: 0.8096\n",
      "Epoch 100/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2915 - acc: 0.8048 - val_loss: 0.2858 - val_acc: 0.8113\n",
      "Epoch 101/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2914 - acc: 0.8051 - val_loss: 0.2845 - val_acc: 0.8096\n",
      "Epoch 102/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2910 - acc: 0.8060 - val_loss: 0.2844 - val_acc: 0.8097\n",
      "Epoch 103/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2909 - acc: 0.8062 - val_loss: 0.2846 - val_acc: 0.8097\n",
      "Epoch 104/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2917 - acc: 0.8065 - val_loss: 0.2852 - val_acc: 0.8099\n",
      "Epoch 105/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2907 - acc: 0.8047 - val_loss: 0.2869 - val_acc: 0.8059\n",
      "Epoch 106/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2908 - acc: 0.8059 - val_loss: 0.2882 - val_acc: 0.8109\n",
      "Epoch 107/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2904 - acc: 0.8065 - val_loss: 0.2858 - val_acc: 0.8086\n",
      "Epoch 108/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2916 - acc: 0.8056 - val_loss: 0.2851 - val_acc: 0.8088\n",
      "Epoch 109/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2903 - acc: 0.8055 - val_loss: 0.2856 - val_acc: 0.8103\n",
      "Epoch 110/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2901 - acc: 0.8064 - val_loss: 0.2878 - val_acc: 0.8108\n",
      "Epoch 111/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2909 - acc: 0.8039 - val_loss: 0.2877 - val_acc: 0.8110\n",
      "Epoch 112/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2893 - acc: 0.8059 - val_loss: 0.2849 - val_acc: 0.8084\n",
      "Epoch 113/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2899 - acc: 0.8060 - val_loss: 0.2867 - val_acc: 0.8078\n",
      "Epoch 114/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2882 - acc: 0.8079 - val_loss: 0.2862 - val_acc: 0.8074\n",
      "Epoch 115/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2904 - acc: 0.8058 - val_loss: 0.2853 - val_acc: 0.8079\n",
      "Epoch 116/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2893 - acc: 0.8067 - val_loss: 0.2872 - val_acc: 0.8109\n",
      "Epoch 117/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2899 - acc: 0.8056 - val_loss: 0.2876 - val_acc: 0.8090\n",
      "Epoch 118/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2899 - acc: 0.8060 - val_loss: 0.2880 - val_acc: 0.8097\n",
      "Epoch 119/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2885 - acc: 0.8066 - val_loss: 0.2870 - val_acc: 0.8086\n",
      "Epoch 120/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2893 - acc: 0.8057 - val_loss: 0.2866 - val_acc: 0.8090\n",
      "Epoch 121/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2888 - acc: 0.8056 - val_loss: 0.2871 - val_acc: 0.8078\n",
      "Epoch 122/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2876 - acc: 0.8077 - val_loss: 0.2874 - val_acc: 0.8077\n",
      "Epoch 123/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2897 - acc: 0.8055 - val_loss: 0.2912 - val_acc: 0.8101\n",
      "Epoch 124/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2875 - acc: 0.8080 - val_loss: 0.2885 - val_acc: 0.8075\n",
      "Epoch 125/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2893 - acc: 0.8058 - val_loss: 0.2890 - val_acc: 0.8083\n",
      "Epoch 126/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2881 - acc: 0.8063 - val_loss: 0.2873 - val_acc: 0.8087\n",
      "Epoch 127/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2879 - acc: 0.8072 - val_loss: 0.2887 - val_acc: 0.8092\n",
      "Epoch 128/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2874 - acc: 0.8068 - val_loss: 0.2882 - val_acc: 0.8076\n",
      "Epoch 129/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2880 - acc: 0.8065 - val_loss: 0.2884 - val_acc: 0.8086\n",
      "Epoch 130/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2873 - acc: 0.8074 - val_loss: 0.2905 - val_acc: 0.8081\n",
      "Epoch 131/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2871 - acc: 0.8072 - val_loss: 0.2901 - val_acc: 0.8072\n",
      "Epoch 132/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2884 - acc: 0.8062 - val_loss: 0.2898 - val_acc: 0.8086\n",
      "Epoch 133/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2869 - acc: 0.8076 - val_loss: 0.2910 - val_acc: 0.8077\n",
      "Epoch 134/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2879 - acc: 0.8068 - val_loss: 0.2919 - val_acc: 0.8082\n",
      "Epoch 135/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2870 - acc: 0.8070 - val_loss: 0.2895 - val_acc: 0.8081\n",
      "Epoch 136/400\n",
      "1568160/1568160 [==============================] - 123s 78us/step - loss: 0.2870 - acc: 0.8069 - val_loss: 0.2901 - val_acc: 0.8084\n",
      "Epoch 137/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2878 - acc: 0.8056 - val_loss: 0.2900 - val_acc: 0.8082\n",
      "Epoch 138/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2868 - acc: 0.8065 - val_loss: 0.2920 - val_acc: 0.8079\n",
      "Epoch 139/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2871 - acc: 0.8065 - val_loss: 0.2943 - val_acc: 0.8082\n",
      "Epoch 140/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2864 - acc: 0.8077 - val_loss: 0.2931 - val_acc: 0.8076\n",
      "Epoch 141/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2866 - acc: 0.8072 - val_loss: 0.2933 - val_acc: 0.8079\n",
      "Epoch 142/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2869 - acc: 0.8071 - val_loss: 0.2928 - val_acc: 0.8081\n",
      "Epoch 143/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2869 - acc: 0.8054 - val_loss: 0.2910 - val_acc: 0.8069\n",
      "Epoch 144/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2862 - acc: 0.8073 - val_loss: 0.2912 - val_acc: 0.8077\n",
      "Epoch 145/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2857 - acc: 0.8074 - val_loss: 0.2908 - val_acc: 0.8090\n",
      "Epoch 146/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2868 - acc: 0.8061 - val_loss: 0.2938 - val_acc: 0.8086\n",
      "Epoch 147/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2848 - acc: 0.8085 - val_loss: 0.2928 - val_acc: 0.8074\n",
      "Epoch 148/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2863 - acc: 0.8064 - val_loss: 0.2940 - val_acc: 0.8075\n",
      "Epoch 149/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2847 - acc: 0.8090 - val_loss: 0.2927 - val_acc: 0.8080\n",
      "Epoch 150/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2848 - acc: 0.8080 - val_loss: 0.2932 - val_acc: 0.8085\n",
      "Epoch 151/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2848 - acc: 0.8080 - val_loss: 0.2937 - val_acc: 0.8082\n",
      "Epoch 152/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2851 - acc: 0.8086 - val_loss: 0.2961 - val_acc: 0.8084\n",
      "Epoch 153/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2838 - acc: 0.8094 - val_loss: 0.2947 - val_acc: 0.8076\n",
      "Epoch 154/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2842 - acc: 0.8087 - val_loss: 0.2959 - val_acc: 0.8074\n",
      "Epoch 155/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2838 - acc: 0.8083 - val_loss: 0.2948 - val_acc: 0.8077\n",
      "Epoch 156/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2838 - acc: 0.8085 - val_loss: 0.2965 - val_acc: 0.8080\n",
      "Epoch 157/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2828 - acc: 0.8090 - val_loss: 0.2950 - val_acc: 0.8082\n",
      "Epoch 158/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2829 - acc: 0.8099 - val_loss: 0.2960 - val_acc: 0.8081\n",
      "Epoch 159/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2828 - acc: 0.8099 - val_loss: 0.2969 - val_acc: 0.8088\n",
      "Epoch 160/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2826 - acc: 0.8095 - val_loss: 0.2963 - val_acc: 0.8083\n",
      "Epoch 161/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2827 - acc: 0.8095 - val_loss: 0.2973 - val_acc: 0.8076\n",
      "Epoch 162/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2829 - acc: 0.8097 - val_loss: 0.2996 - val_acc: 0.8087\n",
      "Epoch 163/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2820 - acc: 0.8101 - val_loss: 0.2976 - val_acc: 0.8075\n",
      "Epoch 164/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2818 - acc: 0.8107 - val_loss: 0.2979 - val_acc: 0.8075\n",
      "Epoch 165/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2817 - acc: 0.8111 - val_loss: 0.2992 - val_acc: 0.8077\n",
      "Epoch 166/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2813 - acc: 0.8108 - val_loss: 0.3004 - val_acc: 0.8076\n",
      "Epoch 167/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2811 - acc: 0.8109 - val_loss: 0.2992 - val_acc: 0.8081\n",
      "Epoch 168/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2810 - acc: 0.8106 - val_loss: 0.3002 - val_acc: 0.8082\n",
      "Epoch 169/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2810 - acc: 0.8106 - val_loss: 0.2997 - val_acc: 0.8083\n",
      "Epoch 170/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2810 - acc: 0.8105 - val_loss: 0.3012 - val_acc: 0.8085\n",
      "Epoch 171/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2806 - acc: 0.8100 - val_loss: 0.2998 - val_acc: 0.8082\n",
      "Epoch 172/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2805 - acc: 0.8105 - val_loss: 0.2993 - val_acc: 0.8085\n",
      "Epoch 173/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2810 - acc: 0.8105 - val_loss: 0.2999 - val_acc: 0.8089\n",
      "Epoch 174/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2803 - acc: 0.8103 - val_loss: 0.3022 - val_acc: 0.8083\n",
      "Epoch 175/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2804 - acc: 0.8108 - val_loss: 0.3039 - val_acc: 0.8080\n",
      "Epoch 176/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2803 - acc: 0.8112 - val_loss: 0.3022 - val_acc: 0.8082\n",
      "Epoch 177/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2799 - acc: 0.8112 - val_loss: 0.3023 - val_acc: 0.8083\n",
      "Epoch 178/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2799 - acc: 0.8111 - val_loss: 0.3024 - val_acc: 0.8082\n",
      "Epoch 179/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2799 - acc: 0.8107 - val_loss: 0.3033 - val_acc: 0.8077\n",
      "Epoch 180/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2795 - acc: 0.8108 - val_loss: 0.3026 - val_acc: 0.8078\n",
      "Epoch 181/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2792 - acc: 0.8114 - val_loss: 0.3023 - val_acc: 0.8083\n",
      "Epoch 182/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2791 - acc: 0.8106 - val_loss: 0.3020 - val_acc: 0.8082\n",
      "Epoch 183/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2784 - acc: 0.8123 - val_loss: 0.3020 - val_acc: 0.8077\n",
      "Epoch 184/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2785 - acc: 0.8113 - val_loss: 0.3018 - val_acc: 0.8079\n",
      "Epoch 185/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2782 - acc: 0.8113 - val_loss: 0.3016 - val_acc: 0.8083\n",
      "Epoch 186/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2780 - acc: 0.8116 - val_loss: 0.3022 - val_acc: 0.8085\n",
      "Epoch 187/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2781 - acc: 0.8123 - val_loss: 0.3028 - val_acc: 0.8079\n",
      "Epoch 188/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2781 - acc: 0.8120 - val_loss: 0.3035 - val_acc: 0.8088\n",
      "Epoch 189/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2779 - acc: 0.8123 - val_loss: 0.3047 - val_acc: 0.8080\n",
      "Epoch 190/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2778 - acc: 0.8111 - val_loss: 0.3039 - val_acc: 0.8085\n",
      "Epoch 191/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2773 - acc: 0.8128 - val_loss: 0.3046 - val_acc: 0.8082\n",
      "Epoch 192/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2775 - acc: 0.8110 - val_loss: 0.3029 - val_acc: 0.8084\n",
      "Epoch 193/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2767 - acc: 0.8121 - val_loss: 0.3031 - val_acc: 0.8085\n",
      "Epoch 194/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2766 - acc: 0.8113 - val_loss: 0.3028 - val_acc: 0.8081\n",
      "Epoch 195/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2765 - acc: 0.8124 - val_loss: 0.3038 - val_acc: 0.8083\n",
      "Epoch 196/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2768 - acc: 0.8121 - val_loss: 0.3053 - val_acc: 0.8080\n",
      "Epoch 197/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2767 - acc: 0.8125 - val_loss: 0.3051 - val_acc: 0.8081\n",
      "Epoch 198/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2759 - acc: 0.8122 - val_loss: 0.3049 - val_acc: 0.8081\n",
      "Epoch 199/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2753 - acc: 0.8123 - val_loss: 0.3051 - val_acc: 0.8080\n",
      "Epoch 200/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2757 - acc: 0.8121 - val_loss: 0.3057 - val_acc: 0.8080\n",
      "Epoch 201/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2755 - acc: 0.8118 - val_loss: 0.3053 - val_acc: 0.8077\n",
      "Epoch 202/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2749 - acc: 0.8124 - val_loss: 0.3064 - val_acc: 0.8075\n",
      "Epoch 203/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2742 - acc: 0.8128 - val_loss: 0.3055 - val_acc: 0.8072\n",
      "Epoch 204/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2740 - acc: 0.8136 - val_loss: 0.3063 - val_acc: 0.8071\n",
      "Epoch 205/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2739 - acc: 0.8139 - val_loss: 0.3063 - val_acc: 0.8074\n",
      "Epoch 206/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2740 - acc: 0.8136 - val_loss: 0.3068 - val_acc: 0.8072\n",
      "Epoch 207/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2741 - acc: 0.8139 - val_loss: 0.3071 - val_acc: 0.8074\n",
      "Epoch 208/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2740 - acc: 0.8142 - val_loss: 0.3077 - val_acc: 0.8071\n",
      "Epoch 209/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2743 - acc: 0.8142 - val_loss: 0.3074 - val_acc: 0.8069\n",
      "Epoch 210/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2741 - acc: 0.8143 - val_loss: 0.3070 - val_acc: 0.8065\n",
      "Epoch 211/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2736 - acc: 0.8148 - val_loss: 0.3077 - val_acc: 0.8065\n",
      "Epoch 212/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2728 - acc: 0.8143 - val_loss: 0.3083 - val_acc: 0.8062\n",
      "Epoch 213/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2722 - acc: 0.8148 - val_loss: 0.3084 - val_acc: 0.8065\n",
      "Epoch 214/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2719 - acc: 0.8156 - val_loss: 0.3088 - val_acc: 0.8068\n",
      "Epoch 215/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2718 - acc: 0.8154 - val_loss: 0.3083 - val_acc: 0.8062\n",
      "Epoch 216/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2717 - acc: 0.8157 - val_loss: 0.3089 - val_acc: 0.8062\n",
      "Epoch 217/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2715 - acc: 0.8163 - val_loss: 0.3088 - val_acc: 0.8067\n",
      "Epoch 218/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2712 - acc: 0.8161 - val_loss: 0.3100 - val_acc: 0.8065\n",
      "Epoch 219/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2712 - acc: 0.8165 - val_loss: 0.3090 - val_acc: 0.8064\n",
      "Epoch 220/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2708 - acc: 0.8159 - val_loss: 0.3091 - val_acc: 0.8063\n",
      "Epoch 221/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2705 - acc: 0.8169 - val_loss: 0.3089 - val_acc: 0.8064\n",
      "Epoch 222/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2699 - acc: 0.8171 - val_loss: 0.3103 - val_acc: 0.8061\n",
      "Epoch 223/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2700 - acc: 0.8169 - val_loss: 0.3103 - val_acc: 0.8063\n",
      "Epoch 224/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2696 - acc: 0.8175 - val_loss: 0.3110 - val_acc: 0.8063\n",
      "Epoch 225/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2693 - acc: 0.8178 - val_loss: 0.3115 - val_acc: 0.8061\n",
      "Epoch 226/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2693 - acc: 0.8176 - val_loss: 0.3108 - val_acc: 0.8061\n",
      "Epoch 227/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2693 - acc: 0.8174 - val_loss: 0.3119 - val_acc: 0.8062\n",
      "Epoch 228/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2691 - acc: 0.8183 - val_loss: 0.3114 - val_acc: 0.8062\n",
      "Epoch 229/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2690 - acc: 0.8179 - val_loss: 0.3121 - val_acc: 0.8061\n",
      "Epoch 230/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2688 - acc: 0.8186 - val_loss: 0.3120 - val_acc: 0.8061\n",
      "Epoch 231/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2687 - acc: 0.8184 - val_loss: 0.3125 - val_acc: 0.8061\n",
      "Epoch 232/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2686 - acc: 0.8186 - val_loss: 0.3119 - val_acc: 0.8058\n",
      "Epoch 233/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2683 - acc: 0.8185 - val_loss: 0.3123 - val_acc: 0.8059\n",
      "Epoch 234/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2680 - acc: 0.8183 - val_loss: 0.3140 - val_acc: 0.8059\n",
      "Epoch 235/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2678 - acc: 0.8184 - val_loss: 0.3138 - val_acc: 0.8060\n",
      "Epoch 236/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2674 - acc: 0.8183 - val_loss: 0.3132 - val_acc: 0.8061\n",
      "Epoch 237/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2672 - acc: 0.8189 - val_loss: 0.3145 - val_acc: 0.8062\n",
      "Epoch 238/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2668 - acc: 0.8194 - val_loss: 0.3153 - val_acc: 0.8062\n",
      "Epoch 239/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2667 - acc: 0.8191 - val_loss: 0.3150 - val_acc: 0.8062\n",
      "Epoch 240/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2665 - acc: 0.8194 - val_loss: 0.3147 - val_acc: 0.8060\n",
      "Epoch 241/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2664 - acc: 0.8193 - val_loss: 0.3153 - val_acc: 0.8061\n",
      "Epoch 242/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2662 - acc: 0.8192 - val_loss: 0.3153 - val_acc: 0.8060\n",
      "Epoch 243/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2659 - acc: 0.8196 - val_loss: 0.3155 - val_acc: 0.8058\n",
      "Epoch 244/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2657 - acc: 0.8195 - val_loss: 0.3158 - val_acc: 0.8060\n",
      "Epoch 245/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2656 - acc: 0.8196 - val_loss: 0.3160 - val_acc: 0.8059\n",
      "Epoch 246/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2654 - acc: 0.8197 - val_loss: 0.3168 - val_acc: 0.8058\n",
      "Epoch 247/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2652 - acc: 0.8194 - val_loss: 0.3173 - val_acc: 0.8057\n",
      "Epoch 248/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2650 - acc: 0.8201 - val_loss: 0.3174 - val_acc: 0.8058\n",
      "Epoch 249/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2648 - acc: 0.8204 - val_loss: 0.3184 - val_acc: 0.8059\n",
      "Epoch 250/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2645 - acc: 0.8203 - val_loss: 0.3182 - val_acc: 0.8060\n",
      "Epoch 251/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2644 - acc: 0.8202 - val_loss: 0.3196 - val_acc: 0.8058\n",
      "Epoch 252/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2640 - acc: 0.8206 - val_loss: 0.3186 - val_acc: 0.8058\n",
      "Epoch 253/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2633 - acc: 0.8214 - val_loss: 0.3184 - val_acc: 0.8057\n",
      "Epoch 254/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2628 - acc: 0.8220 - val_loss: 0.3183 - val_acc: 0.8058\n",
      "Epoch 255/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2626 - acc: 0.8223 - val_loss: 0.3183 - val_acc: 0.8058\n",
      "Epoch 256/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2625 - acc: 0.8227 - val_loss: 0.3185 - val_acc: 0.8060\n",
      "Epoch 257/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2623 - acc: 0.8225 - val_loss: 0.3198 - val_acc: 0.8059\n",
      "Epoch 258/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2621 - acc: 0.8231 - val_loss: 0.3196 - val_acc: 0.8060\n",
      "Epoch 259/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2619 - acc: 0.8230 - val_loss: 0.3204 - val_acc: 0.8059\n",
      "Epoch 260/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2614 - acc: 0.8232 - val_loss: 0.3207 - val_acc: 0.8061\n",
      "Epoch 261/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2615 - acc: 0.8231 - val_loss: 0.3213 - val_acc: 0.8059\n",
      "Epoch 262/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2615 - acc: 0.8233 - val_loss: 0.3201 - val_acc: 0.8061\n",
      "Epoch 263/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2610 - acc: 0.8235 - val_loss: 0.3223 - val_acc: 0.8061\n",
      "Epoch 264/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2609 - acc: 0.8234 - val_loss: 0.3228 - val_acc: 0.8060\n",
      "Epoch 265/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2607 - acc: 0.8236 - val_loss: 0.3225 - val_acc: 0.8057\n",
      "Epoch 266/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2606 - acc: 0.8240 - val_loss: 0.3228 - val_acc: 0.8059\n",
      "Epoch 267/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2604 - acc: 0.8240 - val_loss: 0.3225 - val_acc: 0.8058\n",
      "Epoch 268/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2601 - acc: 0.8239 - val_loss: 0.3252 - val_acc: 0.8060\n",
      "Epoch 269/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2603 - acc: 0.8239 - val_loss: 0.3256 - val_acc: 0.8060\n",
      "Epoch 270/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2599 - acc: 0.8243 - val_loss: 0.3241 - val_acc: 0.8057\n",
      "Epoch 271/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2596 - acc: 0.8244 - val_loss: 0.3247 - val_acc: 0.8060\n",
      "Epoch 272/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2596 - acc: 0.8245 - val_loss: 0.3244 - val_acc: 0.8057\n",
      "Epoch 273/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2594 - acc: 0.8244 - val_loss: 0.3253 - val_acc: 0.8056\n",
      "Epoch 274/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2593 - acc: 0.8243 - val_loss: 0.3237 - val_acc: 0.8060\n",
      "Epoch 275/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2591 - acc: 0.8245 - val_loss: 0.3255 - val_acc: 0.8054\n",
      "Epoch 276/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2589 - acc: 0.8245 - val_loss: 0.3256 - val_acc: 0.8055\n",
      "Epoch 277/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2587 - acc: 0.8248 - val_loss: 0.3260 - val_acc: 0.8060\n",
      "Epoch 278/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2588 - acc: 0.8245 - val_loss: 0.3266 - val_acc: 0.8060\n",
      "Epoch 279/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2585 - acc: 0.8247 - val_loss: 0.3250 - val_acc: 0.8057\n",
      "Epoch 280/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2581 - acc: 0.8252 - val_loss: 0.3269 - val_acc: 0.8058\n",
      "Epoch 281/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2580 - acc: 0.8253 - val_loss: 0.3268 - val_acc: 0.8059\n",
      "Epoch 282/400\n",
      "1568160/1568160 [==============================] - 122s 77us/step - loss: 0.2578 - acc: 0.8255 - val_loss: 0.3275 - val_acc: 0.8060\n",
      "Epoch 283/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2575 - acc: 0.8254 - val_loss: 0.3260 - val_acc: 0.8056\n",
      "Epoch 284/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2575 - acc: 0.8253 - val_loss: 0.3279 - val_acc: 0.8059\n",
      "Epoch 285/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2574 - acc: 0.8254 - val_loss: 0.3284 - val_acc: 0.8059\n",
      "Epoch 286/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2571 - acc: 0.8258 - val_loss: 0.3263 - val_acc: 0.8059\n",
      "Epoch 287/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2569 - acc: 0.8261 - val_loss: 0.3273 - val_acc: 0.8058\n",
      "Epoch 288/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2566 - acc: 0.8256 - val_loss: 0.3281 - val_acc: 0.8057\n",
      "Epoch 289/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2566 - acc: 0.8263 - val_loss: 0.3297 - val_acc: 0.8059\n",
      "Epoch 290/400\n",
      "1568160/1568160 [==============================] - 122s 77us/step - loss: 0.2565 - acc: 0.8261 - val_loss: 0.3278 - val_acc: 0.8057\n",
      "Epoch 291/400\n",
      "1568160/1568160 [==============================] - 122s 77us/step - loss: 0.2564 - acc: 0.8262 - val_loss: 0.3285 - val_acc: 0.8053\n",
      "Epoch 292/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2561 - acc: 0.8263 - val_loss: 0.3279 - val_acc: 0.8057\n",
      "Epoch 293/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2561 - acc: 0.8262 - val_loss: 0.3283 - val_acc: 0.8058\n",
      "Epoch 294/400\n",
      "1568160/1568160 [==============================] - 122s 77us/step - loss: 0.2557 - acc: 0.8266 - val_loss: 0.3291 - val_acc: 0.8058\n",
      "Epoch 295/400\n",
      "1568160/1568160 [==============================] - 122s 77us/step - loss: 0.2556 - acc: 0.8264 - val_loss: 0.3296 - val_acc: 0.8058\n",
      "Epoch 296/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2554 - acc: 0.8268 - val_loss: 0.3295 - val_acc: 0.8056\n",
      "Epoch 297/400\n",
      "1568160/1568160 [==============================] - 122s 78us/step - loss: 0.2553 - acc: 0.8269 - val_loss: 0.3303 - val_acc: 0.8057\n",
      "Epoch 298/400\n",
      "1568160/1568160 [==============================] - 122s 77us/step - loss: 0.2553 - acc: 0.8267 - val_loss: 0.3292 - val_acc: 0.8055\n",
      "Epoch 299/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2550 - acc: 0.8269 - val_loss: 0.3305 - val_acc: 0.8057\n",
      "Epoch 300/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2549 - acc: 0.8269 - val_loss: 0.3316 - val_acc: 0.8057\n",
      "Epoch 301/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2552 - acc: 0.8267 - val_loss: 0.3293 - val_acc: 0.8052\n",
      "Epoch 302/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2545 - acc: 0.8273 - val_loss: 0.3307 - val_acc: 0.8056\n",
      "Epoch 303/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2547 - acc: 0.8272 - val_loss: 0.3303 - val_acc: 0.8055\n",
      "Epoch 304/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2541 - acc: 0.8280 - val_loss: 0.3317 - val_acc: 0.8058\n",
      "Epoch 305/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2541 - acc: 0.8275 - val_loss: 0.3316 - val_acc: 0.8056\n",
      "Epoch 306/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2540 - acc: 0.8279 - val_loss: 0.3323 - val_acc: 0.8055\n",
      "Epoch 307/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2538 - acc: 0.8273 - val_loss: 0.3308 - val_acc: 0.8054\n",
      "Epoch 308/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2536 - acc: 0.8274 - val_loss: 0.3326 - val_acc: 0.8051\n",
      "Epoch 309/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2534 - acc: 0.8278 - val_loss: 0.3314 - val_acc: 0.8055\n",
      "Epoch 310/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2533 - acc: 0.8281 - val_loss: 0.3325 - val_acc: 0.8056\n",
      "Epoch 311/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2534 - acc: 0.8277 - val_loss: 0.3346 - val_acc: 0.8058\n",
      "Epoch 312/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2532 - acc: 0.8277 - val_loss: 0.3323 - val_acc: 0.8054\n",
      "Epoch 313/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2530 - acc: 0.8278 - val_loss: 0.3351 - val_acc: 0.8055\n",
      "Epoch 314/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2528 - acc: 0.8283 - val_loss: 0.3340 - val_acc: 0.8058\n",
      "Epoch 315/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2526 - acc: 0.8284 - val_loss: 0.3363 - val_acc: 0.8057\n",
      "Epoch 316/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2526 - acc: 0.8282 - val_loss: 0.3346 - val_acc: 0.8054\n",
      "Epoch 317/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2525 - acc: 0.8284 - val_loss: 0.3331 - val_acc: 0.8057\n",
      "Epoch 318/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2523 - acc: 0.8282 - val_loss: 0.3350 - val_acc: 0.8055\n",
      "Epoch 319/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2520 - acc: 0.8285 - val_loss: 0.3349 - val_acc: 0.8052\n",
      "Epoch 320/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2521 - acc: 0.8284 - val_loss: 0.3348 - val_acc: 0.8060\n",
      "Epoch 321/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2519 - acc: 0.8288 - val_loss: 0.3350 - val_acc: 0.8057\n",
      "Epoch 322/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2518 - acc: 0.8287 - val_loss: 0.3354 - val_acc: 0.8055\n",
      "Epoch 323/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2516 - acc: 0.8289 - val_loss: 0.3355 - val_acc: 0.8054\n",
      "Epoch 324/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2515 - acc: 0.8288 - val_loss: 0.3371 - val_acc: 0.8056\n",
      "Epoch 325/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2512 - acc: 0.8292 - val_loss: 0.3358 - val_acc: 0.8055\n",
      "Epoch 326/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2512 - acc: 0.8293 - val_loss: 0.3401 - val_acc: 0.8059\n",
      "Epoch 327/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2510 - acc: 0.8292 - val_loss: 0.3362 - val_acc: 0.8054\n",
      "Epoch 328/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2509 - acc: 0.8292 - val_loss: 0.3408 - val_acc: 0.8056\n",
      "Epoch 329/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2509 - acc: 0.8293 - val_loss: 0.3401 - val_acc: 0.8055\n",
      "Epoch 330/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2507 - acc: 0.8297 - val_loss: 0.3380 - val_acc: 0.8055\n",
      "Epoch 331/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2505 - acc: 0.8297 - val_loss: 0.3378 - val_acc: 0.8055\n",
      "Epoch 332/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2503 - acc: 0.8296 - val_loss: 0.3386 - val_acc: 0.8054\n",
      "Epoch 333/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2503 - acc: 0.8295 - val_loss: 0.3416 - val_acc: 0.8058\n",
      "Epoch 334/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2501 - acc: 0.8298 - val_loss: 0.3402 - val_acc: 0.8060\n",
      "Epoch 335/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2499 - acc: 0.8298 - val_loss: 0.3402 - val_acc: 0.8056\n",
      "Epoch 336/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2498 - acc: 0.8299 - val_loss: 0.3394 - val_acc: 0.8056\n",
      "Epoch 337/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2497 - acc: 0.8298 - val_loss: 0.3409 - val_acc: 0.8057\n",
      "Epoch 338/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2496 - acc: 0.8302 - val_loss: 0.3386 - val_acc: 0.8056\n",
      "Epoch 339/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2495 - acc: 0.8306 - val_loss: 0.3401 - val_acc: 0.8059\n",
      "Epoch 340/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2495 - acc: 0.8301 - val_loss: 0.3409 - val_acc: 0.8059\n",
      "Epoch 341/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2493 - acc: 0.8300 - val_loss: 0.3399 - val_acc: 0.8056\n",
      "Epoch 342/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2492 - acc: 0.8306 - val_loss: 0.3382 - val_acc: 0.8060\n",
      "Epoch 343/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2492 - acc: 0.8300 - val_loss: 0.3390 - val_acc: 0.8058\n",
      "Epoch 344/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2488 - acc: 0.8304 - val_loss: 0.3419 - val_acc: 0.8058\n",
      "Epoch 345/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2491 - acc: 0.8304 - val_loss: 0.3400 - val_acc: 0.8058\n",
      "Epoch 346/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2488 - acc: 0.8308 - val_loss: 0.3382 - val_acc: 0.8058\n",
      "Epoch 347/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2486 - acc: 0.8309 - val_loss: 0.3414 - val_acc: 0.8058\n",
      "Epoch 348/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2486 - acc: 0.8309 - val_loss: 0.3417 - val_acc: 0.8058\n",
      "Epoch 349/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2484 - acc: 0.8311 - val_loss: 0.3408 - val_acc: 0.8060\n",
      "Epoch 350/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2485 - acc: 0.8309 - val_loss: 0.3418 - val_acc: 0.8058\n",
      "Epoch 351/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2483 - acc: 0.8311 - val_loss: 0.3357 - val_acc: 0.8061\n",
      "Epoch 352/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2484 - acc: 0.8314 - val_loss: 0.3415 - val_acc: 0.8057\n",
      "Epoch 353/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2481 - acc: 0.8316 - val_loss: 0.3408 - val_acc: 0.8058\n",
      "Epoch 354/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2479 - acc: 0.8314 - val_loss: 0.3407 - val_acc: 0.8058\n",
      "Epoch 355/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2479 - acc: 0.8313 - val_loss: 0.3410 - val_acc: 0.8065\n",
      "Epoch 356/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2477 - acc: 0.8318 - val_loss: 0.3413 - val_acc: 0.8058\n",
      "Epoch 357/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2477 - acc: 0.8313 - val_loss: 0.3412 - val_acc: 0.8061\n",
      "Epoch 358/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2475 - acc: 0.8317 - val_loss: 0.3428 - val_acc: 0.8061\n",
      "Epoch 359/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2474 - acc: 0.8314 - val_loss: 0.3425 - val_acc: 0.8060\n",
      "Epoch 360/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2473 - acc: 0.8315 - val_loss: 0.3425 - val_acc: 0.8062\n",
      "Epoch 361/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2473 - acc: 0.8322 - val_loss: 0.3409 - val_acc: 0.8059\n",
      "Epoch 362/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2470 - acc: 0.8315 - val_loss: 0.3410 - val_acc: 0.8060\n",
      "Epoch 363/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2469 - acc: 0.8319 - val_loss: 0.3414 - val_acc: 0.8061\n",
      "Epoch 364/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2469 - acc: 0.8320 - val_loss: 0.3437 - val_acc: 0.8063\n",
      "Epoch 365/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2468 - acc: 0.8320 - val_loss: 0.3427 - val_acc: 0.8062\n",
      "Epoch 366/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2467 - acc: 0.8317 - val_loss: 0.3379 - val_acc: 0.8059\n",
      "Epoch 367/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2464 - acc: 0.8321 - val_loss: 0.3414 - val_acc: 0.8060\n",
      "Epoch 368/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2465 - acc: 0.8319 - val_loss: 0.3425 - val_acc: 0.8062\n",
      "Epoch 369/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2465 - acc: 0.8323 - val_loss: 0.3436 - val_acc: 0.8063\n",
      "Epoch 370/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2462 - acc: 0.8325 - val_loss: 0.3444 - val_acc: 0.8064\n",
      "Epoch 371/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2461 - acc: 0.8325 - val_loss: 0.3438 - val_acc: 0.8062\n",
      "Epoch 372/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2460 - acc: 0.8326 - val_loss: 0.3408 - val_acc: 0.8064\n",
      "Epoch 373/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2458 - acc: 0.8328 - val_loss: 0.3410 - val_acc: 0.8062\n",
      "Epoch 374/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2457 - acc: 0.8327 - val_loss: 0.3423 - val_acc: 0.8058\n",
      "Epoch 375/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2456 - acc: 0.8332 - val_loss: 0.3414 - val_acc: 0.8061\n",
      "Epoch 376/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2457 - acc: 0.8327 - val_loss: 0.3412 - val_acc: 0.8061\n",
      "Epoch 377/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2455 - acc: 0.8330 - val_loss: 0.3428 - val_acc: 0.8063\n",
      "Epoch 378/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2455 - acc: 0.8327 - val_loss: 0.3431 - val_acc: 0.8062\n",
      "Epoch 379/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2453 - acc: 0.8328 - val_loss: 0.3446 - val_acc: 0.8063\n",
      "Epoch 380/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2453 - acc: 0.8334 - val_loss: 0.3425 - val_acc: 0.8058\n",
      "Epoch 381/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2452 - acc: 0.8332 - val_loss: 0.3431 - val_acc: 0.8069\n",
      "Epoch 382/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2450 - acc: 0.8333 - val_loss: 0.3426 - val_acc: 0.8063\n",
      "Epoch 383/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2450 - acc: 0.8330 - val_loss: 0.3458 - val_acc: 0.8064\n",
      "Epoch 384/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2449 - acc: 0.8332 - val_loss: 0.3423 - val_acc: 0.8063\n",
      "Epoch 385/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2448 - acc: 0.8334 - val_loss: 0.3458 - val_acc: 0.8065\n",
      "Epoch 386/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2447 - acc: 0.8336 - val_loss: 0.3440 - val_acc: 0.8061\n",
      "Epoch 387/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2445 - acc: 0.8334 - val_loss: 0.3436 - val_acc: 0.8063\n",
      "Epoch 388/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2447 - acc: 0.8337 - val_loss: 0.3424 - val_acc: 0.8067\n",
      "Epoch 389/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2446 - acc: 0.8333 - val_loss: 0.3473 - val_acc: 0.8064\n",
      "Epoch 390/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2446 - acc: 0.8339 - val_loss: 0.3444 - val_acc: 0.8062\n",
      "Epoch 391/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2443 - acc: 0.8336 - val_loss: 0.3474 - val_acc: 0.8064\n",
      "Epoch 392/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2443 - acc: 0.8342 - val_loss: 0.3433 - val_acc: 0.8060\n",
      "Epoch 393/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2441 - acc: 0.8340 - val_loss: 0.3453 - val_acc: 0.8063\n",
      "Epoch 394/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2441 - acc: 0.8338 - val_loss: 0.3445 - val_acc: 0.8063\n",
      "Epoch 395/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2441 - acc: 0.8340 - val_loss: 0.3444 - val_acc: 0.8065\n",
      "Epoch 396/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2438 - acc: 0.8340 - val_loss: 0.3472 - val_acc: 0.8064\n",
      "Epoch 397/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2436 - acc: 0.8346 - val_loss: 0.3452 - val_acc: 0.8064\n",
      "Epoch 398/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2436 - acc: 0.8340 - val_loss: 0.3456 - val_acc: 0.8062\n",
      "Epoch 399/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2437 - acc: 0.8339 - val_loss: 0.3451 - val_acc: 0.8066\n",
      "Epoch 400/400\n",
      "1568160/1568160 [==============================] - 121s 77us/step - loss: 0.2434 - acc: 0.8342 - val_loss: 0.3459 - val_acc: 0.8067\n",
      "benchacc:\n",
      "0.6666666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_MLP\n",
      "drop_out\n",
      "compile model success\n",
      "1\n",
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: size=2865 MiB, count=924, average=3176 KiB\n",
      "[Training model......]\n",
      "Train on 2086560 samples, validate on 0 samples\n",
      "Epoch 1/400\n",
      "2086560/2086560 [==============================] - 799s 383us/step - loss: 3.2212 - acc: 0.6415\n",
      "Epoch 2/400\n",
      "2086560/2086560 [==============================] - 151s 72us/step - loss: 0.4674 - acc: 0.7456\n",
      "Epoch 3/400\n",
      "2086560/2086560 [==============================] - 150s 72us/step - loss: 0.3552 - acc: 0.7826\n",
      "Epoch 4/400\n",
      "2086560/2086560 [==============================] - 150s 72us/step - loss: 0.3216 - acc: 0.7969\n",
      "Epoch 5/400\n",
      "2086560/2086560 [==============================] - 150s 72us/step - loss: 0.3062 - acc: 0.8042\n",
      "Epoch 6/400\n",
      "2086560/2086560 [==============================] - 149s 72us/step - loss: 0.2987 - acc: 0.8073\n",
      "Epoch 7/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2948 - acc: 0.8091\n",
      "Epoch 8/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2921 - acc: 0.8101\n",
      "Epoch 9/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2905 - acc: 0.8110\n",
      "Epoch 10/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2889 - acc: 0.8114\n",
      "Epoch 11/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2877 - acc: 0.8119\n",
      "Epoch 12/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2867 - acc: 0.8128\n",
      "Epoch 13/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2857 - acc: 0.8130\n",
      "Epoch 14/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2852 - acc: 0.8134\n",
      "Epoch 15/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2841 - acc: 0.8133\n",
      "Epoch 16/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2835 - acc: 0.8139\n",
      "Epoch 17/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2830 - acc: 0.8138\n",
      "Epoch 18/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2827 - acc: 0.8143\n",
      "Epoch 19/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2819 - acc: 0.8146\n",
      "Epoch 20/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2814 - acc: 0.8145\n",
      "Epoch 21/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2809 - acc: 0.8149\n",
      "Epoch 22/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2811 - acc: 0.8150\n",
      "Epoch 23/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2803 - acc: 0.8154\n",
      "Epoch 24/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2800 - acc: 0.8159\n",
      "Epoch 25/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2801 - acc: 0.8154\n",
      "Epoch 26/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2794 - acc: 0.8159\n",
      "Epoch 27/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2793 - acc: 0.8161\n",
      "Epoch 28/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2789 - acc: 0.8159\n",
      "Epoch 29/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2789 - acc: 0.8162\n",
      "Epoch 30/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2786 - acc: 0.8162\n",
      "Epoch 31/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2785 - acc: 0.8166\n",
      "Epoch 32/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2781 - acc: 0.8170\n",
      "Epoch 33/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2780 - acc: 0.8170\n",
      "Epoch 34/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2780 - acc: 0.8168\n",
      "Epoch 35/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2778 - acc: 0.8170\n",
      "Epoch 36/400\n",
      "2086560/2086560 [==============================] - 149s 71us/step - loss: 0.2776 - acc: 0.8168\n",
      "Epoch 37/400\n",
      " 151552/2086560 [=>............................] - ETA: 2:17 - loss: 0.2922 - acc: 0.8059"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4dc65ee8d449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mfit_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1440eafece2d>\u001b[0m in \u001b[0;36mfit_show\u001b[0;34m(traindata, m1, m2, m3)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mTest_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m288\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cur_pair_P\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mTest_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m288\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cur_pair_P\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrain_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTest_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(0,20,1):\n",
    "\n",
    "    MLP_unit=64\n",
    "    visual_scene = Input((l,currencynum))\n",
    "    visual_conv = ConvolutionNetworks(20,6)(visual_scene)\n",
    "    shapes = visual_conv.shape\n",
    "    w = shapes[1]\n",
    "    features= []\n",
    "    for k1 in range(w):\n",
    "        def get_feature(t):\n",
    "            return t[:, k1, :]\n",
    "        get_feature_layer = Lambda(get_feature)\n",
    "        features.append(get_feature_layer(visual_conv))\n",
    "\n",
    "    input2 = Input((15,))\n",
    "    onehot_encode = input2   \n",
    "\n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, onehot_encode]))    \n",
    "\n",
    "\n",
    "    g_MLP = get_MLP(4, get_dense(4,MLP_unit))\n",
    "    f_MLP = get_MLP(4, get_dense(4,MLP_unit))\n",
    "    print(\"g_MLP\")\n",
    "    mid_relations = []\n",
    "    for r in relations:\n",
    "        mid_relations.append(g_MLP(r))\n",
    "\n",
    "    combined_relation = Add()(mid_relations)\n",
    "\n",
    "    #f_MLP\n",
    "    rn = dropout_dense(combined_relation,MLP_unit)\n",
    "    rn = dropout_dense(rn,MLP_unit)\n",
    "    print('drop_out')\n",
    "\n",
    "    pred = Dense(1, activation = 'sigmoid')(rn)\n",
    "\n",
    "    model = Model(inputs=[visual_scene, input2], outputs = pred)\n",
    "    optimizer = Adam(lr = 3e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('compile model success')    \n",
    "\n",
    "    #model.summary()\n",
    "    print(i)\n",
    "    fit_show(Train_data,month[i],month[i+3],month[i+4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
